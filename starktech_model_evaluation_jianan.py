import numpy as np
import pandas as pd
import warnings

warnings.filterwarnings('ignore')
from datetime import datetime, timedelta
import os
import pickle
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, \
    classification_report, confusion_matrix, roc_curve, auc
import xgboost as xgb
import lightgbm as lgb
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
import re
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV, cross_val_score
import traceback
from tqdm import tqdm
import gc
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import time
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
import optuna
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
import shap
import joblib
import json
from collections import defaultdict

# ==================== å‚æ•°è®¾ç½® ====================
FUTURE_DAYS = 20
LOOKBACK_DAYS = 30
USE_PKL_CACHE = False

# æ•°æ®è·¯å¾„,éœ€æ”¹ä¸ºæœ¬åœ°ç”µè„‘æ–‡ä»¶å­˜å‚¨è·¯å¾„
PRICE_DATA_PATH = 'taiwan_stock_price_202511122027.csv'
REPORTS_DATA_PATH = 'reports_202511122033.csv'
PRE_MERGED_FILE = 'taiwan_stock_data_optimized.pkl'  # é¢„åˆå¹¶æ–‡ä»¶å
# æ¨¡å‹å‚æ•°
RANDOM_STATE = 42
TEST_RATIO = 0.2
VAL_RATIO = 0.1
N_JOBS = -1  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ

# æ€§èƒ½ä¼˜åŒ–å‚æ•°
MAX_SAMPLES = 200000
CHUNK_SIZE = 1000
FEATURE_SELECTION_THRESHOLD = 0.001

QUICK_MODE = False  # å¯ç”¨å¿«é€Ÿæ¨¡å¼
MAX_FEATURES = 50  # é™åˆ¶ç‰¹å¾æ•°é‡
HYPERPARAM_TRIALS = 10  # å‡å°‘è¶…å‚æ•°æœç´¢æ¬¡æ•°
SAMPLE_SIZE_TUNING = 5000  # è°ƒä¼˜æ—¶çš„æ ·æœ¬å¤§å°
MERGE_OPTIMIZATION = True  # å¯ç”¨åˆå¹¶ä¼˜åŒ–
QUICK_TUNING = True  # å¿«é€Ÿè°ƒä¼˜æ¨¡å¼
FORCE_REMERGE = False      # æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆå¹¶ï¼ˆè®¾ä¸ºTrueå¯é‡æ–°ç”Ÿæˆé¢„åˆå¹¶æ–‡ä»¶ï¼‰

# æ‰©å±•å‚æ•°ç½‘æ ¼
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# ==================== è¾…åŠ©å‡½æ•° ====================
def timer_decorator(func):
    """è®¡æ—¶è£…é¥°å™¨"""

    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f" {func.__name__} æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’")
        return result

    return wrapper

def get_conservative_params():
    """è¿”å›ä¿å®ˆçš„æ¨¡å‹å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ"""
    return {
        'rf': {
            'n_estimators': 50,        # æ ‘æ•°é‡
            'max_depth': 6,           # æ·±åº¦
            'min_samples_split': 20,   # åˆ†è£‚æ ·æœ¬æ•°
            'min_samples_leaf': 10,   # å¶èŠ‚ç‚¹æ ·æœ¬ï¼‰
            'max_features': 0.3,      # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
            'class_weight': 'balanced',
            'random_state': RANDOM_STATE,
            'n_jobs': N_JOBS
        },
        'xgb': {
            'n_estimators': 50,        # æ ‘æ•°é‡
            'max_depth': 3,           # æ·±åº¦
            'learning_rate': 0.01,    # å­¦ä¹ ç‡
            'subsample': 0.6,         # é‡‡æ ·æ¯”ä¾‹
            'colsample_bytree': 0.6,  # ç‰¹å¾é‡‡æ ·
            'reg_alpha': 1.0,         # L1æ­£åˆ™
            'reg_lambda': 1.0,        # L2æ­£åˆ™
            'scale_pos_weight': 1,    # æ‰‹åŠ¨æ§åˆ¶ç±»åˆ«æƒé‡
            'random_state': RANDOM_STATE,
            'n_jobs': 1,
            'use_label_encoder': False,
            'eval_metric': 'logloss'
        }
    }

def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f" {title}")
    print("=" * 60)


def reduce_memory_usage(df, verbose=True):
    """å‡å°‘æ•°æ®å†…å­˜ä½¿ç”¨ - ä¿®å¤äº†datetime64[ns, UTC+08:00]ç±»å‹çš„é—®é¢˜"""
    start_mem = df.memory_usage().sum() / 1024 ** 2

    for col in df.columns:
        col_type = str(df[col].dtype)

        # è·³è¿‡æ—¥æœŸåˆ—å’Œéæ•°å€¼åˆ—
        if 'datetime' in col_type or col_type in ['object', 'category', 'bool', 'string']:
            continue

        if np.issubdtype(df[col].dtype, np.number):
            c_min = df[col].min()
            c_max = df[col].max()

            if np.issubdtype(df[col].dtype, np.integer):
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)

    end_mem = df.memory_usage().sum() / 1024 ** 2
    if verbose:
        print(f"å†…å­˜ä½¿ç”¨å‡å°‘: {100 * (start_mem - end_mem) / start_mem:.1f}%")
        print(f"ä» {start_mem:.2f} MB åˆ° {end_mem:.2f} MB")

    return df


# ==================== æ•°æ®åŠ è½½å’Œå¤„ç† ====================
@timer_decorator
def load_and_preprocess_data():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ® - ç¡®ä¿æ­£ç¡®è°ƒç”¨ä¿®å¤åçš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—"""
    print_section("æ•°æ®åŠ è½½å’Œé¢„å¤„ç†")

    # ==================== 1. é¢„åˆå¹¶æ–‡ä»¶æ£€æŸ¥ ====================

    if not FORCE_REMERGE and os.path.exists(PRE_MERGED_FILE):
        print(f"åŠ è½½é¢„åˆå¹¶æ–‡ä»¶: {PRE_MERGED_FILE}")
        try:
            with open(PRE_MERGED_FILE, 'rb') as f:
                data = pickle.load(f)

            # é€‚é…ä¸¤ç§æ•°æ®æ ¼å¼
            if isinstance(data, pd.DataFrame):
                # æ ¼å¼1: åªæœ‰DataFrame
                df = data
                # è‡ªåŠ¨æå–ç‰¹å¾åˆ—
                base_cols = ['date', 'stock_code', 'close', 'volume', 'open', 'high', 'low',
                             'future_return', 'market_avg_return', 'label']
                feature_cols = [col for col in df.columns
                                if col not in base_cols and pd.api.types.is_numeric_dtype(df[col])]
                print("æ£€æµ‹åˆ°DataFrameæ ¼å¼ï¼Œè‡ªåŠ¨æå–ç‰¹å¾åˆ—")
                print(f"é¢„åˆå¹¶æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
                print(f"æ—¶é—´èŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
                print(f"è‚¡ç¥¨æ•°é‡: {df['stock_code'].nunique()}")
                return df, feature_cols
            elif isinstance(data, tuple) and len(data) == 2:
                # æ ¼å¼2: (df, feature_cols)
                df, feature_cols = data
                print("æ£€æµ‹åˆ°å…ƒç»„æ ¼å¼: (DataFrame, feature_cols)")
                print(f"é¢„åˆå¹¶æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
                print(f"æ—¶é—´èŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
                print(f"è‚¡ç¥¨æ•°é‡: {df['stock_code'].nunique()}")
                return df, feature_cols
            else:
                print(f"æœªçŸ¥æ•°æ®æ ¼å¼: {type(data)}")
                # ç»§ç»­æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹
        except Exception as e:
            print(f"é¢„åˆå¹¶æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œé‡æ–°å¤„ç†...")

    # ==================== 2. å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ ====================
    print("æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")

    try:
        # 1. åŠ è½½è‚¡ä»·æ•°æ®
        print(f"åŠ è½½è‚¡ä»·æ•°æ®: {PRICE_DATA_PATH}")
        if PRICE_DATA_PATH.endswith('.csv'):
            price_df = pd.read_csv(PRICE_DATA_PATH, encoding='utf-8')
        else:
            price_df = pd.read_excel(PRICE_DATA_PATH)

        print(f"è‚¡ä»·æ•°æ®åŠ è½½æˆåŠŸ: {price_df.shape}")
        print(f"åˆ—å: {list(price_df.columns)}")

        # æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®
        print("\nè‚¡ä»·æ•°æ®æ ·ä¾‹ï¼ˆå‰3è¡Œï¼‰:")
        print(price_df.head(3))

    except Exception as e:
        print(f"è‚¡ä»·æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

    # 2. æ ‡å‡†åŒ–åˆ—å
    print("æ ‡å‡†åŒ–åˆ—å...")
    column_mapping = {
        'stock_id': 'stock_code', 'stock_code': 'stock_code', 'symbol': 'stock_code', 'number': 'stock_code',
        'date': 'date', 'Date': 'date', 'äº¤æ˜“æ—¥': 'date',
        'close': 'close', 'Close': 'close', 'æ”¶ç›˜ä»·': 'close',
        'open': 'open', 'Open': 'open', 'å¼€ç›˜ä»·': 'open',
        'high': 'high', 'High': 'high', 'æœ€é«˜ä»·': 'high',
        'low': 'low', 'Low': 'low', 'æœ€ä½ä»·': 'low',
        'max': 'high', 'min': 'low',
        'volume': 'volume', 'Volume': 'volume', 'æˆäº¤é‡': 'volume', 'trading_volume': 'volume',
        'trading_money': 'amount', 'æˆäº¤é‡‘é¢': 'amount',
        'spread': 'change', 'change': 'change', 'æ¶¨è·Œ': 'change',
        'turnover_rate': 'turnover_rate', 'trading_turnover': 'turnover_rate',
    }

    # åº”ç”¨åˆ—åæ˜ å°„
    for old_col, new_col in column_mapping.items():
        if old_col in price_df.columns and new_col not in price_df.columns:
            price_df = price_df.rename(columns={old_col: new_col})
            print(f"   é‡å‘½å: {old_col} -> {new_col}")

    # æ£€æŸ¥å¿…è¦çš„åˆ—
    required_cols = ['stock_code', 'date', 'close']
    missing_cols = [col for col in required_cols if col not in price_df.columns]
    if missing_cols:
        print(f"é”™è¯¯: ç¼ºå°‘å¿…è¦åˆ— {missing_cols}")
        print(f"å¯ç”¨åˆ—: {list(price_df.columns)}")
        return None

    # 3. æ•°æ®æ¸…æ´—
    print("æ•°æ®æ¸…æ´—...")

    # è½¬æ¢æ•°æ®ç±»å‹
    price_df['stock_code'] = price_df['stock_code'].astype(str).str.strip()

    # ä¿®å¤æ—¥æœŸè½¬æ¢é—®é¢˜ - ç»Ÿä¸€ä¸ºæ— æ—¶åŒºçš„datetime
    try:
        # å°è¯•ä¸åŒçš„æ—¥æœŸæ ¼å¼
        price_df['date'] = pd.to_datetime(price_df['date'], errors='coerce')
        # ç§»é™¤æ—¶åŒºä¿¡æ¯
        if hasattr(price_df['date'].dtype, 'tz') and price_df['date'].dtype.tz is not None:
            price_df['date'] = price_df['date'].dt.tz_convert(None)
    except Exception as e:
        print(f"æ—¥æœŸè½¬æ¢å¤±è´¥: {e}")
        return None

    # ç§»é™¤æ— æ•ˆæ—¥æœŸ
    initial_rows = len(price_df)
    price_df = price_df.dropna(subset=['date'])
    print(f"ç§»é™¤æ— æ•ˆæ—¥æœŸ: {initial_rows - len(price_df):,} è¡Œ")

    # æŒ‰è‚¡ç¥¨å’Œæ—¥æœŸæ’åº
    price_df = price_df.sort_values(['stock_code', 'date'])

    # ç§»é™¤é‡å¤è¡Œ
    initial_rows = len(price_df)
    price_df = price_df.drop_duplicates(subset=['stock_code', 'date'])
    print(f"ç§»é™¤é‡å¤è¡Œ: {initial_rows - len(price_df):,} è¡Œ")

    # å¤„ç†æ•°å€¼åˆ—
    numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount', 'change', 'turnover_rate']
    numeric_cols = [col for col in numeric_cols if col in price_df.columns]

    for col in numeric_cols:
        price_df[col] = pd.to_numeric(price_df[col], errors='coerce')

    # æŒ‰è‚¡ç¥¨åˆ†ç»„å¡«å……ç¼ºå¤±å€¼
    print("æŒ‰è‚¡ç¥¨å¡«å……ç¼ºå¤±å€¼...")
    for stock_code in tqdm(price_df['stock_code'].unique(), desc="å¡«å……ç¼ºå¤±å€¼"):
        stock_mask = price_df['stock_code'] == stock_code
        for col in numeric_cols:
            if col in price_df.columns:
                # å‰å‘å¡«å……ç„¶ååå‘å¡«å……
                price_df.loc[stock_mask, col] = price_df.loc[stock_mask, col].ffill().bfill()

    # ç§»é™¤ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ
    initial_size = len(price_df)
    price_df = price_df.dropna(subset=numeric_cols)
    print(f"ç§»é™¤ç¼ºå¤±å€¼è¡Œ: {initial_size - len(price_df):,} è¡Œ")

    # å‡å°‘å†…å­˜ä½¿ç”¨
    price_df = reduce_memory_usage(price_df)

    print(f"è‚¡ä»·æ•°æ®å¤„ç†å®Œæˆ!")
    print(f"å¤„ç†åçš„æ•°æ®å½¢çŠ¶: {price_df.shape}")
    print(f"æ—¶é—´èŒƒå›´: {price_df['date'].min()} åˆ° {price_df['date'].max()}")
    print(f"è‚¡ç¥¨æ•°é‡: {price_df['stock_code'].nunique()}")

    # 4. åŠ è½½è´¢æŠ¥æ•°æ®
    financial_df = None
    if os.path.exists(REPORTS_DATA_PATH):
        print(f"\nåŠ è½½è´¢æŠ¥æ•°æ®: {REPORTS_DATA_PATH}")
        try:
            if REPORTS_DATA_PATH.endswith('.csv'):
                financial_df = pd.read_csv(REPORTS_DATA_PATH, encoding='utf-8')
            else:
                financial_df = pd.read_excel(REPORTS_DATA_PATH)

            print(f"è´¢æŠ¥æ•°æ®åŠ è½½æˆåŠŸ: {financial_df.shape}")

            # ä¼˜åŒ–è´¢æŠ¥æ•°æ®å¤„ç†
            if not financial_df.empty:
                print("ä½¿ç”¨ä¼˜åŒ–ç‰ˆå¤„ç†è´¢æŠ¥æ•°æ®...")
                financial_wide = process_financial_data(financial_df)

                if financial_wide is not None and not financial_wide.empty:
                    print("ä½¿ç”¨ä¼˜åŒ–ç‰ˆåˆå¹¶è´¢æŠ¥æ•°æ®...")
                    price_df = merge_financial_data_optimized(price_df, financial_wide)

        except Exception as e:
            print(f"è´¢æŠ¥æ•°æ®åŠ è½½å¤±è´¥: {e}")
            financial_df = None

    # ==================== 5. è°ƒç”¨æŠ€æœ¯æŒ‡æ ‡è®¡ç®— ====================
    print("\nè®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆä½¿ç”¨ä¿®å¤ç‰ˆå‡½æ•°ï¼‰...")

    # éªŒè¯ä»·æ ¼æ•°æ®è´¨é‡
    print("ğŸ” éªŒè¯ä»·æ ¼æ•°æ®è´¨é‡...")
    if not validate_price_data(price_df):
        print("ä»·æ ¼æ•°æ®éªŒè¯å¤±è´¥")
        return None

    # è°ƒç”¨æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å‡½æ•°
    try:
        price_df = calculate_technical_indicators(price_df)
        print(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ!")

        # éªŒè¯æŠ€æœ¯ç‰¹å¾ç”Ÿæˆæƒ…å†µ
        tech_cols = [col for col in price_df.columns
                     if any(pattern in col for pattern in
                            ['ma_', 'ema_', 'volatility_', 'momentum_', 'rsi_',
                             'macd_', 'bb_', 'atr_', 'obv_', 'volume_ratio_',
                             'price_vs_', 'return_', 'log_return', 'price_change'])]

        print(f"ç”ŸæˆæŠ€æœ¯ç‰¹å¾: {len(tech_cols)} ä¸ª")
        if tech_cols:
            print(f"æŠ€æœ¯ç‰¹å¾ç¤ºä¾‹: {tech_cols[:10]}...")

    except Exception as e:
        print(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==================== 6. è®¡ç®—æœªæ¥æ”¶ç›Šç‡å’Œæ ‡ç­¾ ====================
    print("\nè®¡ç®—æœªæ¥æ”¶ç›Šç‡å’Œæ ‡ç­¾ï¼ˆé€‚åˆ20å¤©é¢„æµ‹ï¼‰...")

    # åœ¨è®¡ç®—æœªæ¥æ”¶ç›Šç‡ä¹‹å‰æ·»åŠ éªŒè¯
    print("éªŒè¯ä»·æ ¼æ•°æ®è´¨é‡...")
    if not validate_price_data(price_df):
        print("ä»·æ ¼æ•°æ®éªŒè¯å¤±è´¥")
        return None

    # æœªæ¥æ”¶ç›Šç‡è®¡ç®—å‡½æ•°
    try:
        price_df = calculate_future_returns_and_labels(price_df, days=FUTURE_DAYS)

        if price_df.empty:
            print("è®¡ç®—æœªæ¥æ”¶ç›Šç‡åæ•°æ®ä¸ºç©º")
            return None

        # éªŒè¯æ”¶ç›Šç‡è®¡ç®—
        if 'future_return' in price_df.columns:
            future_returns = price_df['future_return'].dropna()
            print(f"æœªæ¥æ”¶ç›Šç‡è®¡ç®—å®Œæˆ!")
            print(f"æœ‰æ•ˆæ”¶ç›Šç‡æ ·æœ¬: {len(future_returns):,}")
            print(f"æ”¶ç›Šç‡èŒƒå›´: {future_returns.min():.4f} åˆ° {future_returns.max():.4f}")
            print(f"å¹³å‡æ”¶ç›Šç‡: {future_returns.mean():.4f}")

    except Exception as e:
        print(f"âŒ æœªæ¥æ”¶ç›Šç‡è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==================== 7. ç‰¹å¾å·¥ç¨‹ ====================
    print("\nç‰¹å¾å·¥ç¨‹...")
    try:
        price_df, feature_cols = create_features(price_df)

        if price_df is None or len(feature_cols) < 5:
            print("ç‰¹å¾æ•°é‡ä¸è¶³")
            return None

        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ!")
        print(f"æ€»ç‰¹å¾æ•°é‡: {len(feature_cols)} ä¸ª")

        # ç»Ÿè®¡ç‰¹å¾ç±»å‹
        tech_features = [col for col in feature_cols if not col.startswith('fin_')]
        fin_features = [col for col in feature_cols if col.startswith('fin_')]
        other_features = [col for col in feature_cols if col not in tech_features and col not in fin_features]

        print(f"æŠ€æœ¯ç‰¹å¾: {len(tech_features)} ä¸ª")
        print(f"è´¢åŠ¡ç‰¹å¾: {len(fin_features)} ä¸ª")
        print(f"å…¶ä»–ç‰¹å¾: {len(other_features)} ä¸ª")
        print(f"ç‰¹å¾å¹³è¡¡æ¯”ä¾‹: {len(tech_features)}:{len(fin_features)}")

    except Exception as e:
        print(f"ç‰¹å¾å·¥ç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==================== 8. ä¿å­˜é¢„åˆå¹¶æ–‡ä»¶ ====================
    print("\nä¿å­˜é¢„åˆå¹¶æ–‡ä»¶ä¾›åç»­å¿«é€ŸåŠ è½½...")
    try:
        with open(PRE_MERGED_FILE, 'wb') as f:
            pickle.dump((price_df, feature_cols), f, protocol=4)
        print(f"é¢„åˆå¹¶æ•°æ®å·²ä¿å­˜: {PRE_MERGED_FILE}")
        print("ä¸‹æ¬¡è¿è¡Œå°†ç›´æ¥åŠ è½½æ­¤æ–‡ä»¶ï¼Œé€Ÿåº¦æå‡10-100å€ï¼")
    except Exception as e:
        print(f"é¢„åˆå¹¶ä¿å­˜å¤±è´¥: {e}")

    return price_df, feature_cols


def emergency_fix_returns_simple(df, days=FUTURE_DAYS):
    """ä¿®å¤æ”¶ç›Šç‡è®¡ç®— """
    print_section("ä¿®å¤æ”¶ç›Šç‡è®¡ç®—")

    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    df_fixed = df.copy()

    # 1. ç§»é™¤é›¶ä»·æ ¼å’Œæ— æ•ˆæ•°æ®
    print("1. æ¸…ç†æ— æ•ˆæ•°æ®...")
    zero_mask = df_fixed['close'] <= 0
    print(f"   ç§»é™¤é›¶ä»·æ ¼: {zero_mask.sum()} è¡Œ")
    df_fixed = df_fixed[~zero_mask]

    # 2. æŒ‰è‚¡ç¥¨å’Œæ—¥æœŸæ’åº
    df_fixed = df_fixed.sort_values(['stock_code', 'date'])

    # 3. é‡æ–°è®¡ç®—æœªæ¥æ”¶ç›Šç‡
    print("2. é‡æ–°è®¡ç®—æœªæ¥æ”¶ç›Šç‡...")

    def simple_recalculate(group):
        group = group.sort_values('date')
        # ä½¿ç”¨shiftè®¡ç®—æœªæ¥ä»·æ ¼
        group['future_price'] = group['close'].shift(-days)
        # è®¡ç®—æ”¶ç›Šç‡ï¼ˆæ·»åŠ å®‰å…¨æ€§æ£€æŸ¥ï¼‰
        valid_mask = (group['close'] > 0) & (group['future_price'] > 0)
        group['future_return_new'] = np.nan
        group.loc[valid_mask, 'future_return_new'] = (
                group.loc[valid_mask, 'future_price'] / group.loc[valid_mask, 'close'] - 1
        )
        return group

    try:
        df_fixed = df_fixed.groupby('stock_code', group_keys=False).apply(simple_recalculate)
        # ä½¿ç”¨æ–°è®¡ç®—çš„æ”¶ç›Šç‡
        df_fixed['future_return'] = df_fixed['future_return_new']
        print("æ”¶ç›Šç‡é‡æ–°è®¡ç®—å®Œæˆ")
    except Exception as e:
        print(f"åˆ†ç»„è®¡ç®—å¤±è´¥: {e}")
        return df  # å¤±è´¥æ—¶è¿”å›åŸæ•°æ®

    # 4. å¤„ç†ç‰¹æ®Šå€¼
    print("3. å¤„ç†ç‰¹æ®Šå€¼...")
    inf_mask = np.isinf(df_fixed['future_return'])
    if inf_mask.any():
        print(f"   ä¿®å¤ {inf_mask.sum()} ä¸ªinfå€¼...")
        df_fixed.loc[inf_mask, 'future_return'] = np.nan

    # 5. ç§»é™¤æ— æ•ˆè¡Œ
    initial_size = len(df_fixed)
    df_fixed = df_fixed.dropna(subset=['future_return'])
    final_size = len(df_fixed)
    print(f"æœ‰æ•ˆæ•°æ®: {final_size:,}/{initial_size:,} ({final_size / initial_size:.1%})")

    # 6. éªŒè¯ä¿®å¤ç»“æœ
    future_returns = df_fixed['future_return'].dropna()
    if len(future_returns) > 0:
        print(f"ç´§æ€¥ä¿®å¤å®Œæˆ!")
        print(f"æœ‰æ•ˆæ”¶ç›Šç‡: {len(future_returns):,}")
        print(f"èŒƒå›´: {future_returns.min():.6f} åˆ° {future_returns.max():.6f}")
        print(f"å‡å€¼: {future_returns.mean():.6f}")
        print(f"infå€¼: {np.isinf(future_returns).sum()}")
    else:
        print("ç´§æ€¥ä¿®å¤åæ²¡æœ‰æœ‰æ•ˆæ”¶ç›Šç‡!")

    return df_fixed

@timer_decorator
def merge_financial_data_optimized(price_df, financial_df):
    """ä¼˜åŒ–è´¢æŠ¥æ•°æ®åˆå¹¶-ä½¿ç”¨å‘é‡åŒ–æ“ä½œæå‡æ€§èƒ½"""
    if financial_df is None or financial_df.empty:
        return price_df

    print_section("ä¼˜åŒ–åˆå¹¶è´¢æŠ¥æ•°æ®")
    start_time = time.time()

    try:
        # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
        price_df = price_df.copy()
        financial_df = financial_df.copy()

        # ç¡®ä¿è‚¡ç¥¨ä»£ç æ ¼å¼ä¸€è‡´
        price_df['stock_code'] = price_df['stock_code'].astype(str).str.strip()
        financial_df['stock_code'] = financial_df['stock_code'].astype(str).str.strip()

        # å¤„ç†æ—¥æœŸ
        if 'report_date' in financial_df.columns:
            financial_df['report_date'] = pd.to_datetime(financial_df['report_date'])
            # ç§»é™¤æ—¶åŒºä¿¡æ¯
            if hasattr(financial_df['report_date'].dtype, 'tz') and financial_df['report_date'].dtype.tz is not None:
                financial_df['report_date'] = financial_df['report_date'].dt.tz_convert(None)

        # æ‰¾å‡ºå…±åŒè‚¡ç¥¨
        common_stocks = set(price_df['stock_code'].unique()) & set(financial_df['stock_code'].unique())
        print(f"å…±åŒè‚¡ç¥¨æ•°é‡: {len(common_stocks)}")

        if len(common_stocks) == 0:
            print("æ²¡æœ‰å…±åŒè‚¡ç¥¨,ä»…ä½¿ç”¨è‚¡ä»·æ•°æ®")
            return price_df

        # æ–¹æ³•1: ä½¿ç”¨merge_asofè¿›è¡Œå¿«é€Ÿåˆå¹¶(æ€§èƒ½æœ€ä½³)
        try:
            print("ä½¿ç”¨merge_asofè¿›è¡Œå¿«é€Ÿåˆå¹¶...")
            # åªå¤„ç†å…±åŒè‚¡ç¥¨çš„æ•°æ®
            price_common = price_df[price_df['stock_code'].isin(common_stocks)].copy()
            financial_common = financial_df[financial_df['stock_code'].isin(common_stocks)].copy()

            # ä¿®å¤: ç¡®ä¿æ•°æ®æ’åº
            price_common = price_common.sort_values(['stock_code', 'date'])
            financial_common = financial_common.sort_values(['stock_code', 'report_date'])

            # æ£€æŸ¥æ’åºæ˜¯å¦æˆåŠŸ
            print(
                f"ä»·æ ¼æ•°æ®æ’åºæ£€æŸ¥: è‚¡ç¥¨{price_common['stock_code'].iloc[0]}, æ—¥æœŸèŒƒå›´{price_common['date'].min()}åˆ°{price_common['date'].max()}")
            print(
                f"è´¢æŠ¥æ•°æ®æ’åºæ£€æŸ¥: è‚¡ç¥¨{financial_common['stock_code'].iloc[0]}, æ—¥æœŸèŒƒå›´{financial_common['report_date'].min()}åˆ°{financial_common['report_date'].max()}")

            # ä½¿ç”¨merge_asofè¿›è¡Œå¿«é€Ÿè¿‘ä¼¼åˆå¹¶
            merged_df = pd.merge_asof(
                price_common,
                financial_common,
                left_on='date',
                right_on='report_date',
                by='stock_code',
                direction='backward'  # æ‰¾æœ€è¿‘çš„å°äºç­‰äºå½“å‰æ—¥æœŸçš„è´¢æŠ¥
            )

            # å¤„ç†æ²¡æœ‰è´¢æŠ¥æ•°æ®çš„è‚¡ç¥¨
            price_other = price_df[~price_df['stock_code'].isin(common_stocks)].copy()

            # åˆå¹¶æ‰€æœ‰æ•°æ®
            final_merged = pd.concat([merged_df, price_other], ignore_index=True)

            end_time = time.time()
            print(f"merge_asofåˆå¹¶å®Œæˆ! å½¢çŠ¶: {final_merged.shape}")
            print(f"åˆå¹¶æ—¶é—´: {end_time - start_time:.2f}ç§’ (æ¯”åŸæ–¹æ³•å¿«10å€ä»¥ä¸Š)")
            return final_merged

        except Exception as e:
            print(f"merge_asofå¤±è´¥ï¼Œä½¿ç”¨åˆ†ç»„ä¼˜åŒ–æ–¹æ³•: {e}")
            # å›é€€åˆ°åˆ†ç»„ä¼˜åŒ–æ–¹æ³•
            return merge_financial_data_grouped(price_df, financial_df, common_stocks)

    except Exception as e:
        print(f"ä¼˜åŒ–åˆå¹¶å¤±è´¥: {e}")
        return price_df


def merge_financial_data_grouped(price_df, financial_df, common_stocks):
    """ä¼˜åŒ–ç‰ˆåˆ†ç»„åˆå¹¶æ–¹æ³• - æ›¿ä»£åŸæœ‰çš„groupedå‡½æ•°"""
    print("ä½¿ç”¨ä¼˜åŒ–ç‰ˆåˆ†ç»„åˆå¹¶æ–¹æ³•...")
    start_time = time.time()

    # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼åŠ é€Ÿ
    merged_chunks = []

    for stock_code in tqdm(common_stocks, desc="ä¼˜åŒ–åˆå¹¶è´¢æŠ¥"):
        try:
            # è·å–è‚¡ç¥¨æ•°æ®
            stock_prices = price_df[price_df['stock_code'] == stock_code].copy().sort_values('date')
            stock_financials = financial_df[financial_df['stock_code'] == stock_code].sort_values('report_date')

            if stock_financials.empty:
                merged_chunks.append(stock_prices)
                continue

            # ä½¿ç”¨å‘é‡åŒ–æ“ä½œåŠ é€Ÿ
            price_dates = stock_prices['date'].values
            financial_dates = stock_financials['report_date'].values

            # ä½¿ç”¨searchsortedè¿›è¡Œå¿«é€ŸæŸ¥æ‰¾
            indices = np.searchsorted(financial_dates, price_dates, side='right') - 1

            # æ‰¹é‡å¤„ç†
            valid_indices = indices >= 0
            valid_price_indices = np.where(valid_indices)[0]

            if len(valid_price_indices) > 0:
                # æ‰¹é‡å¤„ç†æœ‰æ•ˆç´¢å¼•
                for i in valid_price_indices:
                    idx = indices[i]
                    latest_financial = stock_financials.iloc[idx]
                    price_row = stock_prices.iloc[i:i + 1].copy()

                    # æ·»åŠ è´¢åŠ¡æŒ‡æ ‡ï¼ˆåªæ·»åŠ æ•°å€¼å‹æŒ‡æ ‡ï¼‰
                    for col, value in latest_financial.items():
                        if col not in ['stock_code', 'report_date'] and pd.api.types.is_numeric_dtype(
                                type(value)) and pd.notna(value):
                            price_row[f'fin_{col}'] = value

                    merged_chunks.append(price_row)

                # å¤„ç†æ²¡æœ‰è´¢æŠ¥æ•°æ®çš„æ—¥æœŸ
                invalid_indices = np.where(~valid_indices)[0]
                if len(invalid_indices) > 0:
                    for i in invalid_indices:
                        merged_chunks.append(stock_prices.iloc[i:i + 1])
            else:
                # æ‰€æœ‰æ—¥æœŸéƒ½æ²¡æœ‰è´¢æŠ¥æ•°æ®
                merged_chunks.append(stock_prices)

        except Exception as e:
            print(f"è‚¡ç¥¨ {stock_code} åˆå¹¶å¤±è´¥: {e}")
            # å³ä½¿å¤±è´¥ä¹Ÿæ·»åŠ åŸºç¡€æ•°æ®
            merged_chunks.append(price_df[price_df['stock_code'] == stock_code])

    # åˆå¹¶æ‰€æœ‰å—
    if merged_chunks:
        result_df = pd.concat(merged_chunks, ignore_index=True)
        end_time = time.time()
        print(f"ä¼˜åŒ–åˆ†ç»„åˆå¹¶å®Œæˆ! å½¢çŠ¶: {result_df.shape}")
        print(f"åˆå¹¶æ—¶é—´: {end_time - start_time:.2f}ç§’")
        return result_df

    return price_df

@timer_decorator
def process_financial_data(financial_df):
    """å¤„ç†è´¢æŠ¥æ•°æ®"""
    if financial_df.empty:
        return pd.DataFrame()

    print_section("å¤„ç†è´¢æŠ¥æ•°æ®")

    print(f"è´¢æŠ¥æ•°æ®å½¢çŠ¶: {financial_df.shape}")
    print(f"è´¢æŠ¥åˆ—å: {list(financial_df.columns)}")

    # æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®
    print("\nè´¢æŠ¥æ•°æ®æ ·ä¾‹ï¼ˆå‰5è¡Œï¼‰:")
    print(financial_df.head())

    # åˆ›å»ºå‰¯æœ¬
    df = financial_df.copy()

    # å»é‡
    df = df.drop_duplicates()
    print(f"å»é‡åå½¢çŠ¶: {df.shape}")

    # å¤„ç†è‚¡ç¥¨ä»£ç 
    if 'number' in df.columns:
        df['stock_code'] = df['number'].astype(str).str.strip()
    elif 'symbol' in df.columns:
        df['stock_code'] = df['symbol'].astype(str).str.strip()
    else:
        print("ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºè‚¡ç¥¨ä»£ç ")
        df['stock_code'] = df.iloc[:, 0].astype(str).str.strip()

    print(f"è‚¡ç¥¨æ•°é‡: {df['stock_code'].nunique()}")

    # è´¢åŠ¡æŒ‡æ ‡æ˜ å°„
    financial_mapping = {
        'ç¾é‡‘åŠç´„ç•¶ç¾é‡‘': 'cash',
        'Cash and cash equivalents': 'cash',
        'æµå‹•è³‡ç”¢åˆè¨ˆ': 'current_assets',
        'Total current assets': 'current_assets',
        'è³‡ç”¢ç¸½è¨ˆ': 'total_assets',
        'Total assets': 'total_assets',
        'æµå‹•è² å‚µåˆè¨ˆ': 'current_liabilities',
        'Total current liabilities': 'current_liabilities',
        'è² å‚µåˆè¨ˆ': 'total_liabilities',
        'Total liabilities': 'total_liabilities',
        'è‚¡æ±æ¬Šç›Šåˆè¨ˆ': 'equity',
        'Total equity': 'equity',
        'æ‡‰æ”¶å¸³æ¬¾æ·¨é¡': 'accounts_receivable',
        'Accounts receivable, net': 'accounts_receivable',
        'å­˜è²¨': 'inventory',
        'Current inventories': 'inventory',
        'ç‡Ÿæ¥­æ”¶å…¥åˆè¨ˆ': 'revenue',
        'Total operating revenue': 'revenue',
        'ç‡Ÿæ¥­æˆæœ¬åˆè¨ˆ': 'operating_costs',
        'Total operating costs': 'operating_costs',
        'ç‡Ÿæ¥­æ¯›åˆ©ï¼ˆæ¯›æï¼‰': 'gross_profit',
        'Gross profit (loss)': 'gross_profit',
        'ç‡Ÿæ¥­åˆ©ç›Šï¼ˆæå¤±ï¼‰': 'operating_profit',
        'Operating profit (loss)': 'operating_profit',
        'æœ¬æœŸç¨…å¾Œæ·¨åˆ©ï¼ˆæ·¨æï¼‰': 'net_profit',
        'Profit (loss)': 'net_profit',
        'åŸºæœ¬æ¯è‚¡ç›ˆé¤˜åˆè¨ˆ': 'eps',
        'Total basic earnings per share': 'eps',
        'ç‡Ÿæ¥­æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰': 'operating_cash_flow',
        'Net cash flows from (used in) operating activities': 'operating_cash_flow',
        'æŠ•è³‡æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰': 'investing_cash_flow',
        'Net cash flows from (used in) investing activities': 'investing_cash_flow',
        'ç±Œè³‡æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰': 'financing_cash_flow',
        'Net cash flows from (used in) financing activities': 'financing_cash_flow'
    }

    def map_financial_indicator(key, key_en):
        if pd.isna(key) and pd.isna(key_en):
            return None

        key_str = str(key) if pd.notna(key) else ''
        key_en_str = str(key_en) if pd.notna(key_en) else ''

        # å…ˆå°è¯•ä¸­æ–‡åŒ¹é…
        for chinese_name, std_name in financial_mapping.items():
            if chinese_name in key_str:
                return std_name

        # å†å°è¯•è‹±æ–‡åŒ¹é…
        for english_name, std_name in financial_mapping.items():
            if english_name.lower() in key_en_str.lower():
                return std_name

        return None

    # æŸ¥æ‰¾æŒ‡æ ‡åç§°åˆ—
    indicator_col = None
    for col in ['key', 'key_en', 'indicator', 'account', 'item']:
        if col in df.columns:
            indicator_col = col
            break

    if indicator_col is None:
        print("ä½¿ç”¨ç¬¬ä¸€åˆ—éè‚¡ç¥¨ä»£ç åˆ—ä½œä¸ºæŒ‡æ ‡")
        indicator_col = df.columns[1] if len(df.columns) > 1 else None

    if indicator_col:
        # å¤„ç†æ•°å€¼
        if 'value' in df.columns:
            df['value'] = pd.to_numeric(df['value'], errors='coerce')

        # å¤„ç†æ—¥æœŸ
        if 'year' in df.columns and 'period' in df.columns:
            # å°æ¹¾è´¢æŠ¥æ—¥æœŸé€šå¸¸: Q1(5/15), Q2(8/14), Q3(11/14), Q4(æ¬¡å¹´3/31)
            def get_report_date(row):
                try:
                    year = int(row['year'])
                    period = int(row['period'])

                    if period == 1:  # ç¬¬ä¸€å­£åº¦
                        return pd.Timestamp(f"{year}-05-15")
                    elif period == 2:  # ç¬¬äºŒå­£åº¦
                        return pd.Timestamp(f"{year}-08-14")
                    elif period == 3:  # ç¬¬ä¸‰å­£åº¦
                        return pd.Timestamp(f"{year}-11-14")
                    elif period == 4:  # ç¬¬å››å­£åº¦
                        return pd.Timestamp(f"{year + 1}-03-31")
                    else:
                        return pd.NaT
                except:
                    return pd.NaT

            df['report_date'] = df.apply(get_report_date, axis=1)
        elif 'date' in df.columns:
            df['report_date'] = pd.to_datetime(df['date'], errors='coerce')
        else:
            print("æ— æ³•ç¡®å®šè´¢æŠ¥æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ")
            df['report_date'] = datetime.now()

        # ç§»é™¤æ— æ•ˆæ—¥æœŸ
        df = df[df['report_date'].notna()]

        # è·å–æ˜ å°„åˆ—
        key_col = indicator_col
        key_en_col = None
        for col in ['key_en', 'account_en', 'item_en']:
            if col in df.columns:
                key_en_col = col
                break

        if key_en_col:
            df['mapped_indicator'] = df.apply(
                lambda x: map_financial_indicator(x[key_col], x[key_en_col]), axis=1
            )
        else:
            df['mapped_indicator'] = df[key_col].apply(
                lambda x: map_financial_indicator(x, None)
            )

        # ç»Ÿè®¡æ˜ å°„ç»“æœ
        mapped_count = df['mapped_indicator'].notna().sum()
        print(f"è´¢åŠ¡æŒ‡æ ‡æ˜ å°„æˆåŠŸç‡: {mapped_count / len(df):.2%} ({mapped_count}/{len(df)})")

        if mapped_count > 0:
            # è½¬æ¢ä¸ºå®½è¡¨æ ¼å¼
            financial_wide = df.pivot_table(
                index=['stock_code', 'report_date'],
                columns='mapped_indicator',
                values='value',
                aggfunc='first'
            ).reset_index()

            financial_wide.columns.name = None

            # ç§»é™¤æ—¶åŒºä¿¡æ¯
            if hasattr(financial_wide['report_date'].dtype, 'tz') and financial_wide[
                'report_date'].dtype.tz is not None:
                financial_wide['report_date'] = financial_wide['report_date'].dt.tz_convert(None)

            # è®¡ç®—è´¢åŠ¡æ¯”ç‡
            print("è®¡ç®—è´¢åŠ¡æ¯”ç‡...")

            if all(col in financial_wide.columns for col in ['revenue', 'operating_costs']):
                financial_wide['gross_margin'] = (financial_wide['revenue'] - financial_wide['operating_costs']) / \
                                                 financial_wide['revenue']
                print("  âœ“ è®¡ç®—æ¯›åˆ©ç‡")

            if all(col in financial_wide.columns for col in ['revenue', 'operating_profit']):
                financial_wide['operating_margin'] = financial_wide['operating_profit'] / financial_wide['revenue']
                print("  âœ“ è®¡ç®—è¥ä¸šåˆ©æ¶¦ç‡")

            if all(col in financial_wide.columns for col in ['revenue', 'net_profit']):
                financial_wide['net_margin'] = financial_wide['net_profit'] / financial_wide['revenue']
                print("  âœ“ è®¡ç®—å‡€åˆ©ç‡")

            if all(col in financial_wide.columns for col in ['current_assets', 'current_liabilities']):
                financial_wide['current_ratio'] = financial_wide['current_assets'] / financial_wide[
                    'current_liabilities']
                print("  âœ“ è®¡ç®—æµåŠ¨æ¯”ç‡")

            if all(col in financial_wide.columns for col in ['total_assets', 'total_liabilities']):
                financial_wide['debt_to_assets'] = financial_wide['total_liabilities'] / financial_wide['total_assets']
                financial_wide['equity_ratio'] = 1 - financial_wide['debt_to_assets']
                print("  âœ“ è®¡ç®—èµ„äº§è´Ÿå€ºç‡å’Œæƒç›Šæ¯”ç‡")

            if all(col in financial_wide.columns for col in ['equity', 'net_profit']):
                financial_wide['roe'] = financial_wide['net_profit'] / financial_wide['equity']
                print("  âœ“ è®¡ç®—ROE")

            if all(col in financial_wide.columns for col in ['total_assets', 'net_profit']):
                financial_wide['roa'] = financial_wide['net_profit'] / financial_wide['total_assets']
                print("  âœ“ è®¡ç®—ROA")

            if all(col in financial_wide.columns for col in ['operating_cash_flow', 'total_liabilities']):
                financial_wide['ocf_to_debt'] = financial_wide['operating_cash_flow'] / financial_wide[
                    'total_liabilities']
                print("  âœ“ è®¡ç®—ç»è¥æ´»åŠ¨ç°é‡‘æµ/è´Ÿå€ºæ¯”ç‡")

            if all(col in financial_wide.columns for col in ['operating_cash_flow', 'revenue']):
                financial_wide['ocf_margin'] = financial_wide['operating_cash_flow'] / financial_wide['revenue']
                print("  âœ“ è®¡ç®—ç»è¥æ´»åŠ¨ç°é‡‘æµ/æ”¶å…¥æ¯”ç‡")

            # å¤„ç†ç¼ºå¤±å€¼
            numeric_cols = [col for col in financial_wide.columns
                            if col not in ['stock_code', 'report_date'] and pd.api.types.is_numeric_dtype(
                    financial_wide[col])]

            for col in numeric_cols:
                if col in financial_wide.columns:
                    financial_wide[col] = financial_wide.groupby('stock_code')[col].transform(
                        lambda x: x.ffill().bfill().fillna(x.median())
                    )

            print(f"è´¢æŠ¥å¤„ç†å®Œæˆ: {financial_wide.shape}")
            print(f"æ—¶é—´èŒƒå›´: {financial_wide['report_date'].min()} åˆ° {financial_wide['report_date'].max()}")

            return financial_wide

    print("è´¢æŠ¥æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¿”å›ç©ºDataFrame")
    return pd.DataFrame()


@timer_decorator
def calculate_basic_technical_features(stock_data):
    """æŠ€æœ¯ç‰¹å¾ - ç”Ÿæˆæ›´å¤šæŠ€æœ¯æŒ‡æ ‡"""
    if stock_data.empty or 'close' not in stock_data.columns:
        return stock_data

    stock_data = stock_data.copy()
    close_prices = stock_data['close']

    try:
        # 1. åŸºç¡€ä»·æ ¼å˜åŒ–
        close_shifted = close_prices.shift(1)
        valid_mask = (close_shifted != 0) & close_shifted.notna()

        stock_data['price_change'] = 0.0
        stock_data.loc[valid_mask, 'price_change'] = (close_prices[valid_mask] - close_shifted[valid_mask]) / \
                                                     close_shifted[valid_mask]

        stock_data['log_return'] = 0.0
        stock_data.loc[valid_mask, 'log_return'] = np.log(close_prices[valid_mask] / close_shifted[valid_mask])

        # 2. ä»·æ ¼èŒƒå›´ç‰¹å¾
        if all(col in stock_data.columns for col in ['high', 'low']):
            stock_data['high_low_range'] = stock_data['high'] - stock_data['low']
            stock_data['price_strength'] = (stock_data['close'] - stock_data['low']) / (
                        stock_data['high'] - stock_data['low']).replace(0, 1)

        # 3. ç”Ÿæˆç®€å•ç§»åŠ¨å¹³å‡çº¿
        for window in [3, 5, 10, 20]:
            ma_col = f'ma_{window}'
            stock_data[ma_col] = close_prices.rolling(window=window, min_periods=1).mean()
            stock_data[f'price_vs_ma{window}'] = close_prices / stock_data[ma_col] - 1

        # 4. ç”Ÿæˆç®€å•åŠ¨é‡æŒ‡æ ‡
        for period in [1, 5, 10]:
            momentum_col = f'momentum_{period}d'
            return_col = f'return_{period}d'
            shifted = close_prices.shift(period)
            valid_mask = (shifted != 0) & shifted.notna()
            stock_data[momentum_col] = 0.0
            stock_data[return_col] = 0.0
            stock_data.loc[valid_mask, momentum_col] = (close_prices[valid_mask] - shifted[valid_mask]) / shifted[
                valid_mask]
            stock_data.loc[valid_mask, return_col] = (close_prices[valid_mask] / shifted[valid_mask] - 1)

        # 5. æˆäº¤é‡ç›¸å…³æŒ‡æ ‡
        if 'volume' in stock_data.columns:
            volume = stock_data['volume']
            for window in [5, 10, 20]:
                volume_ma = volume.rolling(window=window, min_periods=1).mean()
                valid_volume_mask = volume_ma != 0
                stock_data[f'volume_ratio_{window}'] = 1.0
                stock_data.loc[valid_volume_mask, f'volume_ratio_{window}'] = volume[valid_volume_mask] / volume_ma[
                    valid_volume_mask]

        print(
            f" åŸºç¡€æŠ€æœ¯ç‰¹å¾è®¡ç®—å®Œæˆï¼Œç”Ÿæˆç‰¹å¾: {len([col for col in stock_data.columns if col not in ['date', 'stock_code', 'close', 'volume']])}ä¸ª")
        return stock_data

    except Exception as e:
        print(f"åŸºç¡€æŠ€æœ¯ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
        return stock_data


# ==================== ä¿®å¤ï¼šå°†å‡½æ•°ç§»å‡ºåµŒå¥— ====================
@timer_decorator
def calculate_technical_indicators(df):
    """ä¿®å¤ç‰ˆæŠ€æœ¯æŒ‡æ ‡è®¡ç®— - ç¡®ä¿ç”Ÿæˆ20+ä¸ªæœ‰æ•ˆæŠ€æœ¯æŒ‡æ ‡"""
    print_section("ä¿®å¤ç‰ˆæŠ€æœ¯æŒ‡æ ‡è®¡ç®—")

    if df.empty or 'close' not in df.columns:
        print("æ•°æ®ä¸ºç©ºæˆ–ç¼ºå°‘closeåˆ—")
        return df

    df_tech = df.copy()
    close_prices = df_tech['close']

    technical_features_generated = 0
    feature_categories = {}

    try:
        # 1. åŸºç¡€ä»·æ ¼å˜åŒ–ç‰¹å¾ (ç¡®ä¿è¿™éƒ¨åˆ†ä¸€å®šèƒ½ç”Ÿæˆ)
        print("1. è®¡ç®—åŸºç¡€ä»·æ ¼å˜åŒ–ç‰¹å¾...")
        close_shifted = close_prices.shift(1)
        valid_mask = (close_shifted > 0) & close_shifted.notna()

        # ä»·æ ¼å˜åŒ–ç‡
        df_tech['price_change'] = 0.0
        df_tech.loc[valid_mask, 'price_change'] = (
                (close_prices[valid_mask] - close_shifted[valid_mask]) / close_shifted[valid_mask]
        )

        # å¯¹æ•°æ”¶ç›Šç‡
        df_tech['log_return'] = 0.0
        df_tech.loc[valid_mask, 'log_return'] = np.log(
            close_prices[valid_mask] / close_shifted[valid_mask]
        )

        technical_features_generated += 2
        feature_categories['price_change'] = 2
        print("ç”Ÿæˆ2ä¸ªåŸºç¡€ä»·æ ¼å˜åŒ–ç‰¹å¾")

    except Exception as e:
        print(f"åŸºç¡€ä»·æ ¼å˜åŒ–ç‰¹å¾å¤±è´¥: {e}")

    # 2. ç§»åŠ¨å¹³å‡çº¿ç³»åˆ— (æ ¸å¿ƒæŒ‡æ ‡)
    try:
        print("2. è®¡ç®—ç§»åŠ¨å¹³å‡çº¿ç³»åˆ—...")
        ma_windows = [3, 5, 8, 10, 13, 20, 30, 50]

        for window in ma_windows:
            try:
                # ç®€å•ç§»åŠ¨å¹³å‡
                ma_col = f'ma_{window}'
                df_tech[ma_col] = close_prices.rolling(
                    window=window, min_periods=max(1, window // 2)
                ).mean()

                # ä»·æ ¼ç›¸å¯¹äºç§»åŠ¨å¹³å‡çš„ä½ç½®
                df_tech[f'price_vs_ma{window}'] = close_prices / df_tech[ma_col] - 1

                # æŒ‡æ•°ç§»åŠ¨å¹³å‡
                ema_col = f'ema_{window}'
                df_tech[ema_col] = close_prices.ewm(
                    span=window, min_periods=max(1, window // 2)
                ).mean()

                df_tech[f'price_vs_ema{window}'] = close_prices / df_tech[ema_col] - 1

                technical_features_generated += 4
                print(f"ç”Ÿæˆçª—å£{window}çš„4ä¸ªç§»åŠ¨å¹³å‡ç‰¹å¾")
            except Exception as e:
                print(f"çª—å£{window}ç§»åŠ¨å¹³å‡è®¡ç®—å¤±è´¥: {e}")
                continue

        feature_categories['moving_averages'] = len(ma_windows) * 4

    except Exception as e:
        print(f"ç§»åŠ¨å¹³å‡çº¿è®¡ç®—å¤±è´¥: {e}")

    # 3. åŠ¨é‡æŒ‡æ ‡ç³»åˆ—
    try:
        print("3. è®¡ç®—åŠ¨é‡æŒ‡æ ‡ç³»åˆ—...")
        momentum_periods = [1, 2, 3, 5, 10, 20]

        for period in momentum_periods:
            try:
                # ç®€å•åŠ¨é‡
                shifted = close_prices.shift(period)
                valid_mask = (shifted > 0) & shifted.notna()

                momentum_col = f'momentum_{period}d'
                return_col = f'return_{period}d'

                df_tech[momentum_col] = 0.0
                df_tech[return_col] = 0.0

                df_tech.loc[valid_mask, momentum_col] = (
                        (close_prices[valid_mask] - shifted[valid_mask]) / shifted[valid_mask]
                )

                df_tech.loc[valid_mask, return_col] = (
                        close_prices[valid_mask] / shifted[valid_mask] - 1
                )

                technical_features_generated += 2
                print(f"ç”Ÿæˆå‘¨æœŸ{period}çš„2ä¸ªåŠ¨é‡ç‰¹å¾")
            except Exception as e:
                print(f"å‘¨æœŸ{period}åŠ¨é‡è®¡ç®—å¤±è´¥: {e}")
                continue

        feature_categories['momentum'] = len(momentum_periods) * 2

    except Exception as e:
        print(f"åŠ¨é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")

    # 4. æ³¢åŠ¨ç‡æŒ‡æ ‡
    try:
        print("4. è®¡ç®—æ³¢åŠ¨ç‡æŒ‡æ ‡...")
        volatility_windows = [5, 10, 20, 30]

        # æ—¥æ”¶ç›Šç‡
        daily_returns = close_prices.pct_change()

        for window in volatility_windows:
            try:
                vol_col = f'volatility_{window}d'
                df_tech[vol_col] = daily_returns.rolling(
                    window=window, min_periods=max(1, window // 2)
                ).std()

                technical_features_generated += 1
                print(f"ç”Ÿæˆçª—å£{window}çš„æ³¢åŠ¨ç‡ç‰¹å¾")
            except Exception as e:
                print(f"çª—å£{window}æ³¢åŠ¨ç‡è®¡ç®—å¤±è´¥: {e}")
                continue

        feature_categories['volatility'] = len(volatility_windows)

    except Exception as e:
        print(f"æ³¢åŠ¨ç‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")

    # 5. æˆäº¤é‡ç›¸å…³æŒ‡æ ‡ (å¦‚æœæœ‰æˆäº¤é‡æ•°æ®)
    if 'volume' in df_tech.columns:
        try:
            print("5. è®¡ç®—æˆäº¤é‡æŒ‡æ ‡...")
            volume = df_tech['volume']
            volume_windows = [5, 10, 20]

            for window in volume_windows:
                try:
                    # æˆäº¤é‡ç§»åŠ¨å¹³å‡
                    vol_ma_col = f'volume_ma_{window}'
                    df_tech[vol_ma_col] = volume.rolling(
                        window=window, min_periods=max(1, window // 2)
                    ).mean()

                    # æˆäº¤é‡æ¯”ç‡
                    df_tech[f'volume_ratio_{window}'] = volume / df_tech[vol_ma_col]

                    technical_features_generated += 2
                    print(f"ç”Ÿæˆçª—å£{window}çš„2ä¸ªæˆäº¤é‡ç‰¹å¾")
                except Exception as e:
                    print(f"çª—å£{window}æˆäº¤é‡è®¡ç®—å¤±è´¥: {e}")
                    continue

            feature_categories['volume'] = len(volume_windows) * 2

        except Exception as e:
            print(f"æˆäº¤é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")

    # 6. RSIæŒ‡æ ‡
    try:
        print("6. è®¡ç®—RSIæŒ‡æ ‡...")
        rsi_periods = [6, 14, 24]

        for period in rsi_periods:
            try:
                delta = close_prices.diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()

                rs = gain / (loss + 1e-10)  # é¿å…é™¤é›¶
                rsi = 100 - (100 / (1 + rs))

                df_tech[f'rsi_{period}'] = rsi
                technical_features_generated += 1
                print(f"ç”Ÿæˆå‘¨æœŸ{period}çš„RSIç‰¹å¾")
            except Exception as e:
                print(f"å‘¨æœŸ{period}RSIè®¡ç®—å¤±è´¥: {e}")
                continue

        feature_categories['rsi'] = len(rsi_periods)

    except Exception as e:
        print(f"RSIè®¡ç®—å¤±è´¥: {e}")

    # 7. ä»·æ ¼ä½ç½®ç‰¹å¾ (éœ€è¦high, low)
    if all(col in df_tech.columns for col in ['high', 'low']):
        try:
            print("7. è®¡ç®—ä»·æ ¼ä½ç½®ç‰¹å¾...")
            high = df_tech['high']
            low = df_tech['low']

            # 1. å½“æ—¥ä»·æ ¼å¼ºåº¦
            print("   a. è®¡ç®—å½“æ—¥ä»·æ ¼å¼ºåº¦...")
            try:
                range_mask = (high != low)
                df_tech['price_strength'] = 0.5
                df_tech.loc[range_mask, 'price_strength'] = (
                        (close_prices[range_mask] - low[range_mask]) /
                        (high[range_mask] - low[range_mask])
                )
                technical_features_generated += 1
                print("ç”Ÿæˆå½“æ—¥ä»·æ ¼å¼ºåº¦ç‰¹å¾")
            except Exception as e:
                print(f"å½“æ—¥ä»·æ ¼å¼ºåº¦è®¡ç®—å¤±è´¥: {e}")

            # 2. ä»·æ ¼åŒºé—´ä½ç½®ï¼ˆ3ä¸ªæ—¶é—´çª—å£ï¼‰
            print("   b. è®¡ç®—ä»·æ ¼åŒºé—´ä½ç½®...")
            windows = [5, 10, 20]
            for window in windows:
                try:
                    # 2.1 è®¡ç®—æ»šåŠ¨çª—å£çš„æœ€é«˜ä»·å’Œæœ€ä½ä»·
                    high_roll = high.rolling(window=window, min_periods=1).max()
                    low_roll = low.rolling(window=window, min_periods=1).min()

                    # 2.2 åˆ›å»ºæœ‰æ•ˆæ©ç ï¼ˆé¿å…é™¤é›¶ï¼‰
                    range_mask = (high_roll != low_roll)

                    # 2.3 è®¾ç½®é»˜è®¤å€¼
                    df_tech[f'price_position_{window}'] = 0.5

                    # 2.4 è®¡ç®—ä»·æ ¼ä½ç½®
                    df_tech.loc[range_mask, f'price_position_{window}'] = (
                            (close_prices[range_mask] - low_roll[range_mask]) /
                            (high_roll[range_mask] - low_roll[range_mask])
                    )

                    # 2.5 æ›´æ–°è®¡æ•°å™¨
                    technical_features_generated += 1
                    feature_categories.setdefault('price_position', 0)
                    feature_categories['price_position'] += 1

                    print(f" ç”Ÿæˆçª—å£{window}çš„ä»·æ ¼ä½ç½®ç‰¹å¾")
                except Exception as e:
                    print(f"çª—å£{window}ä»·æ ¼ä½ç½®è®¡ç®—å¤±è´¥: {e}")
                    continue

            print(f"ä»·æ ¼ä½ç½®ç‰¹å¾è®¡ç®—å®Œæˆ: æ€»è®¡{feature_categories.get('price_position', 0)}ä¸ªç‰¹å¾")

        except Exception as e:
            print(f"ä»·æ ¼ä½ç½®ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")

    # æœ€ç»ˆç»Ÿè®¡
    print_section("æŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆç»Ÿè®¡")
    print(f"æ€»ç”ŸæˆæŠ€æœ¯ç‰¹å¾: {technical_features_generated}ä¸ª")
    for category, count in feature_categories.items():
        print(f"  {category}: {count}ä¸ª")

    # éªŒè¯ç”Ÿæˆçš„ç‰¹å¾
    tech_cols = [col for col in df_tech.columns
                 if any(pattern in col for pattern in
                        ['ma_', 'ema_', 'momentum_', 'return_', 'volatility_',
                         'volume_', 'rsi_', 'price_', 'change_'])]

    print(f"å®é™…æŠ€æœ¯ç‰¹å¾åˆ—: {len(tech_cols)}ä¸ª")

    # æ£€æŸ¥ç‰¹å¾æœ‰æ•ˆæ€§
    valid_tech_cols = []
    for col in tech_cols:
        if col in df_tech.columns:
            non_na_ratio = df_tech[col].notna().mean()
            unique_vals = df_tech[col].nunique()
            if non_na_ratio > 0.5 and unique_vals > 1:
                valid_tech_cols.append(col)

    print(f"æœ‰æ•ˆæŠ€æœ¯ç‰¹å¾(éç©º>50%, å”¯ä¸€å€¼>1): {len(valid_tech_cols)}ä¸ª")

    if len(valid_tech_cols) < 15:
        print("æŠ€æœ¯ç‰¹å¾ä¸è¶³ï¼Œæ‰§è¡Œç´§æ€¥å¢å¼º...")
        df_tech = emergency_enhance_technical_features(df_tech)

    return df_tech


def emergency_enhance_technical_features(df):
    """ç´§æ€¥å¢å¼ºæŠ€æœ¯ç‰¹å¾ - å½“å¸¸è§„æ–¹æ³•å¤±è´¥æ—¶ä½¿ç”¨"""
    print("æ‰§è¡Œç´§æ€¥æŠ€æœ¯ç‰¹å¾å¢å¼º...")

    if 'close' not in df.columns:
        return df

    close_prices = df['close']
    enhanced_features = []

    # 1. åŸºç¡€æ¯”ç‡ç‰¹å¾
    try:
        if 'open' in df.columns:
            df['open_close_ratio'] = df['close'] / df['open'] - 1
            enhanced_features.append('open_close_ratio')

        if all(col in df.columns for col in ['high', 'low']):
            df['price_intensity'] = (df['close'] - df['low']) / (df['high'] - df['low']).replace(0, 1)
            df['daily_range_pct'] = (df['high'] - df['low']) / df['close']
            enhanced_features.extend(['price_intensity', 'daily_range_pct'])
    except Exception as e:
        print(f"åŸºç¡€æ¯”ç‡ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")

    # 2. ç®€å•ç§»åŠ¨å¹³å‡çº¿
    try:
        for window in [3, 5, 8, 13, 21, 34, 55]:  # æ–æ³¢é‚£å¥‘æ•°åˆ—çª—å£
            ma_col = f'emergency_ma_{window}'
            df[ma_col] = close_prices.rolling(window=window, min_periods=1).mean()
            enhanced_features.append(ma_col)
    except Exception as e:
        print(f"ç®€å•ç§»åŠ¨å¹³å‡çº¿è®¡ç®—å¤±è´¥: {e}")

    # 3. ç®€å•åŠ¨é‡æŒ‡æ ‡
    try:
        for period in [1, 2, 3, 5, 8, 13]:
            mom_col = f'emergency_mom_{period}'
            df[mom_col] = close_prices.pct_change(period)
            enhanced_features.append(mom_col)
    except Exception as e:
        print(f"ç®€å•åŠ¨é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")

    # 4. ä»·æ ¼ä½ç½®ç‰¹å¾
    try:
        for window in [5, 10, 20]:
            high_col = f'emergency_high_{window}'
            low_col = f'emergency_low_{window}'
            df[high_col] = close_prices.rolling(window=window).max()
            df[low_col] = close_prices.rolling(window=window).min()
            df[f'emergency_position_{window}'] = (close_prices - df[low_col]) / (df[high_col] - df[low_col]).replace(0,
                                                                                                                     1)
            enhanced_features.extend([high_col, low_col, f'emergency_position_{window}'])
    except Exception as e:
        print(f"ä»·æ ¼ä½ç½®ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")

    print(f"ç´§æ€¥å¢å¼ºå®Œæˆ: {len(enhanced_features)}ä¸ªç‰¹å¾")
    return df
# ==================== è¾…åŠ©å‡½æ•°ä¿æŒä¸å˜ ====================
def calculate_enhanced_moving_averages(stock_data):
    """ç§»åŠ¨å¹³å‡çº¿è®¡ç®—"""
    if 'close' not in stock_data.columns:
        return stock_data

    close_prices = stock_data['close']

    # æ‰©å±•çª—å£èŒƒå›´
    key_windows = [3, 5, 10, 20, 30, 60]
    for window in key_windows:
        # ç®€å•ç§»åŠ¨å¹³å‡
        ma_col = f'ma_{window}'
        stock_data[ma_col] = close_prices.rolling(window=window, min_periods=1).mean()
        stock_data[f'price_vs_ma{window}'] = close_prices / stock_data[ma_col] - 1

        # æŒ‡æ•°ç§»åŠ¨å¹³å‡
        ema_col = f'ema_{window}'
        stock_data[ema_col] = close_prices.ewm(span=window, min_periods=1).mean()
        stock_data[f'price_vs_ema{window}'] = close_prices / stock_data[ema_col] - 1

    return stock_data


def calculate_enhanced_momentum_indicators(stock_data):
    """åŠ¨é‡æŒ‡æ ‡è®¡ç®—"""
    if 'close' not in stock_data.columns:
        return stock_data

    close_prices = stock_data['close']

    try:
        # ä»·æ ¼åŠ¨é‡
        for period in [1, 5, 10, 20]:
            momentum_col = f'momentum_{period}d'
            return_col = f'return_{period}d'
            shifted = close_prices.shift(period)
            valid_mask = (shifted != 0) & shifted.notna()
            stock_data[momentum_col] = 0.0
            stock_data[return_col] = 0.0
            stock_data.loc[valid_mask, momentum_col] = (close_prices[valid_mask] - shifted[valid_mask]) / shifted[
                valid_mask]
            stock_data.loc[valid_mask, return_col] = (close_prices[valid_mask] / shifted[valid_mask] - 1)

        # RSIæŒ‡æ ‡
        period = 14
        delta = close_prices.diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        avg_gain = gain.rolling(window=period, min_periods=1).mean()
        avg_loss = loss.rolling(window=period, min_periods=1).mean()
        rs = avg_gain / avg_loss.replace(0, 1)
        rs = rs.replace([np.inf, -np.inf], 1).fillna(1)
        stock_data[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        return stock_data
    except Exception as e:
        print(f"åŠ¨é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        return stock_data

def calculate_enhanced_volatility_indicators(stock_data):
    """æ³¢åŠ¨ç‡æŒ‡æ ‡è®¡ç®—"""
    if 'close' not in stock_data.columns:
        return stock_data

    close_prices = stock_data['close']
    high_prices = stock_data['high'] if 'high' in stock_data.columns else stock_data['close']
    low_prices = stock_data['low'] if 'low' in stock_data.columns else stock_data['close']

    try:
        # æ³¢åŠ¨ç‡
        window = 20
        close_shifted = close_prices.shift(1)
        valid_mask = (close_shifted != 0) & close_shifted.notna()
        daily_returns = np.zeros(len(close_prices))
        daily_returns[valid_mask] = (close_prices[valid_mask] - close_shifted[valid_mask]) / close_shifted[valid_mask]
        stock_data[f'volatility_{window}d'] = pd.Series(daily_returns).rolling(window=window, min_periods=1).std()

        # å¸ƒæ—å¸¦ä½ç½®
        if 'ma_20' in stock_data.columns:
            ma_20 = stock_data['ma_20']
            std_20 = close_prices.rolling(window=20, min_periods=1).std()
            bb_upper = ma_20 + 2 * std_20
            bb_lower = ma_20 - 2 * std_20
            bb_width = bb_upper - bb_lower
            valid_bb_mask = bb_width != 0
            stock_data['bb_position_20'] = 0.5
            stock_data.loc[valid_bb_mask, 'bb_position_20'] = (close_prices[valid_bb_mask] - bb_lower[valid_bb_mask]) / \
                                                              bb_width[valid_bb_mask]

        return stock_data
    except Exception as e:
        print(f"æ³¢åŠ¨ç‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        return stock_data


def calculate_enhanced_volume_indicators(stock_data):
    """æˆäº¤é‡æŒ‡æ ‡è®¡ç®—"""
    if 'volume' not in stock_data.columns:
        return stock_data

    volume = stock_data['volume']
    close_prices = stock_data['close']

    try:
        # æˆäº¤é‡æ¯”ç‡
        window = 20
        volume_ma = volume.rolling(window=window, min_periods=1).mean()
        valid_volume_mask = volume_ma != 0
        stock_data['volume_ratio_20'] = 1.0
        stock_data.loc[valid_volume_mask, 'volume_ratio_20'] = volume[valid_volume_mask] / volume_ma[valid_volume_mask]

        return stock_data
    except Exception as e:
        print(f"æˆäº¤é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        return stock_data


@timer_decorator
def calculate_future_returns_and_labels(df, days=FUTURE_DAYS):
    """æœªæ¥æ”¶ç›Šç‡è®¡ç®— """
    print_section("æ”¶ç›Šç‡è®¡ç®—")

    if df.empty or 'close' not in df.columns:
        print("æ•°æ®ä¸ºç©ºæˆ–ç¼ºå°‘closeåˆ—")
        return df

    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    df_fixed = df.copy()
    df_fixed = df_fixed.sort_values(['stock_code', 'date'])

    print(f"ä½¿ç”¨æ”¶ç›Šç‡è®¡ç®—ï¼Œé¢„æœŸé—´éš”: {days}ä¸ªäº¤æ˜“æ—¥")
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df_fixed.shape}")
    print(f"è‚¡ç¥¨æ•°é‡: {df_fixed['stock_code'].nunique()}")

    # ==================== å…³é”®ä¿®å¤å¼€å§‹ ====================
    def safe_calculate_returns(group):
        """å®‰å…¨è®¡ç®—æ”¶ç›Šç‡ - é¿å…infå’Œé™¤é›¶é”™è¯¯"""
        group = group.sort_values('date')

        if len(group) < days + 1:
            group['future_return'] = np.nan
            return group

        close_prices = group['close'].values
        returns = np.full(len(group), np.nan)

        for i in range(len(group)):
            if i + days < len(group):
                current_price = close_prices[i]
                future_price = close_prices[i + days]

                #å…³é”®ä¿®å¤ï¼šä¸¥æ ¼çš„ä»·æ ¼æœ‰æ•ˆæ€§æ£€æŸ¥
                if (current_price > 0 and future_price > 0 and
                        not np.isnan(current_price) and not np.isnan(future_price) and
                        not np.isinf(current_price) and not np.isinf(future_price)):

                    # è®¡ç®—æ”¶ç›Šç‡
                    return_val = (future_price / current_price) - 1

                    # ä¿®å¤ï¼šé™åˆ¶æ”¶ç›Šç‡èŒƒå›´ï¼Œé¿å…æç«¯å€¼
                    if return_val < -0.9:  # é™åˆ¶æœ€å¤§äºæŸ90%
                        return_val = -0.9
                    elif return_val > 10.0:  # é™åˆ¶æœ€å¤§æ”¶ç›Š1000%
                        return_val = 10.0

                    # ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰é™å€¼
                    if np.isfinite(return_val):
                        returns[i] = return_val

        group['future_return'] = returns
        return group

    # åº”ç”¨ä¿®å¤è®¡ç®—
    print("åº”ç”¨åˆ†ç»„è®¡ç®—æ”¶ç›Šç‡...")
    df_fixed = df_fixed.groupby('stock_code', group_keys=False).apply(safe_calculate_returns)

    # ç§»é™¤æ— æ•ˆè¡Œ
    initial_size = len(df_fixed)
    df_fixed = df_fixed.dropna(subset=['future_return'])
    removed_count = initial_size - len(df_fixed)
    print(f"æ”¶ç›Šç‡è®¡ç®—å®Œæˆï¼Œç§»é™¤æ— æ•ˆæ•°æ®: {removed_count:,}è¡Œ")

    # ä¿®å¤ï¼šé¢å¤–æ£€æŸ¥å¹¶å¤„ç†æ— ç©·å€¼
    if 'future_return' in df_fixed.columns:
        future_returns = df_fixed['future_return']

        # æ£€æŸ¥æ— ç©·å¤§å€¼
        inf_mask = np.isinf(future_returns)
        inf_count = inf_mask.sum()

        if inf_count > 0:
            print(f"å‘ç°æ— ç©·å¤§æ”¶ç›Šç‡: {inf_count}ä¸ªï¼Œå°†å…¶è®¾ç½®ä¸ºNaN")
            df_fixed.loc[inf_mask, 'future_return'] = np.nan

        # æ£€æŸ¥NaNå€¼
        nan_count = future_returns.isna().sum()
        if nan_count > 0:
            print(f"ç§»é™¤NaNæ”¶ç›Šç‡: {nan_count}ä¸ª")
            df_fixed = df_fixed.dropna(subset=['future_return'])

    # éªŒè¯ä¿®å¤ç»“æœ
    if 'future_return' in df_fixed.columns and len(df_fixed) > 0:
        future_returns = df_fixed['future_return'].dropna()

        if len(future_returns) > 0:
            print(f"å½»åº•ä¿®å¤åæ”¶ç›Šç‡ç»Ÿè®¡:")
            print(f"æœ‰æ•ˆæ ·æœ¬: {len(future_returns):,}")
            print(f"èŒƒå›´: {future_returns.min():.6f} åˆ° {future_returns.max():.6f}")
            print(f"å‡å€¼: {future_returns.mean():.6f}")

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ— æ•ˆå€¼
            if np.isinf(future_returns).any() or np.isnan(future_returns).any():
                print("ä»ç„¶å­˜åœ¨æ— æ•ˆæ”¶ç›Šç‡ï¼Œè¿›è¡Œç´§æ€¥å¤„ç†...")
                median_return = future_returns.replace([np.inf, -np.inf], np.nan).median()
                df_fixed['future_return'] = df_fixed['future_return'].replace(
                    [np.inf, -np.inf], median_return
                )
        else:
            print("ä¿®å¤åæ²¡æœ‰æœ‰æ•ˆæ”¶ç›Šç‡ï¼")

    # ==================== æ ‡ç­¾è®¡ç®—éƒ¨åˆ† ====================
    print("è®¡ç®—å¸‚åœºå¹³å‡æ”¶ç›Šç‡å’Œæ ‡ç­¾...")

    # è®¡ç®—å¸‚åœºå¹³å‡æ”¶ç›Šç‡
    daily_avg_return = df_fixed.groupby('date')['future_return'].mean().reset_index()
    daily_avg_return.columns = ['date', 'market_avg_return']
    df_fixed = pd.merge(df_fixed, daily_avg_return, on='date', how='left')

    # ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•å®šä¹‰æ ‡ç­¾ï¼ˆæ›´ç¨³å¥ï¼‰
    def calculate_smart_labels(group):
        if len(group) < 10:
            group['label'] = 0
            return group

        future_returns = group['future_return']

        # æ–¹æ³•1ï¼šä½¿ç”¨åˆ†ä½æ•°
        try:
            quantile_threshold = future_returns.quantile(0.6)  # å‰40%ä¸ºæ­£æ ·æœ¬
            group['label'] = (future_returns > quantile_threshold).astype(int)
        except:
            # å›é€€æ–¹æ³•ï¼šä½¿ç”¨å¸‚åœºå¹³å‡
            market_avg = group['market_avg_return'].mean()
            group['label'] = (future_returns > market_avg).astype(int)

        return group

    df_fixed = df_fixed.groupby('date', group_keys=False).apply(calculate_smart_labels)

    # éªŒè¯æ ‡ç­¾æœ‰æ•ˆæ€§
    print("éªŒè¯æ ‡ç­¾æœ‰æ•ˆæ€§...")
    if 'label' in df_fixed.columns and 'future_return' in df_fixed.columns:
        positive_mask = df_fixed['label'] == 1
        negative_mask = df_fixed['label'] == 0

        if positive_mask.any() and negative_mask.any():
            positive_return = df_fixed[positive_mask]['future_return'].mean()
            negative_return = df_fixed[negative_mask]['future_return'].mean()
            return_diff = positive_return - negative_return

            print("âœ… æ ‡ç­¾æœ‰æ•ˆæ€§éªŒè¯:")
            print(f"  æ­£æ ·æœ¬å¹³å‡æ”¶ç›Š: {positive_return:.6f} ({positive_return:.4%})")
            print(f"  è´Ÿæ ·æœ¬å¹³å‡æ”¶ç›Š: {negative_return:.6f} ({negative_return:.4%})")
            print(f"  æ”¶ç›Šå·®å¼‚: {return_diff:.6f} ({return_diff:.4%})")
            print(f"  æ­£æ ·æœ¬æ¯”ä¾‹: {df_fixed['label'].mean():.2%}")

            if return_diff < 0.01:
                print("âŒ æ ‡ç­¾åŒºåˆ†åº¦ä¸è¶³ï¼Œå°è¯•è°ƒæ•´...")
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„åˆ†ä½æ•°
                try:
                    df_fixed = df_fixed.groupby('date', group_keys=False).apply(
                        lambda x: x.assign(label=(x['future_return'] > x['future_return'].quantile(0.7)).astype(int))
                    )
                    # é‡æ–°éªŒè¯
                    positive_return = df_fixed[df_fixed['label'] == 1]['future_return'].mean()
                    negative_return = df_fixed[df_fixed['label'] == 0]['future_return'].mean()
                    return_diff = positive_return - negative_return
                    print(f"è°ƒæ•´åæ”¶ç›Šå·®å¼‚: {return_diff:.4f} ({return_diff:.2%})")
                except Exception as e:
                    print(f"è°ƒæ•´å¤±è´¥: {e}")
        else:
            print("âŒ æ— æ³•éªŒè¯æ ‡ç­¾æœ‰æ•ˆæ€§ï¼šç¼ºå°‘æ­£æ ·æœ¬æˆ–è´Ÿæ ·æœ¬")

    print(f"æ ‡ç­¾è®¡ç®—å®Œæˆ! æ­£æ ·æœ¬æ¯”ä¾‹: {df_fixed['label'].mean():.2%}")
    return df_fixed


def filter_financial_features_by_importance(df, financial_features, target_count):
    """ç­›é€‰è´¢åŠ¡ç‰¹å¾"""
    if len(financial_features) <= target_count:
        return financial_features

    print(f"ç­›é€‰è´¢åŠ¡ç‰¹å¾: {len(financial_features)} -> {target_count}ä¸ª")

    financial_features_filtered = []

    # æ–¹æ³•1ï¼šä½¿ç”¨ä¸labelçš„ç›¸å…³æ€§è¿›è¡Œç­›é€‰
    if 'label' in df.columns:
        financial_correlations = []
        for col in financial_features:
            try:
                if df[col].notna().sum() > 100:
                    corr = abs(df[col].corr(df['label']))
                    if not np.isnan(corr):
                        financial_correlations.append((col, corr))
            except:
                continue

        if financial_correlations:
            financial_correlations.sort(key=lambda x: x[1], reverse=True)
            selected_financial = [col for col, corr in financial_correlations[:target_count]]
            financial_features_filtered = selected_financial
            print(f"åŸºäºç›¸å…³æ€§ç­›é€‰: {len(selected_financial)}ä¸ªè´¢åŠ¡ç‰¹å¾")
        else:
            financial_features_filtered = financial_features[:target_count]
            print(f"ä½¿ç”¨ç®€å•æˆªå–: {len(financial_features_filtered)}ä¸ªè´¢åŠ¡ç‰¹å¾")
    else:
        # å¦‚æœæ²¡æœ‰labelï¼Œä½¿ç”¨æ–¹å·®ç­›é€‰
        financial_variances = []
        for col in financial_features:
            try:
                variance = df[col].var()
                if not np.isnan(variance):
                    financial_variances.append((col, variance))
            except:
                continue

        if financial_variances:
            financial_variances.sort(key=lambda x: x[1], reverse=True)
            financial_features_filtered = [col for col, var in financial_variances[:target_count]]
            print(f"åŸºäºæ–¹å·®ç­›é€‰: {len(financial_features_filtered)}ä¸ªè´¢åŠ¡ç‰¹å¾")
        else:
            financial_features_filtered = financial_features[:target_count]
            print(f"ä½¿ç”¨ç®€å•æˆªå–: {len(financial_features_filtered)}ä¸ªè´¢åŠ¡ç‰¹å¾")

    return financial_features_filtered


@timer_decorator
def create_features(df):
    """ç‰¹å¾å·¥ç¨‹ - ç¡®ä¿æŠ€æœ¯ç‰¹å¾å’Œè´¢åŠ¡ç‰¹å¾å¹³è¡¡"""
    print_section("ç‰¹å¾å¹³è¡¡ä¼˜åŒ–")

    if df.empty:
        return df, []

    # åŸºç¡€åˆ—ï¼ˆä¸åŒ…å«åœ¨ç‰¹å¾ä¸­ï¼‰
    base_cols = ['date', 'stock_code', 'close', 'volume', 'open', 'high', 'low',
                 'future_return', 'market_avg_return', 'label']

    # 1. æ”¶é›†æ‰€æœ‰æ•°å€¼å‹ç‰¹å¾
    all_numeric_cols = []
    for col in df.columns:
        if (col not in base_cols and
                pd.api.types.is_numeric_dtype(df[col]) and
                df[col].nunique() > 1 and
                df[col].notna().mean() > 0.3):  # é™ä½éç©ºé˜ˆå€¼åˆ°30%
            all_numeric_cols.append(col)

    print(f"æ‰€æœ‰æ•°å€¼å‹ç‰¹å¾: {len(all_numeric_cols)}ä¸ª")

    if len(all_numeric_cols) == 0:
        print("æ²¡æœ‰æ‰¾åˆ°æ•°å€¼å‹ç‰¹å¾")
        return df, []

    # 2. é‡æ–°å®šä¹‰ç‰¹å¾åˆ†ç±»æ¨¡å¼ - æ›´å…¨é¢çš„åŒ¹é…
    tech_patterns = [
        'ma_', 'ema_', 'volatility_', 'momentum_', 'rsi_', 'macd_', 'bb_', 'atr_', 'obv_',
        'volume_ratio_', 'price_vs_', 'return_', 'log_return', 'price_change', 'change_',
        'breakout_', 'strength_', 'position_', 'ratio_', 'signal_', 'index_', 'oscillator_'
    ]

    # 3. åˆ†ç±»ç‰¹å¾
    tech_features = []
    financial_features = []
    other_features = []

    for col in all_numeric_cols:
        # ä¼˜å…ˆè¯†åˆ«è´¢åŠ¡ç‰¹å¾
        if any(col.startswith(pattern) for pattern in ['fin_', 'financial_']):
            financial_features.append(col)
        # è¯†åˆ«æŠ€æœ¯ç‰¹å¾
        elif any(pattern in col for pattern in tech_patterns):
            tech_features.append(col)
        # è¯†åˆ«å…¶ä»–è´¢åŠ¡ç‰¹å¾ï¼ˆåŸºäºå…³é”®è¯ï¼‰
        elif any(keyword in col.lower() for keyword in
                 ['cash', 'asset', 'liability', 'equity', 'revenue', 'profit',
                  'margin', 'debt', 'flow', 'eps', 'roe', 'roa']):
            financial_features.append(col)
        else:
            other_features.append(col)

    print(f"åˆå§‹ç‰¹å¾ç»Ÿè®¡:")
    print(f"æŠ€æœ¯ç‰¹å¾: {len(tech_features)}ä¸ª")
    print(f"è´¢åŠ¡ç‰¹å¾: {len(financial_features)}ä¸ª")
    print(f"å…¶ä»–ç‰¹å¾: {len(other_features)}ä¸ª")

    # 4. ç›®æ ‡å¹³è¡¡æ¯”ä¾‹
    target_tech = 25  # æŠ€æœ¯ç‰¹å¾ç›®æ ‡
    target_fin = 35  # è´¢åŠ¡ç‰¹å¾ç›®æ ‡

    # 5. å¦‚æœæŠ€æœ¯ç‰¹å¾ä¸è¶³ï¼Œæ‰§è¡Œç´§æ€¥å¢å¼º
    if len(tech_features) < target_tech:
        print(f"æŠ€æœ¯ç‰¹å¾ä¸è¶³({len(tech_features)}ä¸ª)ï¼Œæ‰§è¡Œç´§æ€¥å¢å¼º...")
        df = emergency_enhance_technical_features(df)

        # é‡æ–°æ”¶é›†ç‰¹å¾
        all_numeric_cols = []
        for col in df.columns:
            if (col not in base_cols and
                    pd.api.types.is_numeric_dtype(df[col]) and
                    df[col].nunique() > 1 and
                    df[col].notna().mean() > 0.3):
                all_numeric_cols.append(col)

        # é‡æ–°åˆ†ç±»
        tech_features = []
        financial_features = []
        other_features = []

        for col in all_numeric_cols:
            if any(col.startswith(pattern) for pattern in ['fin_', 'financial_']):
                financial_features.append(col)
            elif any(pattern in col for pattern in tech_patterns):
                tech_features.append(col)
            elif any(keyword in col.lower() for keyword in
                     ['cash', 'asset', 'liability', 'equity', 'revenue', 'profit',
                      'margin', 'debt', 'flow', 'eps', 'roe', 'roa']):
                financial_features.append(col)
            else:
                other_features.append(col)

        print(f"å¢å¼ºåç‰¹å¾ç»Ÿè®¡:")
        print(f" æŠ€æœ¯ç‰¹å¾: {len(tech_features)}ä¸ª")
        print(f" è´¢åŠ¡ç‰¹å¾: {len(financial_features)}ä¸ª")
        print(f" å…¶ä»–ç‰¹å¾: {len(other_features)}ä¸ª")

    # 6. ç®€åŒ–å¹³è¡¡ç­–ç•¥
    print("æ‰§è¡Œç®€åŒ–å¹³è¡¡ç­–ç•¥...")

    # 6.1 å¦‚æœæŠ€æœ¯ç‰¹å¾ä»ç„¶ä¸è¶³ï¼Œä»å…¶ä»–ç‰¹å¾ä¸­å€Ÿç”¨
    if len(tech_features) < target_tech and len(other_features) > 0:
        print(f"æŠ€æœ¯ç‰¹å¾ä»ä¸è¶³({len(tech_features)}ä¸ª)ï¼Œä»å…¶ä»–ç‰¹å¾ä¸­å€Ÿç”¨...")

        # è®¡ç®—å…¶ä»–ç‰¹å¾ä¸æ ‡ç­¾çš„ç›¸å…³æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        correlations = []
        if 'label' in df.columns:
            for col in other_features:
                try:
                    if df[col].notna().sum() > 50:  # é™ä½æ ·æœ¬æ•°é‡è¦æ±‚
                        corr = abs(df[col].corr(df['label']))
                        if not np.isnan(corr):
                            correlations.append((col, corr))
                except:
                    continue

            if correlations:
                correlations.sort(key=lambda x: x[1], reverse=True)
                # å€Ÿç”¨ç›¸å…³æ€§æœ€é«˜çš„ç‰¹å¾
                borrow_count = min(target_tech - len(tech_features), len(correlations), 10)  # æœ€å¤šå€Ÿ10ä¸ª
                borrowed_features = [col for col, corr in correlations[:borrow_count]]
                tech_features.extend(borrowed_features)
                # ä»å…¶ä»–ç‰¹å¾ä¸­ç§»é™¤
                other_features = [col for col in other_features if col not in borrowed_features]
                print(f"  å€Ÿç”¨ {len(borrowed_features)} ä¸ªé«˜ç›¸å…³æ€§ç‰¹å¾ç»™æŠ€æœ¯ç‰¹å¾")

    # 6.2 å¦‚æœè´¢åŠ¡ç‰¹å¾è¿‡å¤šï¼Œè¿›è¡Œç­›é€‰
    if len(financial_features) > target_fin:
        print(f"è´¢åŠ¡ç‰¹å¾è¿‡å¤š({len(financial_features)}ä¸ª)ï¼Œè¿›è¡Œç­›é€‰...")

        # ä½¿ç”¨ç›¸å…³æ€§ç­›é€‰
        fin_correlations = []
        if 'label' in df.columns:
            for col in financial_features:
                try:
                    if df[col].notna().sum() > 50:  # é™ä½æ ·æœ¬æ•°é‡è¦æ±‚
                        corr = abs(df[col].corr(df['label']))
                        if not np.isnan(corr):
                            fin_correlations.append((col, corr))
                except:
                    continue

            if fin_correlations:
                fin_correlations.sort(key=lambda x: x[1], reverse=True)
                financial_features = [col for col, corr in fin_correlations[:target_fin]]
                print(f"  åŸºäºç›¸å…³æ€§ç­›é€‰åˆ° {len(financial_features)} ä¸ªè´¢åŠ¡ç‰¹å¾")
            else:
                # ä½¿ç”¨æ–¹å·®ç­›é€‰
                variances = []
                for col in financial_features:
                    try:
                        variance = df[col].var()
                        if not np.isnan(variance):
                            variances.append((col, variance))
                    except:
                        continue

                if variances:
                    variances.sort(key=lambda x: x[1], reverse=True)
                    financial_features = [col for col, var in variances[:target_fin]]
                    print(f"  åŸºäºæ–¹å·®ç­›é€‰åˆ° {len(financial_features)} ä¸ªè´¢åŠ¡ç‰¹å¾")
                else:
                    # ç®€å•æˆªå–
                    financial_features = financial_features[:target_fin]
                    print(f"  ç®€å•æˆªå–åˆ° {len(financial_features)} ä¸ªè´¢åŠ¡ç‰¹å¾")

    # 6.3 æœ€ç»ˆç‰¹å¾åˆå¹¶
    selected_features = tech_features + financial_features

    # ç¡®ä¿ç‰¹å¾æ•°é‡åœ¨åˆç†èŒƒå›´å†…
    total_target = target_tech + target_fin
    if len(selected_features) > total_target * 1.5:
        print(f"ç‰¹å¾æ•°é‡è¶…é¢({len(selected_features)}ä¸ª)ï¼Œè¿›è¡Œæœ€ç»ˆç²¾ç®€...")
        # ä¼˜å…ˆä¿ç•™æŠ€æœ¯ç‰¹å¾
        tech_keep = min(len(tech_features), int(total_target * 0.4))
        fin_keep = min(len(financial_features), total_target - tech_keep)
        selected_features = tech_features[:tech_keep] + financial_features[:fin_keep]
        print(f"ç²¾ç®€åˆ°: {len(selected_features)}ä¸ªç‰¹å¾")

    # 7. æœ€ç»ˆç»Ÿè®¡
    tech_selected = [col for col in selected_features if col in tech_features]
    fin_selected = [col for col in selected_features if col in financial_features]
    other_selected = [col for col in selected_features if col in other_features]

    print(f"ç‰¹å¾å¹³è¡¡å®Œæˆ!")
    print(f"æœ€ç»ˆæŠ€æœ¯ç‰¹å¾: {len(tech_selected)}ä¸ª")
    print(f"æœ€ç»ˆè´¢åŠ¡ç‰¹å¾: {len(fin_selected)}ä¸ª")
    print(f"å…¶ä»–ç‰¹å¾: {len(other_selected)}ä¸ª")
    print(f"å¹³è¡¡æ¯”ä¾‹: {len(tech_selected)}:{len(fin_selected)} (ç›®æ ‡: {target_tech}:{target_fin})")
    print(f"æ€»ç‰¹å¾æ•°é‡: {len(selected_features)}ä¸ª")

    # æ˜¾ç¤ºç‰¹å¾ç¤ºä¾‹
    if len(selected_features) > 0:
        print(f"æŠ€æœ¯ç‰¹å¾ç¤ºä¾‹: {tech_selected[:5] if tech_selected else 'æ— '}")
        print(f"è´¢åŠ¡ç‰¹å¾ç¤ºä¾‹: {fin_selected[:5] if fin_selected else 'æ— '}")

    return df, selected_features


def emergency_enhance_technical_features(df):
    """ç´§æ€¥å¢å¼ºæŠ€æœ¯ç‰¹å¾"""
    print("æ‰§è¡Œç´§æ€¥æŠ€æœ¯ç‰¹å¾å¢å¼º...")

    if 'close' not in df.columns:
        return df

    df_enhanced = df.copy()
    close_prices = df_enhanced['close']
    enhanced_features = []

    try:
        # 1. åŸºç¡€æ¯”ç‡ç‰¹å¾
        print("1. è®¡ç®—åŸºç¡€æ¯”ç‡ç‰¹å¾...")
        if 'open' in df_enhanced.columns:
            df_enhanced['emergency_open_close_ratio'] = df_enhanced['close'] / df_enhanced['open'] - 1
            enhanced_features.append('emergency_open_close_ratio')

        if all(col in df_enhanced.columns for col in ['high', 'low']):
            df_enhanced['emergency_price_intensity'] = (df_enhanced['close'] - df_enhanced['low']) / (
                        df_enhanced['high'] - df_enhanced['low']).replace(0, 1)
            df_enhanced['emergency_daily_range_pct'] = (df_enhanced['high'] - df_enhanced['low']) / df_enhanced['close']
            enhanced_features.extend(['emergency_price_intensity', 'emergency_daily_range_pct'])

        print(f"ç”Ÿæˆ{len([f for f in enhanced_features if 'emergency' in f])}ä¸ªåŸºç¡€æ¯”ç‡ç‰¹å¾")

    except Exception as e:
        print(f"åŸºç¡€æ¯”ç‡ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")

    try:
        # 2. ç®€å•ç§»åŠ¨å¹³å‡çº¿ (ä½¿ç”¨æ–æ³¢é‚£å¥‘æ•°åˆ—çª—å£)
        print("2. è®¡ç®—ç®€å•ç§»åŠ¨å¹³å‡çº¿...")
        fib_windows = [3, 5, 8, 13, 21, 34, 55]  # æ–æ³¢é‚£å¥‘æ•°åˆ—çª—å£

        for window in fib_windows:
            try:
                ma_col = f'emergency_ma_{window}'
                df_enhanced[ma_col] = close_prices.rolling(window=window, min_periods=1).mean()
                enhanced_features.append(ma_col)

                # ä»·æ ¼ç›¸å¯¹äºç§»åŠ¨å¹³å‡çš„ä½ç½®
                df_enhanced[f'emergency_price_vs_ma{window}'] = close_prices / df_enhanced[ma_col] - 1
                enhanced_features.append(f'emergency_price_vs_ma{window}')
            except Exception as e:
                print(f"çª—å£{window}ç§»åŠ¨å¹³å‡è®¡ç®—å¤±è´¥: {e}")
                continue

        print(f"ç”Ÿæˆ{len(fib_windows) * 2}ä¸ªç§»åŠ¨å¹³å‡ç›¸å…³ç‰¹å¾")

    except Exception as e:
        print(f"ç§»åŠ¨å¹³å‡çº¿è®¡ç®—å¤±è´¥: {e}")

    try:
        # 3. ç®€å•åŠ¨é‡æŒ‡æ ‡
        print("3. è®¡ç®—ç®€å•åŠ¨é‡æŒ‡æ ‡...")
        mom_periods = [1, 2, 3, 5, 8, 13]  # æ–æ³¢é‚£å¥‘æ•°åˆ—å‘¨æœŸ

        for period in mom_periods:
            try:
                mom_col = f'emergency_mom_{period}d'
                df_enhanced[mom_col] = close_prices.pct_change(period)
                enhanced_features.append(mom_col)
            except Exception as e:
                print(f"å‘¨æœŸ{period}åŠ¨é‡è®¡ç®—å¤±è´¥: {e}")
                continue

        print(f"ç”Ÿæˆ{len(mom_periods)}ä¸ªåŠ¨é‡ç‰¹å¾")

    except Exception as e:
        print(f"åŠ¨é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")

    try:
        # 4. ä»·æ ¼ä½ç½®ç‰¹å¾
        print("4. è®¡ç®—ä»·æ ¼ä½ç½®ç‰¹å¾...")
        position_windows = [5, 10, 20]

        for window in position_windows:
            try:
                high_col = f'emergency_high_{window}'
                low_col = f'emergency_low_{window}'

                df_enhanced[high_col] = close_prices.rolling(window=window, min_periods=1).max()
                df_enhanced[low_col] = close_prices.rolling(window=window, min_periods=1).min()

                position_col = f'emergency_position_{window}'
                df_enhanced[position_col] = (close_prices - df_enhanced[low_col]) / (
                            df_enhanced[high_col] - df_enhanced[low_col]).replace(0, 1)

                enhanced_features.extend([high_col, low_col, position_col])
            except Exception as e:
                print(f"çª—å£{window}ä»·æ ¼ä½ç½®è®¡ç®—å¤±è´¥: {e}")
                continue

        print(f"ç”Ÿæˆ{len(position_windows) * 3}ä¸ªä»·æ ¼ä½ç½®ç‰¹å¾")

    except Exception as e:
        print(f"ä»·æ ¼ä½ç½®ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")

    try:
        # 5. æ³¢åŠ¨ç‡ç‰¹å¾
        print("5. è®¡ç®—æ³¢åŠ¨ç‡ç‰¹å¾...")
        vol_windows = [5, 10, 20]

        for window in vol_windows:
            try:
                vol_col = f'emergency_volatility_{window}d'
                df_enhanced[vol_col] = close_prices.pct_change().rolling(window=window, min_periods=1).std()
                enhanced_features.append(vol_col)
            except Exception as e:
                print(f"çª—å£{window}æ³¢åŠ¨ç‡è®¡ç®—å¤±è´¥: {e}")
                continue

        print(f"ç”Ÿæˆ{len(vol_windows)}ä¸ªæ³¢åŠ¨ç‡ç‰¹å¾")

    except Exception as e:
        print(f"æ³¢åŠ¨ç‡ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")

    try:
        # 6. æˆäº¤é‡ç‰¹å¾ (å¦‚æœæœ‰æˆäº¤é‡æ•°æ®)
        if 'volume' in df_enhanced.columns:
            print("6. è®¡ç®—æˆäº¤é‡ç‰¹å¾...")
            volume = df_enhanced['volume']
            volume_windows = [5, 10, 20]

            for window in volume_windows:
                try:
                    # æˆäº¤é‡ç§»åŠ¨å¹³å‡
                    vol_ma_col = f'emergency_volume_ma_{window}'
                    df_enhanced[vol_ma_col] = volume.rolling(window=window, min_periods=1).mean()
                    enhanced_features.append(vol_ma_col)

                    # æˆäº¤é‡æ¯”ç‡
                    vol_ratio_col = f'emergency_volume_ratio_{window}'
                    # é¿å…é™¤é›¶
                    valid_mask = df_enhanced[vol_ma_col] != 0
                    df_enhanced[vol_ratio_col] = 1.0
                    df_enhanced.loc[valid_mask, vol_ratio_col] = volume[valid_mask] / df_enhanced.loc[
                        valid_mask, vol_ma_col]
                    enhanced_features.append(vol_ratio_col)
                except Exception as e:
                    print(f"çª—å£{window}æˆäº¤é‡è®¡ç®—å¤±è´¥: {e}")
                    continue

            print(f"ç”Ÿæˆ{len(volume_windows) * 2}ä¸ªæˆäº¤é‡ç‰¹å¾")

    except Exception as e:
        print(f"æˆäº¤é‡ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")

    try:
        # 7. RSIæŒ‡æ ‡
        print("7. è®¡ç®—RSIæŒ‡æ ‡...")
        rsi_periods = [6, 14]

        for period in rsi_periods:
            try:
                delta = close_prices.diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()

                rs = gain / (loss + 1e-10)  # é¿å…é™¤é›¶
                rsi = 100 - (100 / (1 + rs))

                rsi_col = f'emergency_rsi_{period}'
                df_enhanced[rsi_col] = rsi
                enhanced_features.append(rsi_col)
            except Exception as e:
                print(f"å‘¨æœŸ{period}RSIè®¡ç®—å¤±è´¥: {e}")
                continue

        print(f"ç”Ÿæˆ{len(rsi_periods)}ä¸ªRSIç‰¹å¾")

    except Exception as e:
        print(f"RSIæŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")

    try:
        # 8. ä»·æ ¼å˜åŒ–ç‰¹å¾
        print("8. è®¡ç®—ä»·æ ¼å˜åŒ–ç‰¹å¾...")
        change_periods = [1, 2, 3, 5]

        for period in change_periods:
            try:
                # ç»å¯¹ä»·æ ¼å˜åŒ–
                change_col = f'emergency_price_change_{period}d'
                df_enhanced[change_col] = close_prices.diff(period)
                enhanced_features.append(change_col)

                # ç™¾åˆ†æ¯”ä»·æ ¼å˜åŒ–
                pct_change_col = f'emergency_pct_change_{period}d'
                df_enhanced[pct_change_col] = close_prices.pct_change(period)
                enhanced_features.append(pct_change_col)
            except Exception as e:
                print(f"å‘¨æœŸ{period}ä»·æ ¼å˜åŒ–è®¡ç®—å¤±è´¥: {e}")
                continue

        print(f"ç”Ÿæˆ{len(change_periods) * 2}ä¸ªä»·æ ¼å˜åŒ–ç‰¹å¾")

    except Exception as e:
        print(f"ä»·æ ¼å˜åŒ–ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")

    try:
        # 9. ä»·æ ¼åŠ é€Ÿåº¦ç‰¹å¾
        print("9. è®¡ç®—ä»·æ ¼åŠ é€Ÿåº¦ç‰¹å¾...")
        try:
            # ä¸€é˜¶å·®åˆ†ï¼ˆé€Ÿåº¦ï¼‰
            velocity = close_prices.diff()
            # äºŒé˜¶å·®åˆ†ï¼ˆåŠ é€Ÿåº¦ï¼‰
            acceleration = velocity.diff()

            df_enhanced['emergency_price_velocity'] = velocity
            df_enhanced['emergency_price_acceleration'] = acceleration
            enhanced_features.extend(['emergency_price_velocity', 'emergency_price_acceleration'])

            print("ç”Ÿæˆ2ä¸ªä»·æ ¼åŠ é€Ÿåº¦ç‰¹å¾")
        except Exception as e:
            print(f"ä»·æ ¼åŠ é€Ÿåº¦è®¡ç®—å¤±è´¥: {e}")

    except Exception as e:
        print(f"ä»·æ ¼åŠ é€Ÿåº¦ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")

    try:
        # 10. ä»·æ ¼æ³¢åŠ¨ç‰¹å¾
        print("10. è®¡ç®—ä»·æ ¼æ³¢åŠ¨ç‰¹å¾...")
        volatility_windows = [5, 10, 20]

        for window in volatility_windows:
            try:
                # çœŸå®æ³¢åŠ¨å¹…åº¦ï¼ˆTrue Rangeï¼‰
                if all(col in df_enhanced.columns for col in ['high', 'low']):
                    tr1 = df_enhanced['high'] - df_enhanced['low']
                    tr2 = abs(df_enhanced['high'] - df_enhanced['close'].shift(1))
                    tr3 = abs(df_enhanced['low'] - df_enhanced['close'].shift(1))
                    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)

                    atr_col = f'emergency_atr_{window}'
                    df_enhanced[atr_col] = true_range.rolling(window=window, min_periods=1).mean()
                    enhanced_features.append(atr_col)

                    # æ ‡å‡†åŒ–çœŸå®æ³¢åŠ¨å¹…åº¦
                    atr_pct_col = f'emergency_atr_pct_{window}'
                    df_enhanced[atr_pct_col] = df_enhanced[atr_col] / close_prices
                    enhanced_features.append(atr_pct_col)
            except Exception as e:
                print(f"çª—å£{window}æ³¢åŠ¨ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
                continue

        print(f"ç”Ÿæˆ{len(volatility_windows) * 2}ä¸ªæ³¢åŠ¨ç‰¹å¾")

    except Exception as e:
        print(f"ä»·æ ¼æ³¢åŠ¨ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")

    # æœ€ç»ˆç»Ÿè®¡
    actual_enhanced_features = [col for col in enhanced_features if col in df_enhanced.columns]
    print(f"ç´§æ€¥å¢å¼ºå®Œæˆ! ç”Ÿæˆ{len(actual_enhanced_features)}ä¸ªæŠ€æœ¯ç‰¹å¾")

    # æ˜¾ç¤ºç”Ÿæˆçš„ç‰¹å¾ç±»å‹ç»Ÿè®¡
    feature_types = {
        'ç§»åŠ¨å¹³å‡': len([f for f in actual_enhanced_features if 'ma_' in f or 'price_vs_ma' in f]),
        'åŠ¨é‡': len([f for f in actual_enhanced_features if 'mom_' in f]),
        'æ³¢åŠ¨ç‡': len([f for f in actual_enhanced_features if 'volatility_' in f or 'atr_' in f]),
        'ä»·æ ¼ä½ç½®': len([f for f in actual_enhanced_features if 'position_' in f or 'intensity' in f]),
        'æˆäº¤é‡': len([f for f in actual_enhanced_features if 'volume_' in f]),
        'RSI': len([f for f in actual_enhanced_features if 'rsi_' in f]),
        'ä»·æ ¼å˜åŒ–': len([f for f in actual_enhanced_features if 'change_' in f or 'pct_change' in f])
    }

    print("ç”Ÿæˆç‰¹å¾ç±»å‹ç»Ÿè®¡:")
    for feature_type, count in feature_types.items():
        if count > 0:
            print(f"  {feature_type}: {count}ä¸ª")

    return df_enhanced

@timer_decorator
def prepare_modeling_data(df, feature_cols):
    """å‡†å¤‡å»ºæ¨¡æ•°æ®"""
    print_section("å‡†å¤‡å»ºæ¨¡æ•°æ®")

    if df.empty or len(feature_cols) == 0:
        print("æ•°æ®ä¸ºç©ºæˆ–æ— ç‰¹å¾")
        return pd.DataFrame()

    # åŸºç¡€åˆ—
    base_cols = ['date', 'stock_code', 'close', 'volume', 'future_return', 'market_avg_return', 'label']

    # æ·»åŠ open, high, lowå¦‚æœå­˜åœ¨
    for col in ['open', 'high', 'low', 'spread', 'turnover_rate', 'change', 'amount']:
        if col in df.columns and col not in base_cols:
            base_cols.append(col)

    # åˆå¹¶æ‰€æœ‰éœ€è¦çš„åˆ—
    all_cols = base_cols + feature_cols
    all_cols = [col for col in all_cols if col in df.columns]

    modeling_df = df[all_cols].copy()

    # å¤„ç†ç¼ºå¤±å€¼
    print(f"å¤„ç†å‰æ•°æ®å½¢çŠ¶: {modeling_df.shape}")

    # ç§»é™¤æ ‡ç­¾ç¼ºå¤±çš„è¡Œ
    initial_size = len(modeling_df)
    modeling_df = modeling_df.dropna(subset=['future_return', 'market_avg_return', 'label'])
    print(f"ç§»é™¤æ ‡ç­¾ç¼ºå¤±è¡Œ: {initial_size - len(modeling_df):,} è¡Œ")

    # å¤„ç†ç‰¹å¾ç¼ºå¤±å€¼
    for col in feature_cols:
        if col in modeling_df.columns:
            modeling_df[col] = modeling_df[col].fillna(modeling_df[col].median())

    # å¤„ç†æ— ç©·å€¼
    numeric_cols = modeling_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col in modeling_df.columns:
            modeling_df[col] = modeling_df[col].replace([np.inf, -np.inf], np.nan)
            modeling_df[col] = modeling_df[col].fillna(modeling_df[col].median())

    # ç§»é™¤ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ
    modeling_df = modeling_df.dropna()
    print(f"å¤„ç†åæ•°æ®å½¢çŠ¶: {modeling_df.shape}")

    print(f"å»ºæ¨¡æ•°æ®å‡†å¤‡å®Œæˆ!")
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"æ­£æ ·æœ¬æ¯”ä¾‹: {modeling_df['label'].mean():.2%}")
    print(f"æ—¶é—´èŒƒå›´: {modeling_df['date'].min()} åˆ° {modeling_df['date'].max()}")
    print(f"è‚¡ç¥¨æ•°é‡: {modeling_df['stock_code'].nunique()}")

    return modeling_df


@timer_decorator
def split_train_val_test_data(df, feature_cols, test_ratio=0.2, val_ratio=0.1):
    """æ—¶é—´åºåˆ—æ•°æ®é›†åˆ’åˆ†"""
    print_section("æ•°æ®é›†åˆ’åˆ†")

    if df.empty or len(feature_cols) == 0:
        print("æ•°æ®ä¸ºç©ºæˆ–æ— ç‰¹å¾")
        return None, None, None, None, None, None, None, None, None

    # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values('date')

    # è·å–å”¯ä¸€æ—¥æœŸ
    dates = np.sort(df['date'].unique())
    n_dates = len(dates)

    # è®¡ç®—åˆ†å‰²ç‚¹
    train_end_idx = int(n_dates * (1 - test_ratio - val_ratio))
    val_end_idx = int(n_dates * (1 - test_ratio))

    train_dates = dates[:train_end_idx]
    val_dates = dates[train_end_idx:val_end_idx]
    test_dates = dates[val_end_idx:]

    # åˆ’åˆ†æ•°æ®é›†
    train_df = df[df['date'].isin(train_dates)]
    val_df = df[df['date'].isin(val_dates)]
    test_df = df[df['date'].isin(test_dates)]

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    if len(train_df) == 0 or len(val_df) == 0 or len(test_df) == 0:
        print("æ•°æ®é›†åˆ’åˆ†å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ’åˆ†")
        # å›é€€åˆ°ç®€å•åˆ’åˆ†
        train_idx = int(len(df) * (1 - test_ratio - val_ratio))
        val_idx = int(len(df) * (1 - test_ratio))

        train_df = df.iloc[:train_idx]
        val_df = df.iloc[train_idx:val_idx]
        test_df = df.iloc[val_idx:]

    print(f"è®­ç»ƒé›†: {train_df['date'].min().date()} åˆ° {train_df['date'].max().date()}, å¤§å°: {len(train_df):,}")
    print(f"éªŒè¯é›†: {val_df['date'].min().date()} åˆ° {val_df['date'].max().date()}, å¤§å°: {len(val_df):,}")
    print(f"æµ‹è¯•é›†: {test_df['date'].min().date()} åˆ° {test_df['date'].max().date()}, å¤§å°: {len(test_df):,}")
    print(f"è®­ç»ƒé›†æ­£æ ·æœ¬æ¯”ä¾‹: {train_df['label'].mean():.2%}")
    print(f"éªŒè¯é›†æ­£æ ·æœ¬æ¯”ä¾‹: {val_df['label'].mean():.2%}")
    print(f"æµ‹è¯•é›†æ­£æ ·æœ¬æ¯”ä¾‹: {test_df['label'].mean():.2%}")

    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    X_train = train_df[feature_cols]
    X_val = val_df[feature_cols]
    X_test = test_df[feature_cols]

    y_train = train_df['label']
    y_val = val_df['label']
    y_test = test_df['label']

    print(f"ç‰¹å¾å½¢çŠ¶: X_train{X_train.shape}, X_val{X_val.shape}, X_test{X_test.shape}")

    return X_train, X_val, X_test, y_train, y_val, y_test, train_df, val_df, test_df


@timer_decorator
def hyperparameter_tuning(X_train, y_train, X_val, y_val, n_trials=5):
    """éªŒè¯é›†è¶…å‚æ•°è°ƒä¼˜"""
    # å¿«é€Ÿæ¨¡å¼ï¼šå‡å°‘è°ƒä¼˜æ¬¡æ•°
    if QUICK_MODE:
        n_trials = HYPERPARAM_TRIALS
        print_section(f"å¿«é€Ÿè¶…å‚æ•°è°ƒä¼˜ (n_trials={n_trials})")
    else:
        print_section("éªŒè¯é›†è¶…å‚æ•°è°ƒä¼˜")

    # å¦‚æœæ•°æ®é‡å¤§ï¼Œè¿›è¡Œé‡‡æ ·ä»¥åŠ é€Ÿè°ƒä¼˜
    if len(X_train) > SAMPLE_SIZE_TUNING:
        from sklearn.model_selection import train_test_split
        X_train_sample, _, y_train_sample, _ = train_test_split(
            X_train, y_train,
            train_size=SAMPLE_SIZE_TUNING,
            stratify=y_train,  # ä¿æŒæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
            random_state=RANDOM_STATE
        )

    best_params = {}

    # 1. éšæœºæ£®æ—è°ƒå‚ï¼ˆç®€åŒ–å‚æ•°ç½‘æ ¼ï¼‰
    print("1. éšæœºæ£®æ—è¶…å‚æ•°è°ƒä¼˜...")
    rf_param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [10, 15],
        'min_samples_split': [5, 10],
        'min_samples_leaf': [2, 5],
        'max_features': ['sqrt']
    }

    rf_model = RandomForestClassifier(
        random_state=RANDOM_STATE,
        n_jobs=N_JOBS
    )

    # ä½¿ç”¨éšæœºæœç´¢
    rf_search = RandomizedSearchCV(
        rf_model,
        rf_param_grid,
        n_iter=n_trials,
        cv=2,
        scoring='f1',
        n_jobs=-1,
        verbose=1,
        random_state=RANDOM_STATE
    )

    rf_search.fit(X_train_sample, y_train_sample)
    best_params['rf'] = rf_search.best_params_
    print(f" æœ€ä½³å‚æ•°: {rf_search.best_params_}")
    print(f" æœ€ä½³éªŒè¯åˆ†æ•°: {rf_search.best_score_:.4f}")

    # 2. XGBoostè°ƒå‚ï¼ˆç®€åŒ–å‚æ•°ç½‘æ ¼ï¼‰- ä¿®å¤ï¼šè½¬æ¢ä¸ºnumpyæ•°ç»„
    print("\n2. XGBoostè¶…å‚æ•°è°ƒä¼˜...")

    if hasattr(X_train, 'values'):
        X_train_sample = X_train.values
    else:
        X_train_sample = X_train

    if hasattr(y_train, 'values'):
        y_train_sample = y_train.values
    else:
        y_train_sample = y_train

    # ä¿®å¤ï¼šè½¬æ¢æ•°æ®ç±»å‹
    X_train_sample = X_train_sample.astype(np.float32)
    y_train_sample = y_train_sample.astype(np.int32)

    xgb_param_grid = {
        'n_estimators': [50, 100, 150],
        'max_depth': [3, 4, 5],
        'learning_rate': [0.05, 0.1, 0.15],
        'subsample': [0.6, 0.7, 0.8],
        'colsample_bytree': [0.6, 0.7, 0.8]
    }

    # ä¿®å¤ï¼šä½¿ç”¨æ›´å…¼å®¹çš„XGBoostå‚æ•°
    xgb_model = xgb.XGBClassifier(
        random_state=RANDOM_STATE,
        n_jobs=1,  # é¿å…å¹¶è¡Œé—®é¢˜
        use_label_encoder=False,
        eval_metric='logloss',
        verbosity=0  # å‡å°‘è¾“å‡º
    )

    try:
        xgb_search = RandomizedSearchCV(
            xgb_model, xgb_param_grid,
            n_iter=n_trials, cv=2, scoring='f1', n_jobs=1,
            verbose=1, random_state=RANDOM_STATE, error_score='raise'
        )
        xgb_search.fit(X_train_sample, y_train_sample)  # ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
        best_params['xgb'] = xgb_search.best_params_
        print(f"   æœ€ä½³å‚æ•°: {xgb_search.best_params_}")
        print(f"   æœ€ä½³éªŒè¯åˆ†æ•°: {xgb_search.best_score_:.4f}")

    except Exception as e:
        print(f"  XGBoostè°ƒä¼˜å¤±è´¥: {e}")
        print("   ä½¿ç”¨é»˜è®¤XGBoostå‚æ•°")
        best_params['xgb'] = {
            'n_estimators': 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8
        }

    return best_params


@timer_decorator
def train_models(X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, best_params=None):
    """ä¿®å¤ç‰ˆæ¨¡å‹è®­ç»ƒ - è§£å†³XGBoost dtypeé”™è¯¯å’Œæ”¶ç›Šç‡infé—®é¢˜"""
    print_section("ä¿®å¤ç‰ˆæ¨¡å‹è®­ç»ƒ")

    # ==================== 1. æ•°æ®éªŒè¯å’Œç‰¹å¾æ•°é‡æ£€æŸ¥ ====================
    print("æ•°æ®éªŒè¯å’Œç‰¹å¾æ•°é‡æ£€æŸ¥...")

    if X_train.empty or X_val.empty or X_test.empty:
        print("è¾“å…¥æ•°æ®ä¸ºç©º")
        return {}, None, {}, {}, {}

    # éªŒè¯ç‰¹å¾æ•°é‡ä¸€è‡´æ€§
    print(f"ç‰¹å¾æ•°é‡éªŒè¯:")
    print(f"  ç‰¹å¾åˆ—è¡¨: {len(feature_cols)} ä¸ªç‰¹å¾")
    print(f"  X_train å½¢çŠ¶: {X_train.shape} -> {X_train.shape[1]} ä¸ªç‰¹å¾")
    print(f"  X_val å½¢çŠ¶: {X_val.shape} -> {X_val.shape[1]} ä¸ªç‰¹å¾")
    print(f"  X_test å½¢çŠ¶: {X_test.shape} -> {X_test.shape[1]} ä¸ªç‰¹å¾")

    # æ£€æŸ¥ç‰¹å¾æ•°é‡æ˜¯å¦åŒ¹é…
    if len(feature_cols) != X_train.shape[1]:
        print(f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…: ç‰¹å¾åˆ—è¡¨{len(feature_cols)} vs è®­ç»ƒæ•°æ®{X_train.shape[1]}")
        if hasattr(X_train, 'columns'):
            actual_features = list(X_train.columns)
            print(f"  ä½¿ç”¨å®é™…ç‰¹å¾åç§°: {len(actual_features)} ä¸ª")
            feature_cols = actual_features
        else:
            feature_cols = [f'feature_{i}' for i in range(X_train.shape[1])]
            print(f"  åˆ›å»ºæ–°ç‰¹å¾åç§°: {len(feature_cols)} ä¸ª")

    # ==================== 2. æ ‡å‡†åŒ–ç‰¹å¾ ====================
    print("ç‰¹å¾æ ‡å‡†åŒ–...")
    scaler = StandardScaler()

    try:
        # ç¡®ä¿æ•°æ®æ˜¯numpyæ•°ç»„æ ¼å¼
        X_train_array = X_train.values if hasattr(X_train, 'values') else X_train
        X_val_array = X_val.values if hasattr(X_val, 'values') else X_val
        X_test_array = X_test.values if hasattr(X_test, 'values') else X_test

        X_train_scaled = scaler.fit_transform(X_train_array)
        X_val_scaled = scaler.transform(X_val_array)
        X_test_scaled = scaler.transform(X_test_array)

        print(f"ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
        print(f"æ ‡å‡†åŒ–åå½¢çŠ¶: X_train{X_train_scaled.shape}, X_val{X_val_scaled.shape}, X_test{X_test_scaled.shape}")
    except Exception as e:
        print(f"ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥: {e}")
        X_train_scaled = X_train_array
        X_val_scaled = X_val_array
        X_test_scaled = X_test_array
        print("ä½¿ç”¨æœªæ ‡å‡†åŒ–æ•°æ®ç»§ç»­è®­ç»ƒ")

    # ==================== 3. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ ====================
    print("å¤„ç†ç±»åˆ«ä¸å¹³è¡¡...")
    try:
        smote = SMOTE(random_state=RANDOM_STATE, sampling_strategy=0.8)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)
        print(f"å¹³è¡¡åè®­ç»ƒé›†: {X_train_balanced.shape}")

        # ç¡®ä¿y_train_balancedæ˜¯æ­£ç¡®æ ¼å¼
        if hasattr(y_train_balanced, 'values'):
            y_train_balanced = y_train_balanced.values
        elif hasattr(y_train_balanced, 'to_numpy'):
            y_train_balanced = y_train_balanced.to_numpy()

        # ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        if hasattr(X_train_balanced, 'dtype') and X_train_balanced.dtype != np.float32:
            X_train_balanced = X_train_balanced.astype(np.float32)
        if hasattr(y_train_balanced, 'dtype') and y_train_balanced.dtype != np.int32:
            y_train_balanced = y_train_balanced.astype(np.int32)

    except Exception as e:
        print(f"SMOTEå¤„ç†å¤±è´¥: {e}")
        print("ä½¿ç”¨åŸå§‹ä¸å¹³è¡¡æ•°æ®")
        X_train_balanced, y_train_balanced = X_train_scaled, y_train
        if hasattr(y_train_balanced, 'values'):
            y_train_balanced = y_train_balanced.values
        elif hasattr(y_train_balanced, 'to_numpy'):
            y_train_balanced = y_train_balanced.to_numpy()

    # ==================== 4. åˆå§‹åŒ–ç»“æœå­—å…¸ ====================
    models = {}
    results = {}
    predictions = {}
    probabilities = {}

    # ==================== 5. æ¨¡å‹é»˜è®¤å‚æ•° ====================
    if best_params is None:
        print("ä½¿ç”¨ä¿å®ˆæ¨¡å‹å‚æ•°é˜²æ­¢è¿‡æ‹Ÿåˆ...")
        best_params = {
            'rf': {
                'n_estimators': 100,
                'max_depth': 10,
                'min_samples_split': 10,
                'min_samples_leaf': 4,
                'max_features': 'sqrt',
                'random_state': RANDOM_STATE,
                'n_jobs': N_JOBS
            },
            'xgb': {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': RANDOM_STATE,
                'n_jobs': 1,
                'use_label_encoder': False,
                'eval_metric': 'logloss'
            }
        }

    # ==================== 6. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ ====================
    print("\n1. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
    try:
        rf_params = best_params.get('rf', {})
        rf_model = RandomForestClassifier(**rf_params)

        print(f"è®­ç»ƒæ•°æ®æ ¼å¼æ£€æŸ¥:")
        print(f"  X_train_balanced: {type(X_train_balanced)}, shape: {X_train_balanced.shape}")
        print(f"  y_train_balanced: {type(y_train_balanced)}, shape: {y_train_balanced.shape}")

        # æ·»åŠ æ ·æœ¬æƒé‡
        if len(X_train_balanced) > 1000:
            try:
                print("è®¡ç®—æ ·æœ¬æƒé‡...")
                time_decay = np.linspace(0.8, 1.2, len(X_train_balanced))
                class_balance = 1 + (y_train_balanced * 0.2)
                sample_weights = time_decay * class_balance

                print(f"ä½¿ç”¨æ ·æœ¬æƒé‡è®­ç»ƒ")
                print(f"æƒé‡èŒƒå›´: {sample_weights.min():.2f} - {sample_weights.max():.2f}")
                rf_model.fit(X_train_balanced, y_train_balanced, sample_weight=sample_weights)
            except Exception as e:
                print(f"æ ·æœ¬æƒé‡è®­ç»ƒå¤±è´¥: {e}")
                print("å›é€€åˆ°æ— æƒé‡è®­ç»ƒ")
                rf_model.fit(X_train_balanced, y_train_balanced)
        else:
            rf_model.fit(X_train_balanced, y_train_balanced)

        models['rf'] = rf_model
        print("éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒå®Œæˆ")

        # åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šè¯„ä¼°
        y_val_pred_rf = rf_model.predict(X_val_scaled)
        y_val_proba_rf = rf_model.predict_proba(X_val_scaled)[:, 1]
        y_test_pred_rf = rf_model.predict(X_test_scaled)
        y_test_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]

        # ç¡®ä¿y_trueæ˜¯numpyæ•°ç»„æ ¼å¼
        y_val_array = y_val.values if hasattr(y_val, 'values') else y_val
        y_test_array = y_test.values if hasattr(y_test, 'values') else y_test

        results['rf'] = {
            'val_accuracy': accuracy_score(y_val_array, y_val_pred_rf),
            'val_precision': precision_score(y_val_array, y_val_pred_rf, zero_division=0),
            'val_recall': recall_score(y_val_array, y_val_pred_rf, zero_division=0),
            'val_f1': f1_score(y_val_array, y_val_pred_rf, zero_division=0),
            'val_roc_auc': roc_auc_score(y_val_array, y_val_proba_rf),
            'test_accuracy': accuracy_score(y_test_array, y_test_pred_rf),
            'test_precision': precision_score(y_test_array, y_test_pred_rf, zero_division=0),
            'test_recall': recall_score(y_test_array, y_test_pred_rf, zero_division=0),
            'test_f1': f1_score(y_test_array, y_test_pred_rf, zero_division=0),
            'test_roc_auc': roc_auc_score(y_test_array, y_test_proba_rf)
        }

        predictions['rf'] = y_test_pred_rf
        probabilities['rf'] = y_test_proba_rf

        print("éšæœºæ£®æ—æ¨¡å‹éªŒè¯é›†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {results['rf']['val_accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡: {results['rf']['val_precision']:.4f}")
        print(f"  å¬å›ç‡: {results['rf']['val_recall']:.4f}")
        print(f"  F1åˆ†æ•°: {results['rf']['val_f1']:.4f}")
        print(f"  ROC-AUC: {results['rf']['val_roc_auc']:.4f}")

        print("éšæœºæ£®æ—æ¨¡å‹æµ‹è¯•é›†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {results['rf']['test_accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡: {results['rf']['test_precision']:.4f}")
        print(f"  å¬å›ç‡: {results['rf']['test_recall']:.4f}")
        print(f"  F1åˆ†æ•°: {results['rf']['test_f1']:.4f}")
        print(f"  ROC-AUC: {results['rf']['test_roc_auc']:.4f}")

    except Exception as e:
        print(f"éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        # è®¾ç½®é»˜è®¤ç»“æœ
        y_test_array = y_test.values if hasattr(y_test, 'values') else y_test
        results['rf'] = {
            'val_accuracy': 0.5, 'val_precision': 0.5, 'val_recall': 0.5, 'val_f1': 0.5, 'val_roc_auc': 0.5,
            'test_accuracy': 0.5, 'test_precision': 0.5, 'test_recall': 0.5, 'test_f1': 0.5, 'test_roc_auc': 0.5
        }
        predictions['rf'] = np.zeros(len(y_test_array))
        probabilities['rf'] = np.ones(len(y_test_array)) * 0.5
        models['rf'] = None

    # ==================== 7. è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆå…³é”®ä¿®å¤éƒ¨åˆ†ï¼‰ ====================
    print("\n2. è®­ç»ƒXGBoostæ¨¡å‹...")
    try:
        xgb_params = best_params.get('xgb', {})

        if not xgb_params:
            # è®¡ç®—æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
            scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1]) if len(
                y_train[y_train == 1]) > 0 else 1
            xgb_params = {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_alpha': 0.1,
                'reg_lambda': 0.1,
                'scale_pos_weight': scale_pos_weight,
                'random_state': RANDOM_STATE,
                'n_jobs': 1,
                'use_label_encoder': False,
                'eval_metric': 'logloss',
                'verbosity': 0
            }

        print(f"ä½¿ç”¨XGBoostå‚æ•°:")
        for key, value in xgb_params.items():
            if key in ['n_estimators', 'max_depth', 'learning_rate', 'subsample',
                       'colsample_bytree', 'scale_pos_weight']:
                print(f"   {key}: {value}")

        # å…³é”®ä¿®å¤ï¼šXGBoostæ•°æ®æ ¼å¼å…¼å®¹æ€§
        print("å‡†å¤‡XGBoostè®­ç»ƒæ•°æ®...")

        def safe_convert_to_float32(data):
            """å®‰å…¨è½¬æ¢ä¸ºfloat32ï¼Œå…¼å®¹DataFrameå’Œnumpyæ•°ç»„"""
            if hasattr(data, 'values'):
                # å¦‚æœæ˜¯DataFrameæˆ–Seriesï¼Œè·å–values
                array_data = data.values
            else:
                array_data = data

            # ç¡®ä¿æ˜¯numpyæ•°ç»„
            if not isinstance(array_data, np.ndarray):
                array_data = np.array(array_data)

            # æ£€æŸ¥æ•°æ®ç±»å‹å¹¶å®‰å…¨è½¬æ¢
            try:
                if hasattr(array_data, 'dtype'):
                    if array_data.dtype != np.float32:
                        return array_data.astype(np.float32)
                return array_data
            except Exception as e:
                print(f"æ•°æ®ç±»å‹è½¬æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæ•°æ®ç±»å‹")
                return array_data

        # åº”ç”¨å®‰å…¨è½¬æ¢
        X_train_balanced_float32 = safe_convert_to_float32(X_train_balanced)
        y_train_balanced_int32 = y_train_balanced.astype(np.int32) if hasattr(y_train_balanced, 'astype') else np.array(
            y_train_balanced, dtype=np.int32)
        X_val_float32 = safe_convert_to_float32(X_val_scaled)
        X_test_float32 = safe_convert_to_float32(X_test_scaled)

        print(f"æ•°æ®æ ¼å¼æ£€æŸ¥:")
        print(f"  X_train_balanced: {type(X_train_balanced_float32)}")
        if hasattr(X_train_balanced_float32, 'dtype'):
            print(f"    dtype: {X_train_balanced_float32.dtype}")
        print(f"  y_train_balanced: {type(y_train_balanced_int32)}")
        if hasattr(y_train_balanced_int32, 'dtype'):
            print(f"    dtype: {y_train_balanced_int32.dtype}")
        print(f"  X_val: {type(X_val_float32)}")
        if hasattr(X_val_float32, 'dtype'):
            print(f"    dtype: {X_val_float32.dtype}")
        print(f"  X_test: {type(X_test_float32)}")
        if hasattr(X_test_float32, 'dtype'):
            print(f"    dtype: {X_test_float32.dtype}")

        # åˆ›å»ºXGBoostæ¨¡å‹
        xgb_model = xgb.XGBClassifier(**xgb_params)

        # è®­ç»ƒæ¨¡å‹
        print("è®­ç»ƒXGBoostæ¨¡å‹...")
        xgb_model.fit(X_train_balanced_float32, y_train_balanced_int32)
        models['xgb'] = xgb_model
        print(" XGBoostæ¨¡å‹è®­ç»ƒå®Œæˆ")

        # åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šè¯„ä¼°
        y_val_pred_xgb = xgb_model.predict(X_val_float32)
        y_val_proba_xgb = xgb_model.predict_proba(X_val_float32)[:, 1]
        y_test_pred_xgb = xgb_model.predict(X_test_float32)
        y_test_proba_xgb = xgb_model.predict_proba(X_test_float32)[:, 1]

        # ç¡®ä¿y_trueæ ¼å¼æ­£ç¡®
        y_val_array = y_val.values if hasattr(y_val, 'values') else y_val
        y_test_array = y_test.values if hasattr(y_test, 'values') else y_test

        results['xgb'] = {
            'val_accuracy': accuracy_score(y_val_array, y_val_pred_xgb),
            'val_precision': precision_score(y_val_array, y_val_pred_xgb, zero_division=0),
            'val_recall': recall_score(y_val_array, y_val_pred_xgb, zero_division=0),
            'val_f1': f1_score(y_val_array, y_val_pred_xgb, zero_division=0),
            'val_roc_auc': roc_auc_score(y_val_array, y_val_proba_xgb),
            'test_accuracy': accuracy_score(y_test_array, y_test_pred_xgb),
            'test_precision': precision_score(y_test_array, y_test_pred_xgb, zero_division=0),
            'test_recall': recall_score(y_test_array, y_test_pred_xgb, zero_division=0),
            'test_f1': f1_score(y_test_array, y_test_pred_xgb, zero_division=0),
            'test_roc_auc': roc_auc_score(y_test_array, y_test_proba_xgb)
        }

        predictions['xgb'] = y_test_pred_xgb
        probabilities['xgb'] = y_test_proba_xgb

        print("XGBoostæ¨¡å‹éªŒè¯é›†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {results['xgb']['val_accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡: {results['xgb']['val_precision']:.4f}")
        print(f"  å¬å›ç‡: {results['xgb']['val_recall']:.4f}")
        print(f"  F1åˆ†æ•°: {results['xgb']['val_f1']:.4f}")
        print(f"  ROC-AUC: {results['xgb']['val_roc_auc']:.4f}")

        print("XGBoostæ¨¡å‹æµ‹è¯•é›†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {results['xgb']['test_accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡: {results['xgb']['test_precision']:.4f}")
        print(f"  å¬å›ç‡: {results['xgb']['test_recall']:.4f}")
        print(f"  F1åˆ†æ•°: {results['xgb']['test_f1']:.4f}")
        print(f"  ROC-AUC: {results['xgb']['test_roc_auc']:.4f}")

    except Exception as e:
        print(f" XGBoostæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        print("è·³è¿‡XGBoostæ¨¡å‹ï¼Œä»…ä½¿ç”¨éšæœºæ£®æ—")
        # è®¾ç½®é»˜è®¤ç»“æœ
        y_test_array = y_test.values if hasattr(y_test, 'values') else y_test
        results['xgb'] = {
            'val_accuracy': 0.5, 'val_precision': 0.5, 'val_recall': 0.5, 'val_f1': 0.5, 'val_roc_auc': 0.5,
            'test_accuracy': 0.5, 'test_precision': 0.5, 'test_recall': 0.5, 'test_f1': 0.5, 'test_roc_auc': 0.5
        }
        predictions['xgb'] = np.zeros(len(y_test_array))
        probabilities['xgb'] = np.ones(len(y_test_array)) * 0.5
        models['xgb'] = None

    # ==================== 8. æœ€ç»ˆç»“æœç»Ÿè®¡ ====================
    print_section("æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # ç»Ÿè®¡æˆåŠŸè®­ç»ƒçš„æ¨¡å‹
    successful_models = [name for name, model in models.items() if model is not None]
    print(f"æˆåŠŸè®­ç»ƒçš„æ¨¡å‹: {len(successful_models)}/{len(models)}")

    for model_name in successful_models:
        test_f1 = results[model_name]['test_f1']
        test_auc = results[model_name]['test_roc_auc']
        print(f"  {model_name.upper()}: F1={test_f1:.4f}, AUC={test_auc:.4f}")

    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹
    if not any(models.values()):
        print("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå¤±è´¥!")
        return {}, None, {}, {}, {}

    return models, scaler, results, predictions, probabilities

@timer_decorator
def analyze_feature_importance(models, feature_cols, n_top=20):
    """åˆ†æç‰¹å¾é‡è¦æ€§ - ä¿®å¤ç‰ˆæœ¬"""
    print_section("ç‰¹å¾é‡è¦æ€§åˆ†æ")

    # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
    feature_importance = pd.DataFrame({'feature': feature_cols})

    for model_name, model in models.items():
        if model is not None and hasattr(model, 'feature_importances_'):
            try:
                importances = model.feature_importances_

                # ä¿®å¤ï¼šè·å–æ¨¡å‹å®é™…ä½¿ç”¨çš„ç‰¹å¾åç§°
                if hasattr(model, 'feature_names_in_'):
                    # ä½¿ç”¨æ¨¡å‹è®­ç»ƒæ—¶çš„ç‰¹å¾åç§°
                    model_features = list(model.feature_names_in_)
                else:
                    # å›é€€åˆ°ä¼ å…¥çš„ç‰¹å¾åˆ—è¡¨
                    model_features = feature_cols

                # ç¡®ä¿ç‰¹å¾æ•°é‡åŒ¹é…
                if len(importances) == len(model_features):
                    # åˆ›å»ºä¸´æ—¶DataFrameæ¥åŒ¹é…ç‰¹å¾
                    temp_importance = pd.DataFrame({
                        'feature': model_features,
                        f'importance_{model_name}': importances
                    })
                    # åˆå¹¶åˆ°ä¸»DataFrame
                    feature_importance = feature_importance.merge(
                        temp_importance, on='feature', how='left'
                    )
                else:
                    print(f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…: æ¨¡å‹{model_name}")
                    # ä½¿ç”¨å¯¹é½çš„é€»è¾‘
                    min_len = min(len(importances), len(feature_cols))
                    importance_series = np.zeros(len(feature_cols))
                    importance_series[:min_len] = importances[:min_len]
                    feature_importance[f'importance_{model_name}'] = importance_series

            except Exception as e:
                print(f"æ¨¡å‹ {model_name} ç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥: {e}")
                feature_importance[f'importance_{model_name}'] = 0.0

    # è®¡ç®—å¹³å‡é‡è¦æ€§
    importance_cols = [col for col in feature_importance.columns if col.startswith('importance_')]
    if importance_cols:
        feature_importance['importance_mean'] = feature_importance[importance_cols].mean(axis=1)
        feature_importance = feature_importance.sort_values('importance_mean', ascending=False)

    print(f"Top {n_top} é‡è¦ç‰¹å¾:")
    if len(feature_importance) > 0:
        print(feature_importance.head(min(n_top, len(feature_importance))).to_string(index=False))

        # æ˜¾ç¤ºç‰¹å¾ç±»å‹ç»Ÿè®¡
        tech_features = len([col for col in feature_cols if not col.startswith('fin_')])
        fin_features = len([col for col in feature_cols if col.startswith('fin_')])
        print(f"ç‰¹å¾ç±»å‹ç»Ÿè®¡: æŠ€æœ¯ç‰¹å¾={tech_features}, è´¢åŠ¡ç‰¹å¾={fin_features}")
    else:
        print("æ²¡æœ‰ç‰¹å¾é‡è¦æ€§æ•°æ®")

    return feature_importance


def generate_daily_selected_stocks(test_df, predictions, probabilities, top_n=10):
    """ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨ - ä¿®å¤ç‰ˆæœ¬ï¼ˆåˆ é™¤æ”¶ç›Šç‡è®¡ç®—ï¼‰"""
    print_section("ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨")

    if test_df.empty or not predictions:
        print("æµ‹è¯•æ•°æ®æˆ–é¢„æµ‹ç»“æœä¸ºç©º")
        return pd.DataFrame()

    try:
        # ==================== 1. æ•°æ®å‡†å¤‡å’ŒéªŒè¯ ====================
        print("æ•°æ®å‡†å¤‡å’ŒéªŒè¯...")

        # å¤åˆ¶æµ‹è¯•é›†æ•°æ®
        required_cols = ['date', 'stock_code', 'close', 'future_return']
        missing_cols = [col for col in required_cols if col not in test_df.columns]
        if missing_cols:
            print(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return pd.DataFrame()

        selected_stocks = test_df[required_cols].copy()

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        initial_count = len(selected_stocks)
        selected_stocks = selected_stocks.dropna(subset=['future_return'])
        print(f"ç§»é™¤æœªæ¥æ”¶ç›Šç‡ç¼ºå¤±çš„æ•°æ®: {initial_count - len(selected_stocks):,} è¡Œ")

        if selected_stocks.empty:
            print("é€‰è‚¡æ•°æ®ä¸ºç©º")
            return pd.DataFrame()

        # æ·»åŠ æ¨¡å‹é¢„æµ‹æ¦‚ç‡
        for model_name in predictions.keys():
            if model_name in predictions and len(predictions[model_name]) == len(selected_stocks):
                selected_stocks[f'{model_name}_prediction'] = predictions[model_name]
                selected_stocks[f'{model_name}_probability'] = probabilities[model_name]
            else:
                print(f"æ¨¡å‹ {model_name} é¢„æµ‹ç»“æœé•¿åº¦ä¸åŒ¹é…ï¼Œè·³è¿‡")

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹è¿›è¡Œé€‰è‚¡
        available_models = [m for m in predictions.keys() if f'{m}_probability' in selected_stocks.columns]
        if available_models:
            best_model = available_models[0]
        else:
            best_model = 'rf'
            # å¦‚æœæ²¡æœ‰æ¨¡å‹æ¦‚ç‡ï¼Œä½¿ç”¨éšæœºåˆ†æ•°
            selected_stocks['selection_score'] = np.random.random(len(selected_stocks))
            print("æ— å¯ç”¨æ¨¡å‹æ¦‚ç‡ï¼Œä½¿ç”¨éšæœºé€‰è‚¡")

        print(f"ä½¿ç”¨æ¨¡å‹è¿›è¡Œé€‰è‚¡: {best_model.upper()}")
        selected_stocks['selection_score'] = selected_stocks[f'{best_model}_probability']

        # ==================== 2. ä¿®å¤é€‰è‚¡é€»è¾‘ ====================
        print("ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨...")
        daily_top_stocks = []
        valid_dates = 0

        # è·å–å”¯ä¸€æ—¥æœŸå¹¶æ’åº
        unique_dates = sorted(selected_stocks['date'].unique())
        print(f"å¤„ç† {len(unique_dates)} ä¸ªäº¤æ˜“æ—¥çš„é€‰è‚¡...")

        for date in tqdm(unique_dates, desc="ç”Ÿæˆæ¯æ—¥é€‰è‚¡"):
            date_data = selected_stocks[selected_stocks['date'] == date].copy()

            if len(date_data) == 0:
                continue

            # æŒ‰é¢„æµ‹æ¦‚ç‡æ’åº
            date_data = date_data.sort_values('selection_score', ascending=False)
            date_data = date_data.drop_duplicates(subset=['stock_code'], keep='first')

            # ä¿®å¤ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„è‚¡ç¥¨å¯é€‰
            if len(date_data) < top_n:
                if len(date_data) > 0:
                    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨è‚¡ç¥¨
                    top_n_stocks = date_data.copy()
                    print(f"æ—¥æœŸ {date.date()} åªæœ‰ {len(date_data)} åªè‚¡ç¥¨ï¼Œä½¿ç”¨å…¨éƒ¨å¯ç”¨è‚¡ç¥¨")
                else:
                    print(f"æ—¥æœŸ {date.date()} æ²¡æœ‰å¯ç”¨è‚¡ç¥¨ï¼Œè·³è¿‡")
                    continue
            else:
                # é€‰æ‹©Top N
                top_n_stocks = date_data.head(top_n).copy()

            # ç¡®ä¿æœ‰é€‰è‚¡ç»“æœ
            if len(top_n_stocks) == 0:
                print(f"æ—¥æœŸ {date.date()} é€‰è‚¡ç»“æœä¸ºç©ºï¼Œä½¿ç”¨éšæœºé€‰æ‹©")
                # å›é€€ï¼šéšæœºé€‰æ‹©top_nåªè‚¡ç¥¨
                if len(date_data) > 0:
                    top_n_stocks = date_data.sample(n=min(top_n, len(date_data)),
                                                    random_state=RANDOM_STATE)
                else:
                    continue

            top_n_stocks['rank'] = range(1, len(top_n_stocks) + 1)
            daily_top_stocks.append(top_n_stocks)
            valid_dates += 1

        print(f"æˆåŠŸå¤„ç† {valid_dates}/{len(unique_dates)} ä¸ªäº¤æ˜“æ—¥çš„é€‰è‚¡")

        if not daily_top_stocks:
            print("æ²¡æœ‰ç”Ÿæˆä»»ä½•é€‰è‚¡åˆ—è¡¨")
            return pd.DataFrame()

        # ==================== 3. åˆå¹¶ç»“æœ ====================
        result_df = pd.concat(daily_top_stocks, ignore_index=True)

        # æ·»åŠ é€‰è‚¡ç†ç”±
        result_df['selection_reason'] = result_df.apply(
            lambda
                x: f"æ¨¡å‹é¢„æµ‹æ¦‚ç‡:{x['selection_score']:.3f}, æ’å:{x['rank']}/{min(top_n, len(result_df[result_df['date'] == x['date']]))}",
            axis=1
        )

        # é‡å‘½ååˆ—
        result_df = result_df.rename(columns={
            'date': 'äº¤æ˜“æ—¥',
            'stock_code': 'è‚¡ç¥¨ä»£ç ',
            'close': 'æ”¶ç›˜ä»·',
            'future_return': 'æœªæ¥15å¤©ç»å¯¹æ”¶ç›Šç‡',
            'selection_score': 'æ¨¡å‹é¢„æµ‹æ¦‚ç‡',
            'rank': 'å½“æ—¥æ’å',
            'selection_reason': 'é€‰è‚¡ç†ç”±'
        })

        # é€‰æ‹©éœ€è¦çš„åˆ—
        final_columns = ['äº¤æ˜“æ—¥', 'è‚¡ç¥¨ä»£ç ', 'æ”¶ç›˜ä»·', 'æœªæ¥15å¤©ç»å¯¹æ”¶ç›Šç‡',
                         'æ¨¡å‹é¢„æµ‹æ¦‚ç‡', 'å½“æ—¥æ’å', 'é€‰è‚¡ç†ç”±']
        final_columns = [col for col in final_columns if col in result_df.columns]
        result_df = result_df[final_columns]

        print(f"ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨: {result_df.shape}")

        # ==================== 4. ç®€å•çš„é€‰è‚¡ç»Ÿè®¡ï¼ˆåˆ é™¤æ”¶ç›Šç‡è®¡ç®—ï¼‰ ====================
        print_section("é€‰è‚¡ç»“æœç»Ÿè®¡")

        # ç®€å•çš„ç»Ÿè®¡ï¼ˆä¸æ¶‰åŠå¤æ‚æ”¶ç›Šç‡è®¡ç®—ï¼‰
        total_stocks = len(result_df)
        unique_stocks = result_df['è‚¡ç¥¨ä»£ç '].nunique()
        avg_daily_stocks = result_df.groupby('äº¤æ˜“æ—¥').size().mean()
        avg_prob_all = result_df['æ¨¡å‹é¢„æµ‹æ¦‚ç‡'].mean()

        print(f"é€‰è‚¡ç»Ÿè®¡:")
        print(f"   æ€»é€‰è‚¡è®°å½•: {total_stocks:,} æ¡")
        print(f"   å”¯ä¸€è‚¡ç¥¨æ•°é‡: {unique_stocks} åª")
        print(f"   å¹³å‡æ¯æ—¥é€‰è‚¡: {avg_daily_stocks:.1f} åª")
        print(f"   å¹³å‡é¢„æµ‹æ¦‚ç‡: {avg_prob_all:.3f}")

        # ==================== 5. éªŒè¯é€‰è‚¡ç»“æœ ====================
        print_section("é€‰è‚¡ç»“æœéªŒè¯")

        # æ£€æŸ¥æœ€è¿‘å‡ ä¸ªäº¤æ˜“æ—¥çš„é€‰è‚¡ç»“æœ
        recent_dates = result_df['äº¤æ˜“æ—¥'].unique()[-3:]  # æœ€è¿‘3ä¸ªäº¤æ˜“æ—¥
        for test_date in recent_dates:
            daily_selection = result_df[result_df['äº¤æ˜“æ—¥'] == test_date]
            print(f"éªŒè¯ {test_date.date()} çš„é€‰è‚¡ç»“æœ:")
            print(f"   é€‰è‚¡æ•°é‡: {len(daily_selection)} åª")
            print(f"   å”¯ä¸€è‚¡ç¥¨: {len(daily_selection['è‚¡ç¥¨ä»£ç '].unique())} åª")
            if len(daily_selection) > 0:
                top_stocks = daily_selection['è‚¡ç¥¨ä»£ç '].head(3).tolist()
                avg_prob = daily_selection['æ¨¡å‹é¢„æµ‹æ¦‚ç‡'].mean()
                print(f"   å‰3åªè‚¡ç¥¨: {top_stocks}")
                print(f"   å¹³å‡é¢„æµ‹æ¦‚ç‡: {avg_prob:.3f}")
                # åˆ é™¤æ”¶ç›Šç‡è®¡ç®—ï¼Œåªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            else:
                print(" è¯¥æ—¥æ— é€‰è‚¡ç»“æœ")

        return result_df

    except Exception as e:
        print(f"ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨å¤±è´¥: {e}")
        traceback.print_exc()
        return pd.DataFrame()



def emergency_recalculate_returns(df, days=FUTURE_DAYS):
    """ç´§æ€¥é‡æ–°è®¡ç®—æ”¶ç›Šç‡ - ç®€åŒ–ç‰ˆæœ¬"""
    print("æ‰§è¡Œç´§æ€¥æ”¶ç›Šç‡é‡æ–°è®¡ç®—...")

    df = df.copy().sort_values(['stock_code', 'date'])
    returns = np.full(len(df), np.nan)

    # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—
    for stock_code in df['stock_code'].unique():
        stock_data = df[df['stock_code'] == stock_code].sort_values('date')
        close_prices = stock_data['close'].values

        for i in range(len(stock_data)):
            if i + days < len(stock_data):
                current_price = close_prices[i]
                future_price = close_prices[i + days]

                # æ£€æŸ¥ä»·æ ¼æœ‰æ•ˆæ€§
                if current_price > 0 and future_price > 0 and not np.isnan(current_price) and not np.isnan(
                        future_price):
                    return_val = (future_price / current_price) - 1
                    # æ‰¾åˆ°åœ¨åŸå§‹dfä¸­çš„ç´¢å¼•
                    original_idx = stock_data.index[i]
                    returns[df.index.get_loc(original_idx)] = return_val

    df['future_return'] = returns

    # ç»Ÿè®¡ç»“æœ
    valid_returns = returns[~np.isnan(returns)]
    if len(valid_returns) > 0:
        print(f"ç´§æ€¥è®¡ç®—å®Œæˆ: {len(valid_returns):,} ä¸ªæœ‰æ•ˆæ”¶ç›Šç‡")
        print(f"  æ”¶ç›Šç‡èŒƒå›´: {valid_returns.min():.4f} åˆ° {valid_returns.max():.4f}")

        # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
        sample_count = min(3, len(valid_returns))
        sample_indices = np.random.choice(len(valid_returns), sample_count, replace=False)
        for i, idx in enumerate(sample_indices):
            print(f"  æ ·æœ¬{i + 1}: {valid_returns[idx]:.4f} ({valid_returns[idx]:.2%})")
    else:
        print("ç´§æ€¥è®¡ç®—å¤±è´¥ï¼šæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆæ”¶ç›Šç‡")

    return df
# ==================== ä¸»ç¨‹åº ====================
def main():
    """ä¸»ç¨‹åº - æ·»åŠ ç´§æ€¥æ”¶ç›Šç‡ä¿®å¤ç‰ˆæœ¬"""
    print_section("å°æ¹¾è‚¡ç¥¨é€‰è‚¡é¢„æµ‹æ¨¡å‹")
    print(f"é¢„æµ‹æœªæ¥å¤©æ•°: {FUTURE_DAYS}å¤©")
    print(f"å›çœ‹å¤©æ•°: {LOOKBACK_DAYS}å¤©")
    print(f"éšæœºç§å­: {RANDOM_STATE}")
    print(f"å¿«é€Ÿæ¨¡å¼: {'å¯ç”¨' if QUICK_MODE else 'å…³é—­'}")

    # æ—¶é—´é¢„ä¼°
    print("\né¢„è®¡æ‰§è¡Œæ—¶é—´:")
    if QUICK_MODE:
        print("  è´¢æŠ¥æ•°æ®åˆå¹¶: 1-2åˆ†é’Ÿ (åŸ10-20åˆ†é’Ÿ)")
        print("  è¶…å‚æ•°è°ƒä¼˜: 2-3åˆ†é’Ÿ (åŸ20-30åˆ†é’Ÿ)")
        print("  æ€»æ—¶é—´: 10-15åˆ†é’Ÿ (åŸ60-90åˆ†é’Ÿ)")
    else:
        print("  æ€»æ—¶é—´: 30-45åˆ†é’Ÿ")
    print("=" * 50)

    start_time = time.time()

    try:
        # 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        data = load_and_preprocess_data()
        if data is None:
            print("æ•°æ®åŠ è½½å¤±è´¥")
            return None

        df, feature_cols = data
        if df is None or df.empty:
            print("æ•°æ®ä¸ºç©º")
            return None

        # ====================  ç´§æ€¥æ”¶ç›Šç‡ä¿®å¤ ====================
        print_section("æ‰§è¡Œç´§æ€¥æ”¶ç›Šç‡ä¿®å¤")

        # æ£€æŸ¥å½“å‰æ”¶ç›Šç‡çŠ¶æ€
        if 'future_return' in df.columns:
            current_returns = df['future_return'].dropna()
            inf_count = np.isinf(current_returns).sum()
            print(f"å½“å‰æ”¶ç›Šç‡çŠ¶æ€: æœ‰æ•ˆæ ·æœ¬{len(current_returns):,}, infå€¼{inf_count}ä¸ª")

            if inf_count > 0 or current_returns.mean() == float('inf'):
                print("æ£€æµ‹åˆ°æ”¶ç›Šç‡é—®é¢˜ï¼Œæ‰§è¡Œç´§æ€¥ä¿®å¤...")
                df = emergency_fix_returns_simple(df)
            else:
                print("æ”¶ç›Šç‡æ•°æ®æ­£å¸¸ï¼Œè·³è¿‡ä¿®å¤")
        else:
            print("æ•°æ®ä¸­æ²¡æœ‰future_returnåˆ—ï¼Œéœ€è¦é‡æ–°è®¡ç®—æ”¶ç›Šç‡")
            # è°ƒç”¨ä¿®å¤ç‰ˆçš„æ”¶ç›Šç‡è®¡ç®—å‡½æ•°
            df = calculate_future_returns_and_labels(df)

        # éªŒè¯ä¿®å¤ç»“æœ
        if 'future_return' in df.columns:
            fixed_returns = df['future_return'].dropna()
            inf_count_fixed = np.isinf(fixed_returns).sum()
            print(f"ä¿®å¤åæ”¶ç›Šç‡çŠ¶æ€: æœ‰æ•ˆæ ·æœ¬{len(fixed_returns):,}, infå€¼{inf_count_fixed}ä¸ª")

            if inf_count_fixed > 0:
                print("ç´§æ€¥ä¿®å¤åä»ç„¶å­˜åœ¨infå€¼ï¼Œè¿›è¡ŒäºŒæ¬¡ä¿®å¤...")
                # å¼ºåˆ¶é‡æ–°è®¡ç®—
                df = calculate_future_returns_and_labels(df)

        # ==================== åç»­åŸæœ‰ä»£ç  ====================
        # 2. å‡†å¤‡å»ºæ¨¡æ•°æ®
        modeling_df = prepare_modeling_data(df, feature_cols)
        if modeling_df.empty or len(feature_cols) < 5:
            print("å»ºæ¨¡æ•°æ®ä¸ºç©ºæˆ–ç‰¹å¾æ•°é‡ä¸è¶³")
            return None

        # 3. æ•°æ®é›†åˆ’åˆ†
        data_split = split_train_val_test_data(
            modeling_df, feature_cols, test_ratio=TEST_RATIO, val_ratio=VAL_RATIO
        )
        if data_split[0] is None:
            print("æ•°æ®é›†åˆ’åˆ†å¤±è´¥")
            return None

        X_train, X_val, X_test, y_train, y_val, y_test, train_df, val_df, test_df = data_split

        if X_train.empty or X_test.empty or X_val.empty:
            print("æ•°æ®é›†åˆ’åˆ†å¤±è´¥")
            return None

        # 4. éªŒè¯é›†è¶…å‚æ•°è°ƒä¼˜
        best_params = hyperparameter_tuning(X_train, y_train, X_val, y_val, n_trials=5)

        # 5. ä½¿ç”¨è°ƒä¼˜å‚æ•°è®­ç»ƒæ¨¡å‹
        models, scaler, results, predictions, probabilities = train_models(
            X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, best_params
        )

        # 6. ç‰¹å¾é‡è¦æ€§åˆ†æ
        try:
            feature_importance = analyze_feature_importance(models, feature_cols)
        except Exception as e:
            print(f"ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
            print("è·³è¿‡ç‰¹å¾é‡è¦æ€§åˆ†æ...")
            feature_importance = None

        # 7. åˆ›å»ºç»“æœDataFrame
        results_df = test_df[['date', 'stock_code', 'close']].copy()
        results_df = results_df.rename(columns={
            'date': 'äº¤æ˜“æ—¥',
            'stock_code': 'è‚¡ç¥¨ä»£ç ',
            'close': 'æ”¶ç›˜ä»·'
        })

        # æ·»åŠ é¢„æµ‹ç»“æœ
        for model_name in models.keys():
            results_df[f'{model_name}_é¢„æµ‹'] = predictions[model_name]
            results_df[f'{model_name}_æ¦‚ç‡'] = probabilities[model_name]

        results_df['è¯´æ˜'] = 'åŸºäºå†å²æ•°æ®å’Œè´¢åŠ¡æŒ‡æ ‡çš„æœºå™¨å­¦ä¹ é€‰è‚¡é¢„æµ‹'

        print("è¾“å‡ºè¡¨æ ¼å­—æ®µè¯´æ˜:")
        print("1. äº¤æ˜“æ—¥ - é¢„æµ‹åŸºå‡†æ—¥ï¼ˆå¦‚ 2025-01-05ï¼‰")
        print("2. è‚¡ç¥¨ä»£ç  - ä¸ªè‚¡å”¯ä¸€æ ‡è¯†ï¼ˆå¦‚ 2344ï¼‰")
        print("3. æ”¶ç›˜ä»· - é¢„æµ‹æ—¥æ”¶ç›˜ä»·æ ¼")
        print("4. æ¨¡å‹é¢„æµ‹ - å„æ¨¡å‹é¢„æµ‹ç»“æœï¼ˆ1=çœ‹æ¶¨/0=çœ‹è·Œï¼‰")
        print("5. æ¨¡å‹æ¦‚ç‡ - æ¨¡å‹é¢„æµ‹çš„ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰")

        # ä¿å­˜é¢„æµ‹ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f'stock_predictions_{timestamp}.csv'
        results_df.to_csv(results_file, index=False, encoding='utf-8-sig')
        print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜: {results_file}")

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        if feature_importance is not None:
            importance_file = f'feature_importance_{timestamp}.csv'
            feature_importance.to_csv(importance_file, index=False)
            print(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {importance_file}")

        # ä¿å­˜æ¨¡å‹
        model_file = f'stock_models_{timestamp}.pkl'
        with open(model_file, 'wb') as f:
            pickle.dump({
                'models': models,
                'scaler': scaler,
                'features': feature_cols,
                'best_params': best_params,
                'results': results
            }, f, protocol=4)
        print(f"æ¨¡å‹å·²ä¿å­˜: {model_file}")

        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_file = f'model_evaluation_{timestamp}.txt'
        with open(eval_file, 'w', encoding='utf-8') as f:
            f.write("å°æ¹¾è‚¡ç¥¨é€‰è‚¡é¢„æµ‹æ¨¡å‹è¯„ä¼°ç»“æœ\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"æ•°æ®æ—¶é—´èŒƒå›´: {df['date'].min().date()} åˆ° {df['date'].max().date()}\n")
            f.write(f"è‚¡ç¥¨æ•°é‡: {df['stock_code'].nunique()}\n")
            f.write(f"æ€»æ ·æœ¬æ•°: {len(df):,}\n")
            f.write(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}\n")
            f.write(f"è®­ç»ƒé›†æ ·æœ¬: {len(train_df):,}\n")
            f.write(f"éªŒè¯é›†æ ·æœ¬: {len(val_df):,}\n")
            f.write(f"æµ‹è¯•é›†æ ·æœ¬: {len(test_df):,}\n\n")

            f.write("æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:\n")
            f.write("-" * 60 + "\n")
            f.write(f"{'æ¨¡å‹':<10} {'å‡†ç¡®ç‡':<8} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'ROC-AUC':<8}\n")
            f.write("-" * 60 + "\n")
            for model_name, result in results.items():
                f.write(f"{model_name.upper():<10} {result['test_accuracy']:.4f}   {result['test_precision']:.4f}   "
                        f"{result['test_recall']:.4f}   {result['test_f1']:.4f}   {result['test_roc_auc']:.4f}\n")
            f.write("=" * 60 + "\n")
        print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_file}")

        # 8. ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨
        print_section("ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨")
        daily_selected_df = generate_daily_selected_stocks(test_df, predictions, probabilities, top_n=10)

        if not daily_selected_df.empty:
            selected_stocks_file = f'daily_selected_stocks_top10_{timestamp}.csv'
            daily_selected_df.to_csv(selected_stocks_file, index=False, encoding='utf-8-sig')
            print(f"æ¯æ—¥é€‰è‚¡åˆ—è¡¨å·²ä¿å­˜: {selected_stocks_file}")
        else:
            print("æ¯æ—¥é€‰è‚¡åˆ—è¡¨ç”Ÿæˆå¤±è´¥")

        # 9. æœ€ç»ˆæŠ¥å‘Š
        end_time = time.time()
        execution_time = (end_time - start_time) / 60

        print_section("æœ€ç»ˆæ¨¡å‹è¯„ä¼°æŠ¥å‘Š")

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model_name = max(results.keys(), key=lambda k: results[k]['test_f1'])
        best_f1 = results[best_model_name]['test_f1']
        best_roc_auc = results[best_model_name]['test_roc_auc']

        print(f"æœ€ä½³æ¨¡å‹: {best_model_name.upper()} (F1: {best_f1:.4f}, ROC-AUC: {best_roc_auc:.4f})")
        print(f"æ•°æ®æ—¶é—´èŒƒå›´: {df['date'].min().date()} åˆ° {df['date'].max().date()}")
        print(f"è‚¡ç¥¨æ•°é‡: {df['stock_code'].nunique()}")
        print(f"æ€»æ ·æœ¬æ•°: {len(df):,}")
        print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        print(f"æŠ€æœ¯ç‰¹å¾: {len([col for col in feature_cols if not col.startswith('fin_')])}")
        print(f"è´¢åŠ¡ç‰¹å¾: {len([col for col in feature_cols if col.startswith('fin_')])}")
        print(f"è®­ç»ƒé›†æ ·æœ¬: {len(train_df):,}")
        print(f"éªŒè¯é›†æ ·æœ¬: {len(val_df):,}")
        print(f"æµ‹è¯•é›†æ ·æœ¬: {len(test_df):,}")
        print(f"ç¨‹åºæ‰§è¡Œæ—¶é—´: {execution_time:.1f} åˆ†é’Ÿ")

        return {
            'models': models,
            'scaler': scaler,
            'features': feature_cols,
            'best_params': best_params,
            'results': results,
            'feature_importance': feature_importance,
            'test_df': test_df,
            'predictions': predictions,
            'probabilities': probabilities
        }

    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        traceback.print_exc()
        return None


# ==================== è¿è¡Œç¨‹åº ====================
if __name__ == "__main__":
    print("å¼€å§‹è¿è¡Œå°æ¹¾è‚¡ç¥¨è¶…é¢æ”¶ç›Šé¢„æµ‹æ¨¡å‹...")
    result = main()

    if result is not None:
        print_section("ç¨‹åºæ‰§è¡ŒæˆåŠŸ!")
        print("å·²ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶:")
        print("1. stock_predictions_*.csv - é¢„æµ‹ç»“æœ")
        print("2. feature_importance_*.csv - ç‰¹å¾é‡è¦æ€§")
        print("3. stock_models_*.pkl - ä¿å­˜çš„æ¨¡å‹")
        print("4. model_evaluation_*.txt - æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
    else:
        print_section("ç¨‹åºæ‰§è¡Œå¤±è´¥!")

