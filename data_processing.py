import pandas as pd
import numpy as np
import os


# --------------------------
# å…¨å±€é…ç½®ï¼šç»Ÿä¸€è·¯å¾„ç®¡ç†
# --------------------------
def get_project_paths():
    """è·å–é¡¹ç›®æ‰€æœ‰å…³é”®è·¯å¾„ï¼Œç»Ÿä¸€ç®¡ç†"""
    project_root = r"D:\XX\XX\XX" # æ¢æˆæœ¬åœ°æ–‡ä»¶å¤¹
    raw_dir = os.path.join(project_root, "raw")
    processed_dir = os.path.join(project_root, "processed")

    if not os.path.exists(processed_dir):
        os.makedirs(processed_dir)
        print(f"è‡ªåŠ¨åˆ›å»ºprocessedæ–‡ä»¶å¤¹ï¼š{processed_dir}")

    stock_raw_path = os.path.join(raw_dir, "taiwan_stock_price_202511122027.csv")
    stock_processed_path = os.path.join(processed_dir, "taiwan_stock_cleaned_adjusted.csv")
    report_raw_path = os.path.join(raw_dir, "reports_202511122033.csv")
    report_processed_path = os.path.join(processed_dir, "reports_cleaned.csv")

    return {
        "stock_raw": stock_raw_path,
        "stock_processed": stock_processed_path,
        "report_raw": report_raw_path,
        "report_processed": report_processed_path
    }


# --------------------------
# åå¤æƒè®¡ç®—å‡½æ•°
# --------------------------
def calculate_backward_adjusted_prices(df):
    """
    åå¤æƒè®¡ç®—æ–¹æ³•
    åŸºäºä»·æ ¼è¿ç»­æ€§ï¼Œä½¿ç”¨æ»šåŠ¨ç»Ÿè®¡è¯†åˆ«å¼‚å¸¸è·³ç©º
    è¿”å›åå¤æƒä»·æ ¼åºåˆ—
    """
    # ç¡®ä¿æ•°æ®æŒ‰è‚¡ç¥¨å’Œæ—¶é—´æ’åº
    df = df.sort_values(['stock_id', 'date']).copy()

    # ä¸ºæ¯åªè‚¡ç¥¨è®¡ç®—å¤æƒä»·æ ¼
    all_adjusted = []

    for stock_id, group in df.groupby('stock_id'):
        group = group.copy().reset_index(drop=True)

        # è®¡ç®—æ—¥æ”¶ç›Šç‡
        group['daily_return'] = group['close'].pct_change()

        # ä½¿ç”¨æ»šåŠ¨çª—å£ç»Ÿè®¡è¯†åˆ«å¼‚å¸¸ä¸‹è·Œï¼ˆå¯èƒ½é™¤æƒï¼‰
        window = 60  # 60å¤©çª—å£
        min_periods = 20  # æœ€å°è§‚å¯ŸæœŸ

        # è®¡ç®—æ»šåŠ¨å‡å€¼å’Œæ ‡å‡†å·®
        group['rolling_mean'] = group['daily_return'].rolling(
            window=window, min_periods=min_periods).mean()
        group['rolling_std'] = group['daily_return'].rolling(
            window=window, min_periods=min_periods).std()

        # ç”¨0.05å¡«å……NaNï¼ˆå¯¹äºæ²¡æœ‰è¶³å¤Ÿå†å²æ•°æ®çš„æ—¶æœŸï¼‰
        group['rolling_std'] = group['rolling_std'].fillna(0.05)
        group['rolling_mean'] = group['rolling_mean'].fillna(0.0)

        # è¯†åˆ«å¼‚å¸¸ä¸‹è·Œï¼šè¶…è¿‡3ä¸ªæ ‡å‡†å·®ä¸”è·Œå¹…å¤§äº8%
        # è¿™æ˜¯ä¿å®ˆçš„é˜ˆå€¼ï¼Œé¿å…è¯¯åˆ¤å¸‚åœºæ­£å¸¸æ³¢åŠ¨
        threshold = group['rolling_mean'] - 3 * group['rolling_std']
        threshold = threshold.clip(upper=-0.08)  # è‡³å°‘8%è·Œå¹…æ‰è€ƒè™‘

        # æ ‡è®°å¯èƒ½çš„é™¤æƒæ—¥
        group['is_adjustment_day'] = (
                (group['daily_return'] < threshold) &
                (group['daily_return'] < -0.08) &  # è‡³å°‘ä¸‹è·Œ8%
                (group['daily_return'] > -0.50)  # æ’é™¤æç«¯ä¸‹è·Œï¼ˆ>50%ï¼‰
        )

        # è®¡ç®—è°ƒæ•´å› å­
        # åå¤æƒé€»è¾‘ï¼šå¦‚æœä»Šå¤©é™¤æƒä¸‹è·Œï¼Œé‚£ä¹ˆä¹‹å‰çš„ä»·æ ¼éœ€è¦ä¸Šè°ƒ
        adjustment_factor = 1.0
        cumulative_factors = []

        # ä»åå¾€å‰ç´¯ç§¯è°ƒæ•´å› å­ï¼ˆåå¤æƒçš„å…³é”®ï¼‰
        for i in range(len(group) - 1, -1, -1):
            if group.loc[i, 'is_adjustment_day']:
                # è°ƒæ•´å› å­ = å‰ä¸€æ—¥æ”¶ç›˜ä»· / å½“æ—¥æ”¶ç›˜ä»·
                if i > 0:
                    prev_close = group.loc[i - 1, 'close']
                    curr_close = group.loc[i, 'close']
                    if curr_close > 0:  # é¿å…é™¤ä»¥0
                        adjustment_factor *= (prev_close / curr_close)

            cumulative_factors.append(adjustment_factor)

        # åè½¬å› å­åˆ—è¡¨ï¼ˆä»æœ€æ—©åˆ°æœ€æ™šï¼‰
        cumulative_factors.reverse()
        group['cumulative_factor'] = cumulative_factors

        # è®¡ç®—å¤æƒä»·æ ¼
        group['adj_close'] = group['close'] * group['cumulative_factor']
        group['adj_open'] = group['open'] * group['cumulative_factor']
        group['adj_high'] = group['max'] * group['cumulative_factor']
        group['adj_low'] = group['min'] * group['cumulative_factor']

        # è®¡ç®—è°ƒæ•´æ¯”ç‡ï¼ˆç”¨äºåˆ†æï¼‰
        group['adjustment_ratio'] = 1.0
        adj_mask = group['is_adjustment_day']
        if adj_mask.any():
            group.loc[adj_mask, 'adjustment_ratio'] = (
                    group.loc[adj_mask, 'adj_close'].shift(1) /
                    group.loc[adj_mask, 'adj_close']
            ).fillna(1.0)

        all_adjusted.append(group)

    # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨
    result_df = pd.concat(all_adjusted, ignore_index=True)

    return result_df


# --------------------------
# è‚¡ä»·æ•°æ®å¤„ç†ï¼ˆæ¸…ç†+å¤æƒè®¡ç®—ï¼‰
# --------------------------
def process_stock_data(paths):
    """å¤„ç†è‚¡ä»·æ•°æ®ï¼šè„æ•°æ®æ¸…ç† + å¤æƒè®¡ç®—"""
    print("=" * 50)
    print("å¼€å§‹å¤„ç†è‚¡ä»·æ•°æ®...")
    print("=" * 50)

    if not os.path.exists(paths["stock_raw"]):
        raise FileNotFoundError(f"è‚¡ä»·åŸå§‹æ•°æ®æœªæ‰¾åˆ°ï¼è¯·ç¡®è®¤æ–‡ä»¶åœ¨ï¼š{paths['stock_raw']}")

    raw_path = paths["stock_raw"]
    processed_path = paths["stock_processed"]
    print(f"è‚¡ä»·åŸå§‹æ•°æ®è·¯å¾„ï¼š{raw_path}")
    print(f"è‚¡ä»·å¤„ç†åè·¯å¾„ï¼š{processed_path}\n")

    # 1. æ•°æ®è¯»å–ä¸åˆ—åæ ‡å‡†åŒ–
    df = pd.read_csv(raw_path, encoding="utf-8-sig")

    # åˆ—åæ˜ å°„
    column_mapping = {
        "id": "id",
        "date": "date",
        "stock_id": "stock_id",
        "trading_volume": "trading_volume",
        "trading_money": "trading_money",
        "open": "open",
        "max": "max",
        "min": "min",
        "close": "close",
        "spread": "spread",
        "trading_turnover": "trading_turnover"
    }
    df = df.rename(columns=column_mapping)

    # è½¬æ¢æ—¥æœŸæ ¼å¼
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    print(f"ğŸ” è‚¡ä»·åŸå§‹æ•°æ®æ€»è¡Œæ•°ï¼š{len(df)}")

    # 2. è„æ•°æ®æ¸…ç†
    df_clean = df.dropna(subset=["date"]).copy()

    # åˆ é™¤å…¨é›¶è¡Œ
    zero_mask = (df_clean["open"] == 0) & (df_clean["max"] == 0) & (df_clean["min"] == 0) & \
                (df_clean["close"] == 0) & (df_clean["trading_volume"] == 0)
    df_clean = df_clean[~zero_mask].copy()

    # åˆ é™¤ä»·æ ¼é€»è¾‘çŸ›ç›¾è¡Œ
    df_clean["price_max"] = df_clean[["open", "close"]].max(axis=1)
    df_clean["price_min"] = df_clean[["open", "close"]].min(axis=1)
    contradict_mask = (df_clean["max"] < df_clean["price_max"]) | (df_clean["min"] > df_clean["price_min"])
    df_clean = df_clean[~contradict_mask].copy()
    df_clean = df_clean.drop(columns=["price_max", "price_min"])

    # åˆ é™¤å¼‚å¸¸ä»·æ ¼è¡Œ
    price_abnormal_mask = (df_clean["close"] < 0.1) | (df_clean["close"] > 1000)
    df_clean = df_clean[~price_abnormal_mask].copy()

    # åˆ é™¤æˆäº¤é‡å¼‚å¸¸è¡Œ
    volume_zero_mask = (df_clean["trading_volume"] == 0) & (df_clean["close"] > 0)
    df_clean = df_clean[~volume_zero_mask].copy()

    # åˆ é™¤é‡å¤æ—¥æœŸè¡Œ
    df_clean = df_clean.drop_duplicates(subset=["stock_id", "date"], keep="first").copy()

    cleaned_count = len(df_clean)
    deleted_count = len(df) - cleaned_count
    print(f"è‚¡ä»·è„æ•°æ®æ¸…ç†å®Œæˆï¼š")
    print(f"   - æ¸…ç†åæ€»è¡Œæ•°ï¼š{cleaned_count}")
    print(f"   - åˆ é™¤è„æ•°æ®è¡Œæ•°ï¼š{deleted_count}")
    print(f"   - æ¶‰åŠè‚¡ç¥¨æ•°é‡ï¼š{df_clean['stock_id'].nunique()}åª\n")

    # 3. å¤æƒè®¡ç®—
    print("å¼€å§‹è®¡ç®—åå¤æƒä»·æ ¼...")

    try:
        df_adjusted = calculate_backward_adjusted_prices(df_clean)
        print(f"å¤æƒè®¡ç®—å®Œæˆï¼Œå…±å¤„ç† {df_adjusted['stock_id'].nunique()} åªè‚¡ç¥¨")

        # ç»Ÿè®¡è°ƒæ•´æ—¥ä¿¡æ¯
        adjustment_days = df_adjusted['is_adjustment_day'].sum()
        print(f"  è¯†åˆ«å‡º {adjustment_days} ä¸ªæ½œåœ¨é™¤æƒæ—¥")

        if adjustment_days > 0:
            avg_adjustment = df_adjusted[df_adjusted['is_adjustment_day']]['adjustment_ratio'].mean()
            print(f"  å¹³å‡è°ƒæ•´æ¯”ç‡: {avg_adjustment:.4f}")

    except Exception as e:
        print(f"å¤æƒè®¡ç®—å¤±è´¥: {str(e)}")
        print("ä½¿ç”¨åŸå§‹ä»·æ ¼ä½œä¸ºå¤æƒä»·æ ¼")
        df_adjusted = df_clean.copy()
        df_adjusted['adj_close'] = df_adjusted['close']
        df_adjusted['adj_open'] = df_adjusted['open']
        df_adjusted['adj_high'] = df_adjusted['max']
        df_adjusted['adj_low'] = df_adjusted['min']
        df_adjusted['cumulative_factor'] = 1.0
        df_adjusted['is_adjustment_day'] = False
        df_adjusted['adjustment_ratio'] = 1.0
        df_adjusted['daily_return'] = df_adjusted['close'].pct_change()

    # 4. è®¡ç®—æ”¶ç›Šç‡å’ŒéªŒè¯ç»Ÿè®¡
    df_adjusted['adj_return'] = df_adjusted.groupby('stock_id')['adj_close'].pct_change()

    # 5. ä¿å­˜ç»“æœ
    final_columns = [
        "id", "date", "stock_id", "trading_volume", "trading_money",
        "open", "max", "min", "close", "spread", "trading_turnover",
        "daily_return", "is_adjustment_day", "adjustment_ratio",
        "cumulative_factor", "adj_open", "adj_high", "adj_low", "adj_close", "adj_return"
    ]

    existing_columns = [col for col in final_columns if col in df_adjusted.columns]
    df_final = df_adjusted[existing_columns].copy()

    df_final.to_csv(processed_path, index=False, encoding="utf-8-sig")

    # 6. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print(f"\nå¤æƒç»“æœç»Ÿè®¡ï¼š")
    print(f"   - æ€»è¡Œæ•°: {len(df_final)}")
    print(f"   - è‚¡ç¥¨æ•°é‡: {df_final['stock_id'].nunique()}")

    if 'adj_return' in df_final.columns:
        return_stats = df_final['adj_return'].describe()
        print(f"   - å¤æƒæ”¶ç›Šç‡å‡å€¼: {return_stats['mean']:.6f}")
        print(f"   - å¤æƒæ”¶ç›Šç‡æ ‡å‡†å·®: {return_stats['std']:.6f}")
        print(f"   - å¤æƒæ”¶ç›Šç‡èŒƒå›´: [{return_stats['min']:.4f}, {return_stats['max']:.4f}]")

    if 'adj_close' in df_final.columns:
        price_stats = df_final['adj_close'].describe()
        print(f"   - å¤æƒä»·æ ¼èŒƒå›´: [{price_stats['min']:.2f}, {price_stats['max']:.2f}]")

    print(f"\nè‚¡ä»·æ•°æ®å¤„ç†å®Œæˆï¼")
    print(f"è‚¡ä»·å¤„ç†åæ–‡ä»¶ä½ç½®ï¼š{processed_path}")
    print(f"\nå¤æƒåˆ—è¯´æ˜ï¼š")
    print("1. daily_return: åŸå§‹æ—¥æ”¶ç›Šç‡")
    print("2. is_adjustment_day: æ˜¯å¦ä¸ºæ½œåœ¨é™¤æƒæ—¥")
    print("3. adjustment_ratio: å•æ—¥è°ƒæ•´æ¯”ç‡")
    print("4. cumulative_factor: ç´¯ç§¯å¤æƒå› å­")
    print("5. adj_open/adj_high/adj_low/adj_close: åå¤æƒä»·æ ¼")
    print("6. adj_return: å¤æƒåæ—¥æ”¶ç›Šç‡")

    return df_final


# --------------------------
# è´¢åŠ¡æ•°æ®å¤„ç†ï¼ˆä¿æŒä¸å˜ï¼‰
# --------------------------
def process_report_data(paths):
    """å¤„ç†è´¢åŠ¡æ•°æ®ï¼šè„æ•°æ®æ¸…ç†ï¼ˆæ ¸å¿ƒå­—æ®µ+ä¸šåŠ¡è§„åˆ™+å•ä½æ ¡éªŒï¼‰"""
    print("=" * 50)
    print("å¼€å§‹å¤„ç†è´¢åŠ¡æ•°æ®...")
    print("=" * 50)

    if not os.path.exists(paths["report_raw"]):
        raise FileNotFoundError(f"è´¢åŠ¡åŸå§‹æ•°æ®æœªæ‰¾åˆ°ï¼è¯·ç¡®è®¤æ–‡ä»¶åœ¨ï¼š{paths['report_raw']}")

    raw_path = paths["report_raw"]
    processed_path = paths["report_processed"]
    print(f"è´¢åŠ¡åŸå§‹æ•°æ®è·¯å¾„ï¼š{raw_path}")
    print(f"è´¢åŠ¡å¤„ç†åè·¯å¾„ï¼š{processed_path}\n")

    df_reports = pd.read_csv(raw_path, encoding="utf-8-sig")

    column_mapping = {
        "id": "id",
        "number": "number",
        "symbol": "symbol",
        "year": "year",
        "period": "period",
        "month": "month",
        "type": "type",
        "key": "key",
        "key_en": "key_en",
        "code": "code",
        "custom_code": "custom_code",
        "date": "date",
        "value": "value",
        "manual_value": "manual_value",
        "original_value": "original_value",
        "unit": "unit",
        "parent_id": "parent_id",
        "created_at": "created_at",
        "updated_at": "updated_at"
    }

    df_reports = df_reports.rename(columns=column_mapping)

    df_reports["year"] = pd.to_numeric(df_reports["year"], errors="coerce")
    df_reports["period"] = pd.to_numeric(df_reports["period"], errors="coerce")
    df_reports["month"] = pd.to_numeric(df_reports["month"], errors="coerce")
    df_reports["unit"] = pd.to_numeric(df_reports["unit"], errors="coerce")
    df_reports["value"] = pd.to_numeric(df_reports["value"], errors="coerce")
    df_reports["manual_value"] = pd.to_numeric(df_reports["manual_value"], errors="coerce")
    df_reports["original_value"] = pd.to_numeric(df_reports["original_value"], errors="coerce")

    print(f"è´¢åŠ¡åŸå§‹æ•°æ®æ€»è¡Œæ•°ï¼š{len(df_reports)}")

    df_clean = df_reports.copy()
    initial_count = len(df_clean)

    core_fields = ["number", "year", "type", "code", "unit"]
    df_clean = df_clean.dropna(subset=core_fields).copy()
    missing_core_count = initial_count - len(df_clean)
    print(f"åˆ é™¤æ ¸å¿ƒå­—æ®µç¼ºå¤±è¡Œï¼š{missing_core_count} è¡Œ")

    period_abnormal_mask = (df_clean["period"].notna()) & (~df_clean["period"].isin([1, 2, 3, 4]))
    df_clean = df_clean[~period_abnormal_mask].copy()
    period_abnormal_count = len(period_abnormal_mask[period_abnormal_mask])
    print(f"åˆ é™¤å­£åº¦å¼‚å¸¸è¡Œï¼ˆé1-4ï¼‰ï¼š{period_abnormal_count} è¡Œ")

    month_abnormal_mask = (df_clean["month"] != -1) & (~df_clean["month"].isin(range(1, 13)))
    df_clean = df_clean[~month_abnormal_mask].copy()
    month_abnormal_count = len(month_abnormal_mask[month_abnormal_mask])
    print(f"åˆ é™¤æœˆä»½å¼‚å¸¸è¡Œï¼ˆé-1/1-12ï¼‰ï¼š{month_abnormal_count} è¡Œ")

    valid_report_types = ["balance_sheet", "comprehensive_income_statement", "cash_flow"]
    type_abnormal_mask = ~df_clean["type"].isin(valid_report_types)
    df_clean = df_clean[~type_abnormal_mask].copy()
    type_abnormal_count = len(type_abnormal_mask[type_abnormal_mask])
    print(f"åˆ é™¤æŠ¥è¡¨ç±»å‹å¼‚å¸¸è¡Œï¼ˆéæŒ‡å®šä¸‰ç±»ï¼‰ï¼š{type_abnormal_count} è¡Œ")

    print("è´¢åŠ¡æ•°æ®å•ä½åˆ†å¸ƒï¼ˆå»é‡ï¼‰ï¼š")
    all_units = df_clean["unit"].dropna().unique()
    print(f"æ‰€æœ‰å‡ºç°çš„å•ä½å€¼ï¼š{sorted(all_units)}")

    valid_units = [1, 1000, 0.01]
    unit_abnormal_mask = ~df_clean["unit"].isin(valid_units)
    unit_abnormal_count = len(df_clean[unit_abnormal_mask])
    df_clean = df_clean[~unit_abnormal_mask].copy()
    print(f"åˆ é™¤å•ä½å¼‚å¸¸è¡Œï¼ˆé{valid_units}ï¼‰ï¼š{unit_abnormal_count} è¡Œ")

    unique_keys = ["number", "year", "period", "month", "type", "code"]
    df_clean = df_clean.drop_duplicates(subset=unique_keys, keep="first").copy()
    duplicate_count = initial_count - len(
        df_clean) - missing_core_count - period_abnormal_count - month_abnormal_count - type_abnormal_count - unit_abnormal_count
    print(f"åˆ é™¤é‡å¤è¡Œï¼ˆæŒ‰å”¯ä¸€çº¦æŸï¼‰ï¼š{duplicate_count} è¡Œ")

    value_fields = ["value", "manual_value", "original_value"]
    for field in value_fields:
        if field in df_clean.columns:
            extreme_mask = df_clean[field].abs() > 1e12
            df_clean = df_clean[~extreme_mask].copy()
            extreme_count = len(extreme_mask[extreme_mask])
            print(f"åˆ é™¤{field}æç«¯å€¼è¡Œï¼ˆ>1e12ï¼‰ï¼š{extreme_count} è¡Œ")

    df_clean["date"] = pd.to_datetime(df_clean["date"], errors="coerce")
    invalid_date_count = df_clean["date"].isna().sum() - df_reports["date"].isna().sum()
    print(f"æ— æ•ˆæ—¥æœŸæ ‡è®°ä¸ºç©ºï¼š{invalid_date_count} è¡Œ")

    final_count = len(df_clean)
    deleted_total = initial_count - final_count
    print(f"\nè´¢åŠ¡æ•°æ®æ¸…ç†å®Œæˆï¼š")
    print(f"   - åŸå§‹æ€»è¡Œæ•°ï¼š{initial_count}")
    print(f"   - æ¸…ç†åæ€»è¡Œæ•°ï¼š{final_count}")
    print(f"   - ç´¯è®¡åˆ é™¤è„æ•°æ®ï¼š{deleted_total} è¡Œ")

    df_clean.to_csv(processed_path, index=False, encoding="utf-8-sig")
    print(f"\nè´¢åŠ¡æ•°æ®å¤„ç†å®Œæˆï¼")
    print(f"è´¢åŠ¡å¤„ç†åæ–‡ä»¶ä½ç½®ï¼š{processed_path}\n")


# --------------------------
# å¢å¼ºéªŒè¯å‡½æ•°
# --------------------------
def validate_adjustment_results(processed_path):
    """éªŒè¯å¤æƒè®¡ç®—ç»“æœ"""
    try:
        df = pd.read_csv(processed_path)
        df["date"] = pd.to_datetime(df["date"], errors="coerce")

        print("=" * 50)
        print("éªŒè¯å¤æƒè®¡ç®—ç»“æœ...")
        print("=" * 50)

        print(f"æ•°æ®æ€»è¡Œæ•°: {len(df)}")
        print(f"è‚¡ç¥¨æ•°é‡: {df['stock_id'].nunique()}")

        if "adj_close" in df.columns:
            nan_count = df["adj_close"].isna().sum()
            print(f"å¤æƒä»·æ ¼NaNå€¼æ•°é‡: {nan_count}")

            df["adj_return"] = df.groupby("stock_id")["adj_close"].pct_change()
            extreme_returns = (df["adj_return"].abs() > 0.2).sum()
            print(f"å¤æƒåæ”¶ç›Šç‡ç»å¯¹å€¼>20%çš„å¤©æ•°: {extreme_returns}")

            if "is_adjustment_day" in df.columns:
                adjustment_data = df[df["is_adjustment_day"]]
                if len(adjustment_data) > 0:
                    print(f"\nè°ƒæ•´æ—¥ç»Ÿè®¡:")
                    print(f"  æ€»è°ƒæ•´æ—¥æ•°: {len(adjustment_data)}")
                    print(f"  å¹³å‡è°ƒæ•´æ¯”ç‡: {adjustment_data['adjustment_ratio'].mean():.4f}")
                    print(
                        f"  è°ƒæ•´æ¯”ç‡èŒƒå›´: {adjustment_data['adjustment_ratio'].min():.4f} - {adjustment_data['adjustment_ratio'].max():.4f}")

            sample_stocks = df["stock_id"].unique()[:3] if len(df["stock_id"].unique()) >= 3 else df[
                "stock_id"].unique()

            for stock_id in sample_stocks:
                stock_data = df[df["stock_id"] == stock_id].copy()
                stock_data = stock_data.sort_values("date")

                if len(stock_data) > 1:
                    raw_returns = stock_data["close"].pct_change()
                    adj_returns = stock_data["adj_close"].pct_change()

                    print(f"\nè‚¡ç¥¨ {stock_id} çš„å¤æƒéªŒè¯:")
                    print(f"  æ•°æ®å¤©æ•°: {len(stock_data)}")
                    print(f"  åŸå§‹ä»·æ ¼èŒƒå›´: {stock_data['close'].min():.2f} - {stock_data['close'].max():.2f}")
                    print(f"  å¤æƒä»·æ ¼èŒƒå›´: {stock_data['adj_close'].min():.2f} - {stock_data['adj_close'].max():.2f}")
                    print(f"  åŸå§‹æ”¶ç›Šç‡æ ‡å‡†å·®: {raw_returns.std():.6f}")
                    print(f"  å¤æƒæ”¶ç›Šç‡æ ‡å‡†å·®: {adj_returns.std():.6f}")

                    if "is_adjustment_day" in stock_data.columns:
                        adjustment_days = stock_data["is_adjustment_day"].sum()
                        if adjustment_days > 0:
                            adj_dates = stock_data[stock_data["is_adjustment_day"]]["date"]
                            print(f"  è°ƒæ•´æ—¥æ•°é‡: {adjustment_days}")
                            if len(adj_dates) > 0:
                                dates_str = ', '.join([d.strftime('%Y-%m-%d') for d in adj_dates[:3]])
                                if len(adj_dates) > 3:
                                    dates_str += f" ... (å…±{len(adj_dates)}ä¸ª)"
                                print(f"  è°ƒæ•´æ—¥æœŸ: {dates_str}")

        return True
    except Exception as e:
        print(f"éªŒè¯å¤±è´¥: {str(e)}")
        return False


# --------------------------
# ä¸»å‡½æ•°ï¼šç»Ÿä¸€æ‰§è¡Œæ‰€æœ‰æ•°æ®å¤„ç†
# --------------------------
def main():
    """ä¸»å‡½æ•°ï¼šä¾æ¬¡å¤„ç†è‚¡ä»·æ•°æ®å’Œè´¢åŠ¡æ•°æ®"""
    try:
        paths = get_project_paths()

        print("=" * 60)
        print("STARKTECH è‚¡ç¥¨æ•°æ®å¤„ç†ç³»ç»Ÿ")
        print("=" * 60)

        # å¤„ç†è‚¡ä»·æ•°æ®
        stock_df = process_stock_data(paths)

        # éªŒè¯è‚¡ä»·æ•°æ®
        print("\n" + "=" * 50)
        print("å¼€å§‹éªŒè¯è‚¡ä»·æ•°æ®...")
        validation_result = validate_adjustment_results(paths["stock_processed"])
        if validation_result:
            print("è‚¡ä»·æ•°æ®éªŒè¯é€šè¿‡")
        else:
            print("è‚¡ä»·æ•°æ®éªŒè¯å‘ç°é—®é¢˜")

        # å¤„ç†è´¢åŠ¡æ•°æ®
        print("\n" + "=" * 50)
        process_report_data(paths)

        print("=" * 60)
        print("æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"å¤„ç†åæ–‡ä»¶å‡ä¿å­˜åœ¨ï¼š{os.path.dirname(paths['stock_processed'])}")
        print("=" * 60)

        # é¢å¤–ç»Ÿè®¡ä¿¡æ¯
        if 'stock_df' in locals():
            print("\næœ€ç»ˆæ•°æ®æ¦‚è§ˆ:")
            print(f"è‚¡ç¥¨æ•°é‡: {stock_df['stock_id'].nunique()}")
            print(f"æ—¶é—´èŒƒå›´: {stock_df['date'].min().date()} åˆ° {stock_df['date'].max().date()}")
            print(f"æ€»äº¤æ˜“æ—¥æ•°: {stock_df['date'].nunique()}")

            # æ£€æŸ¥å¤æƒæ•ˆæœ
            if 'adj_return' in stock_df.columns:
                extreme_up = (stock_df['adj_return'] > 0.1).sum()
                extreme_down = (stock_df['adj_return'] < -0.1).sum()
                print(f"å¤§å¹…ä¸Šæ¶¨(>10%)å¤©æ•°: {extreme_up}")
                print(f"å¤§å¹…ä¸‹è·Œ(<-10%)å¤©æ•°: {extreme_down}")

    except Exception as e:
        print(f"\næ•°æ®å¤„ç†å‡ºé”™ï¼š{str(e)}")
        import traceback
        traceback.print_exc()
        raise


# --------------------------
# æ‰§è¡Œä¸»å‡½æ•°
# --------------------------
if __name__ == "__main__":
    main()