import numpy as np
import pandas as pd
import warnings

warnings.filterwarnings('ignore')
from datetime import datetime, timedelta
import os
import pickle
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, \
    classification_report, confusion_matrix, roc_curve, auc
import xgboost as xgb
import lightgbm as lgb
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
import re
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV, cross_val_score
import traceback
from tqdm import tqdm
import gc
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats.mstats import winsorize
from sklearn.linear_model import LinearRegression
import time
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
import optuna
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
import shap
import joblib
import json
from collections import defaultdict

# ==================== å‚æ•°è®¾ç½® ====================
FUTURE_DAYS = 20
LOOKBACK_DAYS = 30
USE_PKL_CACHE = False

# æ•°æ®è·¯å¾„
PRICE_DATA_PATH = 'taiwan_stock_cleaned_adjusted.csv'
REPORTS_DATA_PATH = 'reports_cleaned.csv'
PRE_MERGED_FILE = 'taiwan_stock_data_optimized.pkl'

# å·²ä¿å­˜æ•°æ®çš„æ–‡ä»¶å
LGB_FEATURE_IMPORTANCE_FILE = 'lgb_feature_importance.csv'
CORE_FACTORS_FILE = 'core_factors_top10.csv'
FACTOR_IC_METRICS_FILE = 'factor_ic_metrics.csv'
FINANCIAL_IC_FILE = 'financial_ic_metrics.csv'
TECHNICAL_IC_FILE = 'technical_ic_metrics.csv'

# æ¨¡å‹å‚æ•°
RANDOM_STATE = 42
TEST_RATIO = 0.2
VAL_RATIO = 0.1
N_JOBS = -1

# æ€§èƒ½ä¼˜åŒ–å‚æ•°
MAX_SAMPLES = 200000
CHUNK_SIZE = 1000
FEATURE_SELECTION_THRESHOLD = 0.001

QUICK_MODE = False
MAX_FEATURES = 50
HYPERPARAM_TRIALS = 10
SAMPLE_SIZE_TUNING = 5000
MERGE_OPTIMIZATION = True
QUICK_TUNING = True
FORCE_REMERGE = False

# ==================== å¿«é€Ÿæµ‹è¯•æ¨¡å¼ ====================
TEST_LIGHTGBM_ONLY = False
LIGHTGBM_TEST_SAMPLE_SIZE = 20000
LIGHTGBM_TEST_FEATURES = 30

# å› å­ç­›é€‰å‚æ•°
FIN_IC_MEAN_THRESHOLD = 0.008
FIN_ICIR_THRESHOLD = 0.1
FIN_WINRATE_THRESHOLD = 0.5
FIN_CORR_THRESHOLD = 0.85

TECH_IC_MEAN_THRESHOLD = 0.005
TECH_ICIR_THRESHOLD = 0.05
TECH_WINRATE_THRESHOLD = 0.5
TECH_CORR_THRESHOLD = 0.90

# ç¨³å®šæ€§æ£€æŸ¥å‚æ•°
ROLLING_WINDOW_MONTHS = 6
ROLLING_STD_THRESHOLD = 0.1

# å»æå€¼å‚æ•°
WINSORIZE_LIMITS = (0.01, 0.01)

# æ»šåŠ¨äº¤å‰éªŒè¯å‚æ•°
ROLLING_CV_SPLITS = 5
USE_ROLLING_CV = True
ENFORCE_ROLLING_CV_FOR_ALL_MODELS = True

# æ‰©å±•å‚æ•°ç½‘æ ¼
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# ==================== åˆ†å±‚å›æµ‹å‚æ•° ====================
STRATIFIED_BACKTEST = True
N_STRATIFICATION = 5
HOLDING_PERIOD = 15
REBALANCE_MONTHLY = True
REBALANCE_DAY = 1
TOP_N = None

# åˆ†å±‚å›æµ‹éªŒè¯æŒ‡æ ‡é˜ˆå€¼
MIN_TOP_BOTTOM_SPREAD = 0.0
SHARPE_THRESHOLD = 1.0
MONOTONICITY_THRESHOLD = 0.7

# ==================== å›æµ‹å‚æ•°è®¾ç½® ====================
# äº¤æ˜“æˆæœ¬å‚æ•°
TRANSACTION_COSTS = {
    'commission': 0.001425,  # æ‰‹ç»­è´¹0.1425%
    'tax': 0.003,  # è¯äº¤ç¨0.3%ï¼ˆå–å‡ºæ—¶ï¼‰
    'slippage': 0.0005  # æ»‘ç‚¹0.05%
}
TOTAL_COST_PER_TRADE = 0.005  # æ€»æˆæœ¬çº¦0.5%æ¯æ¬¡äº¤æ˜“

# é£æ§å‚æ•°
RISK_CONTROL = {
    'single_stock_limit': 0.08,      # å•åªè‚¡ç¥¨ä¸Šé™ï¼š8% â† é€‚å½“æ”¾å®½
    'monthly_turnover_limit': 0.25,   # æœˆæ¢æ‰‹ç‡é™åˆ¶ï¼š<25% â† é™ä½æ¢æ‰‹ç‡ç›®æ ‡
    'individual_stop_loss': -0.20,   # ä¸ªè‚¡æ­¢æŸï¼š-20% â† æ”¾å®½æ­¢æŸé˜ˆå€¼
    'individual_stop_profit': 0.30,  # ä¸ªè‚¡æ­¢ç›ˆï¼š+30% â† æé«˜æ­¢ç›ˆé˜ˆå€¼
    'portfolio_stop_loss': -0.25,    # ç»„åˆæœ€å¤§å›æ’¤>25%æ—¶å‡ä»“ â† æ”¾å®½ç»„åˆæ­¢æŸ
    'reduction_ratio': 0.2,          # å‡ä»“æ¯”ä¾‹ï¼š20% â† æ›´æ¸©å’Œçš„å‡ä»“
    'min_holding_days': 60,          # æœ€å°æŒæœ‰å¤©æ•°æ”¹ä¸º60å¤© â† å¼ºåˆ¶é•¿çº¿æŒæœ‰
    'max_daily_trades': 3,           # æ¯æ—¥æœ€å¤§äº¤æ˜“æ¬¡æ•°å‡å°‘ä¸º3æ¬¡
    'drawdown_check_frequency': 30   # å›æ’¤æ£€æŸ¥é¢‘ç‡æ”¹ä¸º30å¤©ä¸€æ¬¡ â† é™ä½æ£€æŸ¥é¢‘ç‡
}

# å›æµ‹ç›®æ ‡æŒ‡æ ‡
TARGET_METRICS = {
    'annual_return': 0.08,
    'sharpe_ratio': 0.6,
    'max_drawdown': 0.25,
    'information_ratio': 0.2
}

# å›æµ‹åŸºç¡€å‚æ•°
INITIAL_CAPITAL = 1000000  # åˆå§‹èµ„é‡‘100ä¸‡
REBALANCE_FREQUENCY = 'quarterly'  # è°ƒä»“é¢‘ç‡ï¼šæ¯æœˆ
TOP_N_HOLDINGS = 8  # æœ€å¤§æŒä»“æ•°é‡
MIN_HOLDING_DAYS = 60  # æœ€å°æŒæœ‰å¤©æ•°ï¼ˆæ”¹ä¸º20å¤©ï¼‰

# æŒä»“å‘¨æœŸå‚æ•°
HOLDING_PERIOD = 20  # æ”¹ä¸º20å¤©ï¼Œæ¥è¿‘ä¸€ä¸ªæœˆ

# æ˜¯å¦ä½¿ç”¨å·²ä¿å­˜çš„æ•°æ®
USE_SAVED_DATA = True
FORCE_RECOMPUTE_FACTORS = False

def timer_decorator(func):
    """è®¡æ—¶è£…é¥°å™¨"""

    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f" {func.__name__} æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’")
        return result

    return wrapper
# ==================== æ–°å¢ï¼šç¼“å­˜äº¤æ˜“æˆæœ¬è®¡ç®— ====================
class TransactionCostCache:
    """äº¤æ˜“æˆæœ¬ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—"""

    def __init__(self):
        self.cache = {}

    def get_cost(self, trade_value, is_buy=True, use_cache=True):
        """è·å–äº¤æ˜“æˆæœ¬ï¼Œå¯é€‰ä½¿ç”¨ç¼“å­˜"""
        if not use_cache:
            return self._calculate_cost(trade_value, is_buy)

        # åˆ›å»ºç¼“å­˜é”®
        cache_key = f"{trade_value:.2f}_{is_buy}"

        if cache_key in self.cache:
            return self.cache[cache_key]

        # è®¡ç®—å¹¶ç¼“å­˜
        cost = self._calculate_cost(trade_value, is_buy)
        self.cache[cache_key] = cost
        return cost

    def _calculate_cost(self, trade_value, is_buy=True):
        """å®é™…è®¡ç®—äº¤æ˜“æˆæœ¬"""
        commission = trade_value * TRANSACTION_COSTS['commission']
        tax = 0
        if not is_buy:  # å–å‡ºæ—¶å¾æ”¶è¯äº¤ç¨
            tax = trade_value * TRANSACTION_COSTS['tax']
        slippage = trade_value * TRANSACTION_COSTS['slippage']
        return commission + tax + slippage


# å…¨å±€äº¤æ˜“æˆæœ¬ç¼“å­˜å®ä¾‹
transaction_cost_cache = TransactionCostCache()


# ==================== æ–°å¢ï¼šä»“ä½ç®¡ç†å™¨ ====================
class PositionManager:
    """ä»“ä½ç®¡ç†å™¨ï¼Œç¡®ä¿ä»“ä½ä¸è¶…é™"""

    def __init__(self, max_position_ratio=0.05):
        self.max_position_ratio = max_position_ratio
        self.positions = {}
        self.total_value = 0

    def can_add_position(self, stock_code, target_value, current_total_value):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ·»åŠ ä»“ä½"""
        if current_total_value <= 0:
            return True

        # è®¡ç®—å½“å‰ä»“ä½
        current_position_value = self.positions.get(stock_code, 0)
        new_total_value = current_total_value + target_value

        # è®¡ç®—æ–°ä»“ä½æ¯”ä¾‹
        new_position_ratio = (current_position_value + target_value) / new_total_value

        return new_position_ratio <= self.max_position_ratio

    def update_position(self, stock_code, value_change):
        """æ›´æ–°ä»“ä½"""
        current_value = self.positions.get(stock_code, 0)
        self.positions[stock_code] = current_value + value_change

    def update_total_value(self, total_value):
        """æ›´æ–°æ€»å¸‚å€¼"""
        self.total_value = total_value

    def get_position_ratio(self, stock_code):
        """è·å–ä»“ä½æ¯”ä¾‹"""
        if self.total_value <= 0:
            return 0
        return self.positions.get(stock_code, 0) / self.total_value

    def check_all_positions(self):
        """æ£€æŸ¥æ‰€æœ‰ä»“ä½æ˜¯å¦è¶…é™"""
        violations = []
        for stock_code, position_value in self.positions.items():
            ratio = position_value / self.total_value if self.total_value > 0 else 0
            if ratio > self.max_position_ratio:
                violations.append((stock_code, ratio))
        return violations


# ==================== ä¼˜åŒ–äº¤æ˜“æˆæœ¬è®¡ç®—å‡½æ•° ====================
@timer_decorator
def calculate_transaction_costs(trade_value, is_buy=True, use_cache=True):
    """
    è®¡ç®—äº¤æ˜“æˆæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼Œä½¿ç”¨ç¼“å­˜ï¼‰
    :param trade_value: äº¤æ˜“é‡‘é¢
    :param is_buy: æ˜¯å¦ä¹°å…¥ï¼ˆTrue:ä¹°å…¥, False:å–å‡ºï¼‰
    :param use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    :return: äº¤æ˜“æˆæœ¬
    """
    return transaction_cost_cache.get_cost(trade_value, is_buy, use_cache)


# ==================== ä¼˜åŒ–RiskControlManagerç±» ====================
class RiskControlManager:
    """é£æ§ç®¡ç†å™¨ï¼ˆä¿®å¤ç‰ˆï¼‰"""

    def __init__(self):
        self.portfolio_value = INITIAL_CAPITAL
        self.positions = {}
        self.trading_records = []
        self.daily_portfolio_values = []
        self.max_portfolio_value = INITIAL_CAPITAL
        self.position_manager = PositionManager(RISK_CONTROL['single_stock_limit'])
        self.trade_count_today = 0
        self.last_trade_date = None

        # æ–°å¢ï¼šé¿å…é¢‘ç¹æ­¢æŸ
        self.last_drawdown_check_date = None
        self.consecutive_stop_loss = 0  # è¿ç»­æ­¢æŸæ¬¡æ•°
        self.stop_loss_cooldown = False  # æ­¢æŸå†·å´æœŸ

    def check_portfolio_drawdown(self, current_value, current_date):
        """æ£€æŸ¥ç»„åˆå›æ’¤ï¼ˆä¿®å¤ç‰ˆï¼šé¿å…é¢‘ç¹è§¦å‘ï¼‰"""
        self.max_portfolio_value = max(self.max_portfolio_value, current_value)
        drawdown = (current_value - self.max_portfolio_value) / self.max_portfolio_value

        # æ£€æŸ¥é¢‘ç‡æ§åˆ¶
        if self.last_drawdown_check_date is None:
            self.last_drawdown_check_date = current_date
            return False, drawdown

        days_since_check = (current_date - self.last_drawdown_check_date).days
        if days_since_check < RISK_CONTROL.get('drawdown_check_frequency', 10):
            return False, drawdown

        self.last_drawdown_check_date = current_date

        # è¿ç»­æ­¢æŸä¿æŠ¤
        if self.stop_loss_cooldown:
            if self.consecutive_stop_loss >= 3:
                print(f"è¿ç»­æ­¢æŸ{self.consecutive_stop_loss}æ¬¡ï¼Œè¿›å…¥å†·å´æœŸ")
                self.stop_loss_cooldown = False
                self.consecutive_stop_loss = 0
            return False, drawdown

        if drawdown <= RISK_CONTROL['portfolio_stop_loss']:
            self.consecutive_stop_loss += 1
            if self.consecutive_stop_loss > 2:
                self.stop_loss_cooldown = True
            return True, drawdown

        # é‡ç½®è¿ç»­æ­¢æŸè®¡æ•°
        if drawdown > -0.05:  # å›æ’¤å°äº5%æ—¶é‡ç½®
            self.consecutive_stop_loss = 0
            self.stop_loss_cooldown = False

        return False, drawdown

    def reduce_positions(self, reduction_ratio=None):
        """å‡ä»“æ“ä½œï¼ˆä¿®å¤ç‰ˆï¼šæ›´æ¸©å’Œï¼‰"""
        if reduction_ratio is None:
            reduction_ratio = RISK_CONTROL['reduction_ratio']

        # æ ¹æ®è¿ç»­æ­¢æŸæ¬¡æ•°è°ƒæ•´å‡ä»“æ¯”ä¾‹
        if self.consecutive_stop_loss >= 2:
            reduction_ratio = min(0.2, reduction_ratio * 0.5)  # æ›´æ¸©å’Œçš„å‡ä»“
            print(f"è¿ç»­æ­¢æŸ{self.consecutive_stop_loss}æ¬¡ï¼Œé‡‡ç”¨æ¸©å’Œå‡ä»“{reduction_ratio:.0%}")

        print(f"è§¦å‘ç»„åˆæ­¢æŸï¼Œå‡ä»“{reduction_ratio:.0%}")

        # æŒ‰æŒä»“æ¯”ä¾‹æ’åºï¼Œä¼˜å…ˆå‡ä»“äºæŸæœ€å¤šçš„è‚¡ç¥¨
        positions_to_reduce = []
        for stock_code, position in self.positions.items():
            if 'current_price' in position and 'avg_price' in position:
                current_price = position['current_price']
                avg_price = position['avg_price']
                profit_ratio = (current_price - avg_price) / avg_price
                positions_to_reduce.append((stock_code, position, profit_ratio))

        # æŒ‰ç›ˆåˆ©æƒ…å†µæ’åºï¼ˆäºæŸæœ€å¤šçš„ä¼˜å…ˆï¼‰
        positions_to_reduce.sort(key=lambda x: x[2])

        # æ‰§è¡Œå‡ä»“
        total_reduction_value = 0
        target_reduction = self.portfolio_value * reduction_ratio

        for stock_code, position, profit_ratio in positions_to_reduce:
            if total_reduction_value >= target_reduction:
                break

            # æ¸©å’Œå‡ä»“ï¼šæœ€å¤šå‡ä»“30%çš„æŒä»“
            max_reduce_ratio = 0.3
            reduce_shares = int(position['shares'] * min(reduction_ratio, max_reduce_ratio))

            if reduce_shares > 0:
                current_price = position.get('current_price', position['avg_price'])
                trade_value = reduce_shares * current_price

                if total_reduction_value + trade_value > target_reduction:
                    remaining_reduction = target_reduction - total_reduction_value
                    reduce_shares = int(remaining_reduction / current_price)

                if reduce_shares > 0:
                    actual_shares = self.execute_sell(stock_code, reduce_shares, current_price,
                                                      reason='portfolio_stop_loss')
                    total_reduction_value += actual_shares * current_price

        print(f"å®é™…å‡ä»“é‡‘é¢: {total_reduction_value:.2f}")
        return total_reduction_value

# ==================== ä¼˜åŒ–äº¤æ˜“ç»Ÿè®¡ç”Ÿæˆ ====================
def generate_trading_statistics(trading_records, portfolio_values):
    """ç”Ÿæˆäº¤æ˜“ç»Ÿè®¡ - ä¿®å¤ç‰ˆæœ¬"""

    if not trading_records:
        return {
            'æ€»äº¤æ˜“æ¬¡æ•°': 0,
            'ä¹°å…¥æ¬¡æ•°': 0,
            'å–å‡ºæ¬¡æ•°': 0,
            'ç›ˆåˆ©äº¤æ˜“æ•°': 0,
            'äºæŸäº¤æ˜“æ•°': 0,
            'èƒœç‡': 0,
            'æ€»äº¤æ˜“æˆæœ¬': 0,
            'å¹³å‡æŒä»“å¤©æ•°': 0,
            'å¹³å‡äº¤æ˜“æˆæœ¬ç‡': 0,
            'å¹´åŒ–æ¢æ‰‹ç‡': 0,
            'æŒä»“å¤©æ•°åˆ†å¸ƒ': {}
        }

    # è½¬æ¢ä¸ºDataFrame
    trades_df = pd.DataFrame(trading_records)

    # åŸºæœ¬ç»Ÿè®¡
    total_trades = len(trades_df)
    buy_trades = len(trades_df[trades_df['type'] == 'buy'])
    sell_trades = len(trades_df[trades_df['type'] == 'sell'])

    # ç›ˆåˆ©äº¤æ˜“ç»Ÿè®¡
    sell_trades_df = trades_df[trades_df['type'] == 'sell']
    if not sell_trades_df.empty:
        profitable = len(sell_trades_df[sell_trades_df['profit'] > 0])
        loss_trades = len(sell_trades_df[sell_trades_df['profit'] <= 0])
        win_rate = profitable / sell_trades if sell_trades > 0 else 0
    else:
        profitable = 0
        loss_trades = 0
        win_rate = 0

    # æ€»äº¤æ˜“æˆæœ¬
    total_cost = trades_df['cost'].sum() if 'cost' in trades_df.columns else 0

    # å¹³å‡æŒä»“æ—¶é—´ - ä¿®å¤è¿™é‡Œ
    avg_hold_days = 0
    if 'hold_days' in trades_df.columns:
        # åªè®¡ç®—å–å‡ºäº¤æ˜“çš„æŒä»“å¤©æ•°
        sell_trades = trades_df[trades_df['type'] == 'sell']
        if not sell_trades.empty and 'hold_days' in sell_trades.columns:
            avg_hold_days = sell_trades['hold_days'].mean() if not sell_trades['hold_days'].isnull().all() else 0

    # è®¡ç®—æ¢æ‰‹ç‡
    if portfolio_values and len(portfolio_values) > 1:
        trades_df = pd.DataFrame(trading_records)
        turnover_rate = calculate_turnover_rate(trades_df, portfolio_values)
    else:
        turnover_rate = 0

    # å¹³å‡äº¤æ˜“æˆæœ¬ç‡
    total_trade_value = trades_df['total_value'].sum() if 'total_value' in trades_df.columns and trades_df[
        'total_value'].sum() > 0 else 1
    avg_cost_rate = total_cost / total_trade_value

    stats = {
        'æ€»äº¤æ˜“æ¬¡æ•°': total_trades,
        'ä¹°å…¥æ¬¡æ•°': buy_trades,
        'å–å‡ºæ¬¡æ•°': sell_trades,
        'ç›ˆåˆ©äº¤æ˜“æ•°': profitable,
        'äºæŸäº¤æ˜“æ•°': loss_trades,
        'èƒœç‡': win_rate,
        'æ€»äº¤æ˜“æˆæœ¬': total_cost,
        'å¹³å‡æŒä»“å¤©æ•°': avg_hold_days,
        'å¹³å‡äº¤æ˜“æˆæœ¬ç‡': avg_cost_rate,
        'å¹´åŒ–æ¢æ‰‹ç‡': turnover_rate
    }
    # åŒæ—¶è¾“å‡ºäº¤æ˜“è¯¦æƒ…
    if not trades_df.empty and 'type' in trades_df.columns:
        buy_amount = trades_df[trades_df['type'] == 'buy']['total_value'].sum()
        sell_amount = trades_df[trades_df['type'] == 'sell']['total_value'].sum()
        stats['ä¹°å…¥æ€»é¢'] = buy_amount
        stats['å–å‡ºæ€»é¢'] = sell_amount
        stats['æ€»äº¤æ˜“é¢'] = buy_amount + sell_amount
    # æ–°å¢ï¼šæŒä»“å¤©æ•°åˆ†å¸ƒç»Ÿè®¡
    hold_days_distribution = {}
    if 'hold_days' in trades_df.columns:
        sell_trades = trades_df[trades_df['type'] == 'sell']
        if not sell_trades.empty and 'hold_days' in sell_trades.columns:
            hold_days = sell_trades['hold_days'].dropna()
            if len(hold_days) > 0:
                # ç»Ÿè®¡åˆ†å¸ƒ
                hold_days_distribution = {
                    'â‰¤30å¤©': (hold_days <= 30).sum(),
                    '31-60å¤©': ((hold_days > 30) & (hold_days <= 60)).sum(),
                    '61-90å¤©': ((hold_days > 60) & (hold_days <= 90)).sum(),
                    '>90å¤©': (hold_days > 90).sum(),
                    'æœ€é•¿æŒä»“': hold_days.max() if not hold_days.empty else 0,
                    'æœ€çŸ­æŒä»“': hold_days.min() if not hold_days.empty else 0
                }

    # åœ¨statså­—å…¸ä¸­æ·»åŠ 
    stats['æŒä»“å¤©æ•°åˆ†å¸ƒ'] = hold_days_distribution
    return stats

# ==================== ä¼˜åŒ–é€‰è‚¡ç”Ÿæˆå‡½æ•° ====================
def generate_daily_selected_stocks(test_df, predictions, probabilities, top_n=10):
    """ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨ - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå‡å°‘é€‰è‚¡æ•°é‡ï¼Œæé«˜è´¨é‡ï¼‰"""
    print_section("ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")

    if test_df.empty or not predictions:
        print("æµ‹è¯•æ•°æ®æˆ–é¢„æµ‹ç»“æœä¸ºç©º")
        return pd.DataFrame()

    try:
        # ==================== 1. æ•°æ®å‡†å¤‡å’ŒéªŒè¯ ====================
        print("æ•°æ®å‡†å¤‡å’ŒéªŒè¯...")

        # å¤åˆ¶æµ‹è¯•é›†æ•°æ®
        required_cols = ['date', 'stock_code', 'close', 'future_return']
        missing_cols = [col for col in required_cols if col not in test_df.columns]
        if missing_cols:
            print(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return pd.DataFrame()

        selected_stocks = test_df[required_cols].copy()

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        initial_count = len(selected_stocks)
        selected_stocks = selected_stocks.dropna(subset=['future_return'])
        print(f"ç§»é™¤æœªæ¥æ”¶ç›Šç‡ç¼ºå¤±çš„æ•°æ®: {initial_count - len(selected_stocks):,} è¡Œ")

        if selected_stocks.empty:
            print("é€‰è‚¡æ•°æ®ä¸ºç©º")
            return pd.DataFrame()

        # æ·»åŠ æ¨¡å‹é¢„æµ‹æ¦‚ç‡
        for model_name in predictions.keys():
            if model_name in predictions and len(predictions[model_name]) == len(selected_stocks):
                selected_stocks[f'{model_name}_prediction'] = predictions[model_name]
                selected_stocks[f'{model_name}_probability'] = probabilities[model_name]
            else:
                print(f"æ¨¡å‹ {model_name} é¢„æµ‹ç»“æœé•¿åº¦ä¸åŒ¹é…ï¼Œè·³è¿‡")

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹è¿›è¡Œé€‰è‚¡
        available_models = [m for m in predictions.keys() if f'{m}_probability' in selected_stocks.columns]
        if available_models:
            best_model = available_models[0]
        else:
            best_model = 'rf'
            # å¦‚æœæ²¡æœ‰æ¨¡å‹æ¦‚ç‡ï¼Œä½¿ç”¨éšæœºåˆ†æ•°
            selected_stocks['selection_score'] = np.random.random(len(selected_stocks))
            print("æ— å¯ç”¨æ¨¡å‹æ¦‚ç‡ï¼Œä½¿ç”¨éšæœºé€‰è‚¡")

        print(f"ä½¿ç”¨æ¨¡å‹è¿›è¡Œé€‰è‚¡: {best_model.upper()}")
        selected_stocks['selection_score'] = selected_stocks[f'{best_model}_probability']

        # ==================== 2. ä¼˜åŒ–é€‰è‚¡é€»è¾‘ ====================
        print("ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨...")
        daily_top_stocks = []
        valid_dates = 0

        # è·å–å”¯ä¸€æ—¥æœŸå¹¶æ’åº
        unique_dates = sorted(selected_stocks['date'].unique())
        print(f"å¤„ç† {len(unique_dates)} ä¸ªäº¤æ˜“æ—¥çš„é€‰è‚¡...")

        for date in tqdm(unique_dates, desc="ç”Ÿæˆæ¯æ—¥é€‰è‚¡"):
            date_data = selected_stocks[selected_stocks['date'] == date].copy()

            if len(date_data) == 0:
                continue

            # æŒ‰é¢„æµ‹æ¦‚ç‡æ’åº
            date_data = date_data.sort_values('selection_score', ascending=False)
            date_data = date_data.drop_duplicates(subset=['stock_code'], keep='first')

            # ä¼˜åŒ–ï¼šæ·»åŠ æ³¢åŠ¨ç‡ç­›é€‰ï¼Œæ’é™¤é«˜æ³¢åŠ¨è‚¡ç¥¨
            if 'volatility' in date_data.columns:
                # å‡è®¾æœ‰æ³¢åŠ¨ç‡æ•°æ®
                volatility_threshold = date_data['volatility'].quantile(0.8)  # æ’é™¤æ³¢åŠ¨ç‡æœ€é«˜çš„20%
                date_data = date_data[date_data['volatility'] <= volatility_threshold]

            # ä¼˜åŒ–ï¼šå‡å°‘æ¯æ—¥é€‰è‚¡æ•°é‡ï¼Œæé«˜è´¨é‡
            daily_top_n = max(3, min(top_n, len(date_data) // 4))  # æ¯æ—¥é€‰3-10åª
            if len(date_data) < daily_top_n:
                # è‚¡ç¥¨æ•°é‡ä¸è¶³æ—¶ï¼Œä½¿ç”¨æ‰€æœ‰è‚¡ç¥¨ä½†å¢åŠ ç­›é€‰æ¡ä»¶
                date_data = date_data[date_data['selection_score'] > 0.6]  # åªé€‰æ¦‚ç‡>0.6çš„
                if len(date_data) == 0:
                    continue
            else:
                # é€‰æ‹©Top N
                date_data = date_data.head(daily_top_n)

            date_data['rank'] = range(1, len(date_data) + 1)
            daily_top_stocks.append(date_data)
            valid_dates += 1

        print(f"æˆåŠŸå¤„ç† {valid_dates}/{len(unique_dates)} ä¸ªäº¤æ˜“æ—¥çš„é€‰è‚¡")

        if not daily_top_stocks:
            print("æ²¡æœ‰ç”Ÿæˆä»»ä½•é€‰è‚¡åˆ—è¡¨")
            return pd.DataFrame()

        # ==================== 3. åˆå¹¶ç»“æœ ====================
        result_df = pd.concat(daily_top_stocks, ignore_index=True)

        # æ·»åŠ é€‰è‚¡ç†ç”±
        result_df['selection_reason'] = result_df.apply(
            lambda x: f"æ¨¡å‹é¢„æµ‹æ¦‚ç‡:{x['selection_score']:.3f}, æ’å:{x['rank']}",
            axis=1
        )

        # é‡å‘½ååˆ—
        result_df = result_df.rename(columns={
            'date': 'äº¤æ˜“æ—¥',
            'stock_code': 'è‚¡ç¥¨ä»£ç ',
            'close': 'æ”¶ç›˜ä»·',
            'future_return': 'æœªæ¥20å¤©ç»å¯¹æ”¶ç›Šç‡',
            'selection_score': 'æ¨¡å‹é¢„æµ‹æ¦‚ç‡',
            'rank': 'å½“æ—¥æ’å',
            'selection_reason': 'é€‰è‚¡ç†ç”±'
        })

        # é€‰æ‹©éœ€è¦çš„åˆ—
        final_columns = ['äº¤æ˜“æ—¥', 'è‚¡ç¥¨ä»£ç ', 'æ”¶ç›˜ä»·', 'æœªæ¥20å¤©ç»å¯¹æ”¶ç›Šç‡',
                         'æ¨¡å‹é¢„æµ‹æ¦‚ç‡', 'å½“æ—¥æ’å', 'é€‰è‚¡ç†ç”±']
        final_columns = [col for col in final_columns if col in result_df.columns]
        result_df = result_df[final_columns]

        print(f"ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨: {result_df.shape}")

        # ==================== 4. é€‰è‚¡ç»Ÿè®¡ ====================
        print_section("é€‰è‚¡ç»“æœç»Ÿè®¡")

        total_stocks = len(result_df)
        unique_stocks = result_df['è‚¡ç¥¨ä»£ç '].nunique()
        avg_daily_stocks = result_df.groupby('äº¤æ˜“æ—¥').size().mean()
        avg_prob_all = result_df['æ¨¡å‹é¢„æµ‹æ¦‚ç‡'].mean()

        print(f"é€‰è‚¡ç»Ÿè®¡:")
        print(f"   æ€»é€‰è‚¡è®°å½•: {total_stocks:,} æ¡")
        print(f"   å”¯ä¸€è‚¡ç¥¨æ•°é‡: {unique_stocks} åª")
        print(f"   å¹³å‡æ¯æ—¥é€‰è‚¡: {avg_daily_stocks:.1f} åª")
        print(f"   å¹³å‡é¢„æµ‹æ¦‚ç‡: {avg_prob_all:.3f}")

        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤é€‰è‚¡
        duplicate_check = result_df.groupby(['äº¤æ˜“æ—¥', 'è‚¡ç¥¨ä»£ç ']).size()
        if (duplicate_check > 1).any():
            print("âš ï¸ è­¦å‘Šï¼šå‘ç°é‡å¤é€‰è‚¡è®°å½•")
            duplicates = duplicate_check[duplicate_check > 1]
            print(f"é‡å¤è®°å½•æ•°é‡: {len(duplicates)}")

        return result_df

    except Exception as e:
        print(f"ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨å¤±è´¥: {e}")
        traceback.print_exc()
        return pd.DataFrame()


# ==================== ä¿®å¤çš„ç®€åŒ–å›æµ‹å‡½æ•° ====================
@timer_decorator
def perform_backtest_simple(daily_selected_df, test_df, initial_capital=INITIAL_CAPITAL):
    """
    æŒ‰å­£åº¦è°ƒä»“çš„ç®€åŒ–ç‰ˆå›æµ‹å‡½æ•°
    """
    print_section("æŒ‰å­£åº¦è°ƒä»“çš„ç®€åŒ–ç‰ˆå›æµ‹")

    try:
        # å‡†å¤‡æ•°æ®
        backtest_data = daily_selected_df.copy()

        # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
        if 'è‚¡ç¥¨ä»£ç ' in backtest_data.columns:
            backtest_data = backtest_data.rename(columns={
                'è‚¡ç¥¨ä»£ç ': 'stock_code',
                'äº¤æ˜“æ—¥': 'date',
                'æ”¶ç›˜ä»·': 'close',
                'æœªæ¥20å¤©ç»å¯¹æ”¶ç›Šç‡': 'future_return'
            })

        # è·å–å”¯ä¸€æ—¥æœŸå¹¶æ’åº
        unique_dates = sorted(backtest_data['date'].unique())
        if len(unique_dates) == 0:
            print("é”™è¯¯ï¼šæ²¡æœ‰å›æµ‹æ—¥æœŸ")
            return None

        print(f"å›æµ‹æœŸé—´: {unique_dates[0]} åˆ° {unique_dates[-1]}")
        print(f"æ€»äº¤æ˜“æ—¥æ•°: {len(unique_dates)}")

        # è·å–ä»·æ ¼æ•°æ®å­—å…¸ä»¥ä¾¿å¿«é€ŸæŸ¥è¯¢
        price_dict = {}
        if test_df is not None and not test_df.empty:
            for stock_code in test_df['stock_code'].unique():
                stock_data = test_df[test_df['stock_code'] == stock_code]
                if not stock_data.empty:
                    price_dict[stock_code] = dict(zip(stock_data['date'], stock_data['close']))

        # ==================== å…³é”®ä¿®æ”¹ï¼šæŒ‰å­£åº¦è°ƒä»“ ====================
        # ç¡®å®šæ¯å­£åº¦è°ƒä»“æ—¥ï¼ˆæ¯å­£åº¦ç¬¬ä¸€ä¸ªäº¤æ˜“æ—¥ï¼‰
        quarterly_rebalance_dates = []
        current_quarter = None
        for date in unique_dates:
            quarter = (date.year, (date.month - 1) // 3 + 1)  # è®¡ç®—å­£åº¦
            if quarter != current_quarter:
                quarterly_rebalance_dates.append(date)
                current_quarter = quarter

        print(f"æ¯å­£åº¦è°ƒä»“æ—¥æ•°é‡: {len(quarterly_rebalance_dates)}")
        print(f"è°ƒä»“æ—¥: {[d.strftime('%Y-%m-%d') for d in quarterly_rebalance_dates[:5]]}...")

        # ç®€å•å›æµ‹é€»è¾‘ï¼šæ¯æœˆç­‰æƒé‡ä¹°å…¥é€‰ä¸­çš„è‚¡ç¥¨ï¼ŒæŒæœ‰åˆ°ä¸‹ä¸ªæœˆè°ƒä»“æ—¥å–å‡º
        portfolio_value = initial_capital
        cash = initial_capital
        holdings = {}  # {è‚¡ç¥¨ä»£ç : {'shares': æ•°é‡, 'buy_date': ä¹°å…¥æ—¥æœŸ, 'buy_price': ä¹°å…¥ä»·æ ¼}}

        portfolio_values = []
        portfolio_returns = []
        trading_records = []

        # æŒ‰æ—¥æœŸæ’åº
        backtest_data = backtest_data.sort_values('date')

        # æ¨¡æ‹Ÿæ¯æ—¥äº¤æ˜“ï¼Œä½†åªåœ¨è°ƒä»“æ—¥äº¤æ˜“
        for i, current_date in enumerate(tqdm(unique_dates, desc="æ‰§è¡Œå›æµ‹")):
            # ==================== å…³é”®ä¿®æ”¹ï¼šåªåœ¨è°ƒä»“æ—¥äº¤æ˜“ ====================
            is_rebalance_day = current_date in quarterly_rebalance_dates

            # å¦‚æœæ˜¯è°ƒä»“æ—¥ï¼Œå–å‡ºæ‰€æœ‰æŒä»“ï¼ˆä¸ŠæœˆæŒä»“ï¼‰
            if is_rebalance_day and holdings:
                print(f"\nè°ƒä»“æ—¥ {current_date.date()}: å–å‡ºä¸ŠæœˆæŒä»“")
                stocks_to_sell = list(holdings.keys())
                # æ·»åŠ æœ€å°æŒæœ‰æœŸæ£€æŸ¥
                filtered_stocks_to_sell = []
                for stock_code in stocks_to_sell:
                    holding = holdings[stock_code]
                    hold_days = (current_date - holding['buy_date']).days

                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°æŒæœ‰æœŸ
                    if hold_days >= RISK_CONTROL['min_holding_days']:
                        filtered_stocks_to_sell.append(stock_code)
                    else:
                        print(
                            f"  è‚¡ç¥¨ {stock_code} æŒæœ‰ä»… {hold_days} å¤©ï¼Œæœªè¾¾ {RISK_CONTROL['min_holding_days']} å¤©æœ€å°æŒæœ‰æœŸï¼Œè·³è¿‡å–å‡º")


                for stock_code in stocks_to_sell:
                    if stock_code in price_dict and current_date in price_dict[stock_code]:
                        sell_price = price_dict[stock_code][current_date]
                        holding = holdings[stock_code]
                        shares = holding['shares']
                        buy_price = holding['buy_price']

                        # è®¡ç®—äº¤æ˜“ä»·å€¼
                        trade_value = shares * sell_price

                        # è®¡ç®—äº¤æ˜“æˆæœ¬
                        cost = calculate_transaction_costs(trade_value, is_buy=False)

                        # è®¡ç®—ç›ˆäº
                        profit = trade_value - (shares * buy_price) - cost
                        return_rate = profit / (shares * buy_price) if shares * buy_price > 0 else 0

                        # è®¡ç®—æŒä»“å¤©æ•°
                        hold_days = (current_date - holding['buy_date']).days

                        # æ›´æ–°ç°é‡‘
                        cash += trade_value - cost

                        # è®°å½•äº¤æ˜“
                        trading_records.append({
                            'date': current_date,
                            'type': 'sell',
                            'stock_code': stock_code,
                            'shares': shares,
                            'price': sell_price,
                            'cost': cost,
                            'total_value': trade_value,
                            'profit': profit,
                            'return_rate': return_rate,
                            'hold_days': hold_days,
                            'reason': 'monthly_rebalance'
                        })

                        # ç§»é™¤æŒä»“
                        del holdings[stock_code]

            # å¦‚æœæ˜¯è°ƒä»“æ—¥ï¼Œä¹°å…¥æ–°è‚¡ç¥¨ï¼ˆå¦‚æœç°é‡‘å……è¶³ï¼‰
            if is_rebalance_day and cash > 0:
                print(f"è°ƒä»“æ—¥ {current_date.date()}: ä¹°å…¥æ–°è‚¡ç¥¨")
                # è·å–å½“æ—¥é€‰ä¸­çš„è‚¡ç¥¨
                daily_stocks = backtest_data[backtest_data['date'] == current_date]

                if not daily_stocks.empty:
                    # ç­‰æƒé‡åˆ†é…ç°é‡‘
                    num_stocks = min(len(daily_stocks), TOP_N_HOLDINGS)
                    cash_per_stock = cash / num_stocks if num_stocks > 0 else 0

                    bought_count = 0
                    for idx, row in daily_stocks.head(num_stocks).iterrows():
                        stock_code = row['stock_code']

                        # è·å–å½“å‰ä»·æ ¼
                        if stock_code in price_dict and current_date in price_dict[stock_code]:
                            buy_price = price_dict[stock_code][current_date]

                            # è®¡ç®—å¯ä¹°æ•°é‡
                            max_shares = int(cash_per_stock / buy_price)
                            if max_shares > 0:
                                # è®¡ç®—äº¤æ˜“æˆæœ¬
                                trade_value = max_shares * buy_price
                                cost = calculate_transaction_costs(trade_value, is_buy=True)

                                # ç¡®ä¿æœ‰è¶³å¤Ÿç°é‡‘
                                if cash >= trade_value + cost:
                                    # ä¹°å…¥
                                    holdings[stock_code] = {
                                        'shares': max_shares,
                                        'buy_date': current_date,
                                        'buy_price': buy_price
                                    }

                                    # æ›´æ–°ç°é‡‘
                                    cash -= (trade_value + cost)

                                    # è®°å½•äº¤æ˜“
                                    trading_records.append({
                                        'date': current_date,
                                        'type': 'buy',
                                        'stock_code': stock_code,
                                        'shares': max_shares,
                                        'price': buy_price,
                                        'cost': cost,
                                        'total_value': trade_value,
                                        'reason': 'monthly_rebalance'
                                    })

                                    bought_count += 1

                    print(f"  ä¹°å…¥ {bought_count} åªè‚¡ç¥¨ï¼Œç°é‡‘å‰©ä½™: {cash:.2f}")

            # è®¡ç®—å½“æ—¥ç»„åˆä»·å€¼
            positions_value = 0
            for stock_code, holding in holdings.items():
                if stock_code in price_dict and current_date in price_dict[stock_code]:
                    current_price = price_dict[stock_code][current_date]
                    positions_value += holding['shares'] * current_price

            total_value = cash + positions_value
            portfolio_value = total_value  # æ›´æ–°ç»„åˆä»·å€¼

            # è®°å½•æ¯æ—¥ç»„åˆä»·å€¼
            portfolio_values.append({
                'date': current_date,
                'portfolio_value': portfolio_value,
                'cash': cash,
                'positions_value': positions_value,
                'total_value': total_value
            })

            # è®¡ç®—æ—¥æ”¶ç›Šç‡
            if i > 0:
                prev_value = portfolio_values[i - 1]['portfolio_value']
                if prev_value > 0:
                    daily_return = (portfolio_value - prev_value) / prev_value
                else:
                    daily_return = 0
                portfolio_returns.append(daily_return)
            else:
                portfolio_returns.append(0)

        # åœ¨å›æµ‹ç»“æŸæ—¶å–å‡ºæ‰€æœ‰æŒä»“
        if holdings and unique_dates:
            last_date = unique_dates[-1]
            for stock_code, holding in list(holdings.items()):
                if stock_code in price_dict and last_date in price_dict[stock_code]:
                    sell_price = price_dict[stock_code][last_date]
                    shares = holding['shares']
                    buy_price = holding['buy_price']
                    hold_days = (last_date - holding['buy_date']).days

                    trade_value = shares * sell_price
                    cost = calculate_transaction_costs(trade_value, is_buy=False)
                    profit = trade_value - (shares * buy_price) - cost

                    cash += trade_value - cost

                    trading_records.append({
                        'date': last_date,
                        'type': 'sell',
                        'stock_code': stock_code,
                        'shares': shares,
                        'price': sell_price,
                        'cost': cost,
                        'total_value': trade_value,
                        'profit': profit,
                        'hold_days': hold_days,
                        'reason': 'end_of_backtest'
                    })

        # è®¡ç®—å›æµ‹æŒ‡æ ‡
        metrics = calculate_backtest_metrics(portfolio_values, portfolio_returns)

        # ç”Ÿæˆäº¤æ˜“ç»Ÿè®¡
        trading_stats = generate_trading_statistics(trading_records, portfolio_values)

        print(f"å›æµ‹å®Œæˆ!")
        print(f"æœ€ç»ˆç»„åˆä»·å€¼: {portfolio_value:,.2f}")
        print(f"æ€»æ”¶ç›Šç‡: {metrics.get('æ€»æ”¶ç›Šç‡', 0):.2%}")
        print(f"æœ€å¤§å›æ’¤: {metrics.get('æœ€å¤§å›æ’¤', 0):.2%}")
        print(f"å¹³å‡æŒä»“å¤©æ•°: {trading_stats.get('å¹³å‡æŒä»“å¤©æ•°', 0):.1f}å¤©")
        print(f"å¹´åŒ–æ¢æ‰‹ç‡: {trading_stats.get('å¹´åŒ–æ¢æ‰‹ç‡', 0):.2%}")

        return {
            'portfolio_values': portfolio_values,
            'portfolio_returns': portfolio_returns,
            'metrics': metrics,
            'trading_stats': trading_stats,
            'trading_records': trading_records,
            'positions': holdings,
            'initial_capital': initial_capital,
            'final_value': portfolio_value
        }

    except Exception as e:
        print(f"æŒ‰æœˆè°ƒä»“å›æµ‹æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


def calculate_backtest_metrics(portfolio_values, portfolio_returns, benchmark_returns=None):
    """è®¡ç®—å›æµ‹æ ¸å¿ƒæŒ‡æ ‡ - ä¿®å¤ç‰ˆï¼šå¢åŠ åŒºé—´æ—¥æœŸå’Œå¹´åŒ–è®¡ç®—"""

    if not portfolio_values:
        return {}

    # å‡†å¤‡æ•°æ®
    returns_series = pd.Series(portfolio_returns)

    # ==================== æ–°å¢ï¼šè·å–æ”¶ç›ŠåŒºé—´èµ·å§‹å’Œç»“æŸæ—¥æœŸ ====================
    if len(portfolio_values) > 0:
        start_date = portfolio_values[0]['date']
        end_date = portfolio_values[-1]['date']
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ä¾¿äºæ˜¾ç¤º
        start_date_str = start_date.strftime('%Y-%m-%d')
        end_date_str = end_date.strftime('%Y-%m-%d')
        days = (end_date - start_date).days
    else:
        start_date_str = "N/A"
        end_date_str = "N/A"
        days = 0

    # æ€»æ”¶ç›Šç‡
    if len(portfolio_values) > 0:
        initial_value = portfolio_values[0]['portfolio_value']
        final_value = portfolio_values[-1]['portfolio_value']
        if initial_value > 0:
            total_return = (final_value - initial_value) / initial_value
        else:
            total_return = 0
    else:
        total_return = 0

    # ==================== å¹´åŒ–æ”¶ç›Šç‡è®¡ç®— ====================
    annualized_return = 0
    if days > 0:
        # ä½¿ç”¨å¤åˆ©å…¬å¼è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡
        annualized_return = (1 + total_return) ** (365.25 / days) - 1

    # ==================== å¹´åŒ–æ³¢åŠ¨ç‡è®¡ç®— ====================
    if len(returns_series) > 1:
        annualized_volatility = returns_series.std() * np.sqrt(252)
    else:
        annualized_volatility = 0

    # ==================== å¹´åŒ–å¤æ™®æ¯”ç‡è®¡ç®— ====================
    sharpe_ratio = annualized_return / annualized_volatility if annualized_volatility != 0 else 0

    # ==================== æœ€å¤§å›æ’¤è®¡ç®— ====================
    if len(portfolio_values) > 0:
        values_series = pd.Series([pv['portfolio_value'] for pv in portfolio_values])
        running_max = values_series.expanding().max()
        drawdown = (values_series - running_max) / running_max
        max_drawdown = drawdown.min()
    else:
        max_drawdown = 0

    # ==================== å¡ç›æ¯”ç‡ï¼ˆCalmar Ratioï¼‰ ====================
    calmar_ratio = -annualized_return / max_drawdown if max_drawdown != 0 else 0

    # ==================== å…¶ä»–æŒ‡æ ‡è®¡ç®— ====================
    # èƒœç‡ï¼ˆæ­£æ”¶ç›Šå¤©æ•°æ¯”ä¾‹ï¼‰
    win_rate = (returns_series > 0).mean() if len(returns_series) > 0 else 0

    # ç›ˆäºæ¯”
    avg_win = returns_series[returns_series > 0].mean() if (returns_series > 0).any() else 0
    avg_loss = returns_series[returns_series < 0].mean() if (returns_series < 0).any() else 0
    profit_loss_ratio = abs(avg_win / avg_loss) if avg_loss != 0 else 0

    # ==================== ä¿¡æ¯æ¯”ç‡ï¼ˆå¦‚æœæœ‰åŸºå‡†ï¼‰ ====================
    information_ratio = 0
    if benchmark_returns is not None and len(benchmark_returns) == len(returns_series):
        excess_returns = returns_series - benchmark_returns
        tracking_error = excess_returns.std() * np.sqrt(252)
        information_ratio = (
                                    annualized_return - benchmark_returns.mean() * 252
                            ) / tracking_error if tracking_error != 0 else 0

    # ==================== æ•´ç†æŒ‡æ ‡ ====================
    metrics = {
        # åŒºé—´ä¿¡æ¯
        'èµ·å§‹æ—¥æœŸ': start_date_str,
        'ç»“æŸæ—¥æœŸ': end_date_str,
        'å›æµ‹å¤©æ•°': days,
        'äº¤æ˜“æ—¥æ•°': len(portfolio_values),

        # æ”¶ç›Šç‡æŒ‡æ ‡
        'æ€»æ”¶ç›Šç‡': total_return,
        'å¹´åŒ–æ”¶ç›Šç‡': annualized_return,

        # é£é™©æŒ‡æ ‡
        'å¹´åŒ–æ³¢åŠ¨ç‡': annualized_volatility,
        'æœ€å¤§å›æ’¤': max_drawdown,

        # é£é™©è°ƒæ•´æ”¶ç›ŠæŒ‡æ ‡
        'å¹´åŒ–å¤æ™®æ¯”ç‡': sharpe_ratio,
        'å¡ç›æ¯”ç‡': calmar_ratio,
        'ä¿¡æ¯æ¯”ç‡': information_ratio,

        # äº¤æ˜“ç»Ÿè®¡æŒ‡æ ‡
        'èƒœç‡': win_rate,
        'ç›ˆäºæ¯”': profit_loss_ratio,

        # åŸå§‹æ•°æ®
        'åˆå§‹å‡€å€¼': initial_value if len(portfolio_values) > 0 else 0,
        'æœ€ç»ˆå‡€å€¼': final_value if len(portfolio_values) > 0 else 0
    }

    return metrics


def print_backtest_metrics(metrics):
    """æ‰“å°å›æµ‹æŒ‡æ ‡ï¼ˆåŒ…å«åŒºé—´æ—¥æœŸï¼‰"""
    print_section("å›æµ‹ç»“æœæ±‡æ€»")

    # æ‰“å°åŒºé—´ä¿¡æ¯
    if 'èµ·å§‹æ—¥æœŸ' in metrics and 'ç»“æŸæ—¥æœŸ' in metrics:
        print(f"ğŸ“… å›æµ‹åŒºé—´: {metrics['èµ·å§‹æ—¥æœŸ']} è‡³ {metrics['ç»“æŸæ—¥æœŸ']}")
        print(f"   å›æµ‹å¤©æ•°: {metrics.get('å›æµ‹å¤©æ•°', 0)} å¤©")
        print(f"   äº¤æ˜“æ—¥æ•°: {metrics.get('äº¤æ˜“æ—¥æ•°', 0)} å¤©")

    # æ‰“å°æ”¶ç›Šç‡æŒ‡æ ‡
    print(f"\nğŸ“ˆ æ”¶ç›Šç‡æŒ‡æ ‡:")
    if 'æ€»æ”¶ç›Šç‡' in metrics:
        print(f"   æ€»æ”¶ç›Šç‡: {metrics['æ€»æ”¶ç›Šç‡']:.2%}")
    if 'å¹´åŒ–æ”¶ç›Šç‡' in metrics:
        print(f"   å¹´åŒ–æ”¶ç›Šç‡: {metrics['å¹´åŒ–æ”¶ç›Šç‡']:.2%}")

    # æ‰“å°é£é™©æŒ‡æ ‡
    print(f"\nâš ï¸  é£é™©æŒ‡æ ‡:")
    if 'å¹´åŒ–æ³¢åŠ¨ç‡' in metrics:
        print(f"   å¹´åŒ–æ³¢åŠ¨ç‡: {metrics['å¹´åŒ–æ³¢åŠ¨ç‡']:.2%}")
    if 'æœ€å¤§å›æ’¤' in metrics:
        print(f"   æœ€å¤§å›æ’¤: {metrics['æœ€å¤§å›æ’¤']:.2%}")

    # æ‰“å°é£é™©è°ƒæ•´æ”¶ç›ŠæŒ‡æ ‡
    print(f"\nâš–ï¸  é£é™©è°ƒæ•´æ”¶ç›ŠæŒ‡æ ‡:")
    if 'å¹´åŒ–å¤æ™®æ¯”ç‡' in metrics:
        print(f"   å¹´åŒ–å¤æ™®æ¯”ç‡: {metrics['å¹´åŒ–å¤æ™®æ¯”ç‡']:.2f}")
    if 'å¡ç›æ¯”ç‡' in metrics:
        print(f"   å¡ç›æ¯”ç‡: {metrics['å¡ç›æ¯”ç‡']:.2f}")
    if 'ä¿¡æ¯æ¯”ç‡' in metrics and metrics['ä¿¡æ¯æ¯”ç‡'] != 0:
        print(f"   ä¿¡æ¯æ¯”ç‡: {metrics['ä¿¡æ¯æ¯”ç‡']:.2f}")

    # æ‰“å°äº¤æ˜“ç»Ÿè®¡æŒ‡æ ‡
    print(f"\nğŸ’¹ äº¤æ˜“ç»Ÿè®¡æŒ‡æ ‡:")
    if 'èƒœç‡' in metrics:
        print(f"   èƒœç‡: {metrics['èƒœç‡']:.2%}")
    if 'ç›ˆäºæ¯”' in metrics:
        print(f"   ç›ˆäºæ¯”: {metrics['ç›ˆäºæ¯”']:.2f}")

    # æ‰“å°å‡€å€¼ä¿¡æ¯
    print(f"\nğŸ’° å‡€å€¼ä¿¡æ¯:")
    if 'åˆå§‹å‡€å€¼' in metrics and metrics['åˆå§‹å‡€å€¼'] > 0:
        print(f"   åˆå§‹å‡€å€¼: {metrics['åˆå§‹å‡€å€¼']:,.2f}")
    if 'æœ€ç»ˆå‡€å€¼' in metrics and metrics['æœ€ç»ˆå‡€å€¼'] > 0:
        print(f"   æœ€ç»ˆå‡€å€¼: {metrics['æœ€ç»ˆå‡€å€¼']:,.2f}")

# ==================== å…¶ä»–åŸæœ‰å‡½æ•° ====================
def check_saved_files():
    """æ£€æŸ¥å·²ä¿å­˜çš„æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    saved_files = {
        'lgb_feature_importance': os.path.exists(LGB_FEATURE_IMPORTANCE_FILE),
        'core_factors': os.path.exists(CORE_FACTORS_FILE),
        'factor_ic_metrics': os.path.exists(FACTOR_IC_METRICS_FILE),
        'financial_ic': os.path.exists(FINANCIAL_IC_FILE),
        'technical_ic': os.path.exists(TECHNICAL_IC_FILE)
    }
    return saved_files


def load_saved_data():
    """åŠ è½½å·²ä¿å­˜çš„æ•°æ®æ–‡ä»¶"""
    saved_data = {}

    if os.path.exists(LGB_FEATURE_IMPORTANCE_FILE):
        saved_data['lgb_feature_importance'] = pd.read_csv(LGB_FEATURE_IMPORTANCE_FILE)
        print(f"âœ… å·²åŠ è½½LightGBMç‰¹å¾é‡è¦æ€§: {LGB_FEATURE_IMPORTANCE_FILE}")

    if os.path.exists(CORE_FACTORS_FILE):
        saved_data['core_factors'] = pd.read_csv(CORE_FACTORS_FILE)
        print(f"âœ… å·²åŠ è½½æ ¸å¿ƒå› å­åˆ—è¡¨: {CORE_FACTORS_FILE}")

    if os.path.exists(FACTOR_IC_METRICS_FILE):
        saved_data['factor_ic_metrics'] = pd.read_csv(FACTOR_IC_METRICS_FILE)
        print(f"âœ… å·²åŠ è½½å› å­ICæŒ‡æ ‡: {FACTOR_IC_METRICS_FILE}")

    if os.path.exists(FINANCIAL_IC_FILE):
        saved_data['financial_ic'] = pd.read_csv(FINANCIAL_IC_FILE)
        print(f"âœ… å·²åŠ è½½è´¢åŠ¡å› å­ICæŒ‡æ ‡: {FINANCIAL_IC_FILE}")

    if os.path.exists(TECHNICAL_IC_FILE):
        saved_data['technical_ic'] = pd.read_csv(TECHNICAL_IC_FILE)
        print(f"âœ… å·²åŠ è½½æŠ€æœ¯å› å­ICæŒ‡æ ‡: {TECHNICAL_IC_FILE}")

    return saved_data


def save_factor_data(feature_importance_lgb, core_factors, ic_df=None,
                     financial_ic_df=None, technical_ic_df=None):
    """ä¿å­˜å› å­ç›¸å…³æ•°æ®ï¼ˆå»æ‰æ—¶é—´åç¼€ï¼‰"""

    # ä¿å­˜LightGBMç‰¹å¾é‡è¦æ€§
    if feature_importance_lgb is not None and not feature_importance_lgb.empty:
        feature_importance_lgb.to_csv(LGB_FEATURE_IMPORTANCE_FILE, index=False)
        print(f"âœ… LightGBMç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {LGB_FEATURE_IMPORTANCE_FILE}")

    # ä¿å­˜æ ¸å¿ƒå› å­åˆ—è¡¨
    if core_factors is not None:
        core_factors_df = pd.DataFrame({
            'core_factors': core_factors,
            'factor_type': ['è´¢åŠ¡å› å­' if col.startswith('fin_') else 'æŠ€æœ¯å› å­' for col in core_factors]
        })
        core_factors_df.to_csv(CORE_FACTORS_FILE, index=False)
        print(f"âœ… æ ¸å¿ƒå› å­åˆ—è¡¨å·²ä¿å­˜: {CORE_FACTORS_FILE}")

    # ä¿å­˜å› å­ICæŒ‡æ ‡
    if ic_df is not None and not ic_df.empty:
        ic_df.to_csv(FACTOR_IC_METRICS_FILE, index=False)
        print(f"âœ… å› å­ICæŒ‡æ ‡å·²ä¿å­˜: {FACTOR_IC_METRICS_FILE}")

    # ä¿å­˜è´¢åŠ¡å› å­ICæŒ‡æ ‡
    if financial_ic_df is not None and not financial_ic_df.empty:
        financial_ic_df.to_csv(FINANCIAL_IC_FILE, index=False)
        print(f"âœ… è´¢åŠ¡å› å­ICæŒ‡æ ‡å·²ä¿å­˜: {FINANCIAL_IC_FILE}")

    # ä¿å­˜æŠ€æœ¯å› å­ICæŒ‡æ ‡
    if technical_ic_df is not None and not technical_ic_df.empty:
        technical_ic_df.to_csv(TECHNICAL_IC_FILE, index=False)
        print(f"âœ… æŠ€æœ¯å› å­ICæŒ‡æ ‡å·²ä¿å­˜: {TECHNICAL_IC_FILE}")


# ==================== è¾…åŠ©å‡½æ•° ====================

def validate_price_data(df):
    """
    éªŒè¯ä»·æ ¼æ•°æ®çš„åŸºæœ¬è´¨é‡ã€‚
    åœ¨è¿›è¡ŒæŠ€æœ¯æŒ‡æ ‡å’Œæ”¶ç›Šç‡è®¡ç®—å‰ï¼Œç¡®ä¿æ•°æ®æ˜¯æœ‰æ•ˆçš„ã€‚
    """
    if df.empty:
        print("âŒ é”™è¯¯ï¼šä»·æ ¼æ•°æ®ä¸ºç©ºã€‚")
        return False

    # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = ['close', 'stock_code', 'date']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"âŒ é”™è¯¯ï¼šä»·æ ¼æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")
        return False

    # æ£€æŸ¥ 'close' åˆ—æ˜¯å¦æœ‰è¶³å¤Ÿçš„éç©ºå€¼
    non_null_close_count = df['close'].notna().sum()
    if non_null_close_count < 100:  # å‡è®¾è‡³å°‘éœ€è¦100ä¸ªæœ‰æ•ˆæ”¶ç›˜ä»·
        print(f"âŒ é”™è¯¯ï¼š'close' åˆ—çš„æœ‰æ•ˆæ•°æ®ç‚¹å¤ªå°‘ ({non_null_close_count} ä¸ª)ã€‚")
        return False

    # æ£€æŸ¥ä»·æ ¼æ˜¯å¦ä¸ºæ­£æ•°
    invalid_price_count = (df['close'] <= 0).sum()
    if invalid_price_count > 0:
        print(f"âš ï¸ è­¦å‘Šï¼šå‘ç° {invalid_price_count} ä¸ªéæ­£ä»·æ ¼ã€‚è¿™äº›è¡Œå°†åœ¨åç»­æ­¥éª¤ä¸­è¢«ç§»é™¤ã€‚")
        # è¿™é‡Œä¸ç›´æ¥è¿”å›Falseï¼Œå› ä¸ºåç»­æ­¥éª¤å¯ä»¥å¤„ç†ï¼Œä½†å‘å‡ºè­¦å‘Š

    print("âœ… ä»·æ ¼æ•°æ®éªŒè¯é€šè¿‡ã€‚")
    return True


def get_conservative_params():
    """è¿”å›ä¿å®ˆçš„æ¨¡å‹å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ"""
    return {
        'rf': {
            'n_estimators': 50,  # æ ‘æ•°é‡
            'max_depth': 6,  # æ·±åº¦
            'min_samples_split': 20,  # åˆ†è£‚æ ·æœ¬æ•°
            'min_samples_leaf': 10,  # å¶èŠ‚ç‚¹æ ·æœ¬ï¼‰
            'max_features': 0.3,  # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
            'class_weight': 'balanced',
            'random_state': RANDOM_STATE,
            'n_jobs': N_JOBS
        },
        'xgb': {
            'n_estimators': 50,  # æ ‘æ•°é‡
            'max_depth': 3,  # æ·±åº¦
            'learning_rate': 0.01,  # å­¦ä¹ ç‡
            'subsample': 0.6,  # é‡‡æ ·æ¯”ä¾‹
            'colsample_bytree': 0.6,  # ç‰¹å¾é‡‡æ ·
            'reg_alpha': 1.0,  # L1æ­£åˆ™ ç‰¹å¾
            'reg_lambda': 1.0,  # L2æ­£åˆ™ æƒé‡
            'scale_pos_weight': 1,  # æ‰‹åŠ¨æ§åˆ¶ç±»åˆ«æƒé‡
            'random_state': RANDOM_STATE,
            'n_jobs': 1,
            'use_label_encoder': False,
            'eval_metric': 'logloss'
        }
    }


def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f" {title}")
    print("=" * 60)


def reduce_memory_usage(df, verbose=True):
    """å‡å°‘æ•°æ®å†…å­˜ä½¿ç”¨ - ä¿®å¤äº†datetime64[ns, UTC+08:00]ç±»å‹çš„é—®é¢˜"""
    start_mem = df.memory_usage().sum() / 1024 ** 2

    for col in df.columns:
        col_type = str(df[col].dtype)

        # è·³è¿‡æ—¥æœŸåˆ—å’Œéæ•°å€¼åˆ—
        if 'datetime' in col_type or col_type in ['object', 'category', 'bool', 'string']:
            continue

        if np.issubdtype(df[col].dtype, np.number):
            c_min = df[col].min()
            c_max = df[col].max()

            if np.issubdtype(df[col].dtype, np.integer):
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)

    end_mem = df.memory_usage().sum() / 1024 ** 2
    if verbose:
        print(f"å†…å­˜ä½¿ç”¨å‡å°‘: {100 * (start_mem - end_mem) / start_mem:.1f}%")
        print(f"ä» {start_mem:.2f} MB åˆ° {end_mem:.2f} MB")

    return df

# ==================== å…¨å±€å®šä¹‰é›†æˆæ¨¡å‹ç±» ====================
class EnsembleRF:
    """éšæœºæ£®æ—é›†æˆæ¨¡å‹ï¼ˆå¯åºåˆ—åŒ–ç‰ˆæœ¬ï¼‰"""
    def __init__(self, models):
        self.models = models if models else []

    def predict(self, X):
        if not self.models:
            return np.zeros(len(X), dtype=np.int32)

        preds = []
        for i, model in enumerate(self.models):
            try:
                pred = model.predict(X)
                # ä¿®å¤ï¼šæ£€æŸ¥å¹¶æ›¿æ¢ç‰¹æ®Šå€¼
                if hasattr(pred, '__len__'):
                    # æ›¿æ¢-2147483648ä¸º0
                    pred = np.where(pred == -2147483648, 0, pred)
                preds.append(pred)
            except Exception:
                # å¦‚æœæ¨¡å‹é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨å…¨0é¢„æµ‹
                preds.append(np.zeros(len(X), dtype=np.int32))

        if not preds:
            return np.zeros(len(X), dtype=np.int32)

        try:
            preds_array = np.array(preds)
            avg_pred = np.mean(preds_array, axis=0)
            return np.round(avg_pred).astype(np.int32)
        except Exception:
            return np.zeros(len(X), dtype=np.int32)

    def predict_proba(self, X):
        if not self.models:
            return np.column_stack([np.ones(len(X)), np.zeros(len(X))]) * 0.5

        probas_list = []
        for model in self.models:
            try:
                proba = model.predict_proba(X)
                probas_list.append(proba)
            except Exception:
                probas_list.append(np.column_stack([np.zeros(len(X)), np.ones(len(X))]) * 0.5)

        if not probas_list:
            return np.column_stack([np.zeros(len(X)), np.ones(len(X))]) * 0.5

        try:
            probas = np.array(probas_list)
            return np.mean(probas, axis=0)
        except Exception:
            return probas_list[0] if probas_list else np.column_stack(
                [np.zeros(len(X)), np.ones(len(X))]) * 0.5


class EnsembleXGB:
    """XGBoosté›†æˆæ¨¡å‹ï¼ˆå¯åºåˆ—åŒ–ç‰ˆæœ¬ï¼‰"""
    def __init__(self, models):
        self.models = models

    def predict(self, X):
        if not self.models:
            return np.zeros(len(X), dtype=np.int32)

        preds = []
        for i, model in enumerate(self.models):
            try:
                pred = model.predict(X)
                # ä¿®å¤ï¼šæ£€æŸ¥å¹¶æ›¿æ¢ç‰¹æ®Šå€¼
                if hasattr(pred, '__len__'):
                    # æ›¿æ¢-2147483648ä¸º0
                    pred = np.where(pred == -2147483648, 0, pred)
                preds.append(pred)
            except Exception:
                # å¦‚æœæ¨¡å‹é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨å…¨0é¢„æµ‹
                preds.append(np.zeros(len(X), dtype=np.int32))

        if not preds:
            return np.zeros(len(X), dtype=np.int32)

        try:
            preds_array = np.array(preds)
            avg_pred = np.mean(preds_array, axis=0)
            return np.round(avg_pred).astype(np.int32)
        except Exception:
            return np.zeros(len(X), dtype=np.int32)

    def predict_proba(self, X):
        if not self.models:
            return np.column_stack([np.ones(len(X)), np.zeros(len(X))]) * 0.5

        probas_list = []
        for model in self.models:
            try:
                proba = model.predict_proba(X)
                probas_list.append(proba)
            except Exception:
                probas_list.append(np.column_stack([np.zeros(len(X)), np.ones(len(X))]) * 0.5)

        if not probas_list:
            return np.column_stack([np.zeros(len(X)), np.ones(len(X))]) * 0.5

        try:
            probas = np.array(probas_list)
            return np.mean(probas, axis=0)
        except Exception:
            return probas_list[0] if probas_list else np.column_stack(
                [np.zeros(len(X)), np.ones(len(X))]) * 0.5

    def __getattr__(self, name):
        # è½¬å‘åˆ°ç¬¬ä¸€ä¸ªæ¨¡å‹çš„å…¶ä»–å±æ€§
        if self.models:
            return getattr(self.models[0], name)
        raise AttributeError(f"'EnsembleXGB' object has no attribute '{name}'")


class EnsembleLGB:
    """LightGBMé›†æˆæ¨¡å‹ï¼ˆå¯åºåˆ—åŒ–ç‰ˆæœ¬ï¼‰"""
    def __init__(self, models, fold_scores=None):
        self.models = models
        self.fold_scores = fold_scores if fold_scores is not None else []
        self.n_models = len(models)

    def predict(self, X):
        # æ£€æŸ¥æ¨¡å‹æ•°é‡
        if self.n_models == 0:
            print("è­¦å‘Šï¼šæ²¡æœ‰æ¨¡å‹å¯ç”¨ï¼Œè¿”å›å…¨0é¢„æµ‹")
            return np.zeros(len(X), dtype=np.int32)

        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        all_predictions = []
        for i, model in enumerate(self.models):
            try:
                pred = model.predict(X)
                # ä¿®å¤ï¼šæ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦ä¸ºæœ‰æ•ˆæ•´æ•°
                if hasattr(pred, '__len__'):
                    # ç¡®ä¿é¢„æµ‹å€¼æ˜¯æ•´æ•°ç±»å‹ï¼Œé¿å…ç‰¹æ®Šå€¼
                    pred = pred.astype(np.int32)
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
                    mask_invalid = (pred != 0) & (pred != 1)
                    if mask_invalid.any():
                        print(f"æ¨¡å‹{i + 1}é¢„æµ‹åŒ…å«å¼‚å¸¸å€¼ï¼Œè½¬æ¢ä¸º0")
                        pred[mask_invalid] = 0
                else:
                    # å¦‚æœæ˜¯å•ä¸ªå€¼ï¼Œæ£€æŸ¥æ˜¯å¦ä¸º0æˆ–1
                    if pred not in [0, 1]:
                        pred = 0
                    pred = np.array([pred], dtype=np.int32)
                all_predictions.append(pred)
            except Exception as e:
                print(f"æ¨¡å‹{i + 1}é¢„æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨å…¨0é¢„æµ‹")
                all_predictions.append(np.zeros(len(X), dtype=np.int32))

        # è®¡ç®—å¹³å‡é¢„æµ‹
        if all_predictions:
            try:
                preds_array = np.array(all_predictions)
                avg_pred = np.mean(preds_array, axis=0)
                final_pred = np.round(avg_pred).astype(np.int32)
                # å†æ¬¡æ£€æŸ¥æœ€ç»ˆé¢„æµ‹å€¼
                mask_invalid = (final_pred != 0) & (final_pred != 1)
                if mask_invalid.any():
                    print(f"æœ€ç»ˆé¢„æµ‹åŒ…å«å¼‚å¸¸å€¼{np.unique(final_pred[mask_invalid])}ï¼Œä¿®æ­£ä¸º0")
                    final_pred[mask_invalid] = 0
                return final_pred
            except Exception as e:
                print(f"é¢„æµ‹èšåˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„é¢„æµ‹")
                return all_predictions[0] if all_predictions else np.zeros(len(X), dtype=np.int32)
        else:
            return np.zeros(len(X), dtype=np.int32)

    def predict_proba(self, X):
        if self.n_models == 0:
            # ä¿®å¤ï¼šé»˜è®¤æ¦‚ç‡ç»´åº¦æ›´å¥å£®ï¼Œé¿å…åˆ—æ‹¼æ¥é”™è¯¯
            return np.hstack([np.ones((len(X), 1)) * 0.5, np.ones((len(X), 1)) * 0.5])

        probas_list = []
        for i, model in enumerate(self.models):
            try:
                proba = model.predict_proba(X)
                # æ£€æŸ¥æ¦‚ç‡ç»´åº¦
                if proba.ndim == 2 and proba.shape[1] >= 2:
                    probas_list.append(proba)
                else:
                    # å¦‚æœæ¨¡å‹è¿”å›çš„ç»´åº¦ä¸å¯¹ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    probas_list.append(np.hstack([np.ones((len(X), 1)) * 0.5, np.ones((len(X), 1)) * 0.5]))
            except Exception:
                probas_list.append(np.hstack([np.ones((len(X), 1)) * 0.5, np.ones((len(X), 1)) * 0.5]))

        if not probas_list:
            return np.hstack([np.ones((len(X), 1)) * 0.5, np.ones((len(X), 1)) * 0.5])

        try:
            probas = np.array(probas_list)
            return np.mean(probas, axis=0)
        except Exception:
            return probas_list[0] if probas_list else np.hstack([np.ones((len(X), 1)) * 0.5, np.ones((len(X), 1)) * 0.5])

# ==================== æ•°æ®åŠ è½½å’Œå¤„ç† ====================
@timer_decorator
def load_and_preprocess_data():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ® - ç¡®ä¿æ­£ç¡®è°ƒç”¨ä¿®å¤åçš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—"""
    print_section("æ•°æ®åŠ è½½å’Œé¢„å¤„ç†")

    # ==================== 1. é¢„åˆå¹¶æ–‡ä»¶æ£€æŸ¥ ====================

    if not FORCE_REMERGE and os.path.exists(PRE_MERGED_FILE):
        print(f"åŠ è½½é¢„åˆå¹¶æ–‡ä»¶: {PRE_MERGED_FILE}")
        try:
            with open(PRE_MERGED_FILE, 'rb') as f:
                data = pickle.load(f)

            # é€‚é…ä¸¤ç§æ•°æ®æ ¼å¼
            if isinstance(data, pd.DataFrame):
                # æ ¼å¼1: åªæœ‰DataFrame
                df = data
                # è‡ªåŠ¨æå–ç‰¹å¾åˆ—
                base_cols = ['date', 'stock_code', 'close', 'volume', 'open', 'high', 'low',
                             'future_return', 'market_avg_return', 'label']
                feature_cols = [col for col in df.columns
                                if col not in base_cols and pd.api.types.is_numeric_dtype(df[col])]
                print("æ£€æµ‹åˆ°DataFrameæ ¼å¼ï¼Œè‡ªåŠ¨æå–ç‰¹å¾åˆ—")
                print(f"é¢„åˆå¹¶æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
                print(f"æ—¶é—´èŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
                print(f"è‚¡ç¥¨æ•°é‡: {df['stock_code'].nunique()}")
                return df, feature_cols
            elif isinstance(data, tuple) and len(data) == 2:
                # æ ¼å¼2: (df, feature_cols)
                df, feature_cols = data
                print("æ£€æµ‹åˆ°å…ƒç»„æ ¼å¼: (DataFrame, feature_cols)")
                print(f"é¢„åˆå¹¶æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
                print(f"æ—¶é—´èŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
                print(f"è‚¡ç¥¨æ•°é‡: {df['stock_code'].nunique()}")
                return df, feature_cols
            else:
                print(f"æœªçŸ¥æ•°æ®æ ¼å¼: {type(data)}")
                # ç»§ç»­æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹
        except Exception as e:
            print(f"é¢„åˆå¹¶æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œé‡æ–°å¤„ç†...")

    # ==================== 2. å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ ====================
    print("æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")

    try:
        # 1. åŠ è½½è‚¡ä»·æ•°æ®
        print(f"åŠ è½½è‚¡ä»·æ•°æ®: {PRICE_DATA_PATH}")
        if PRICE_DATA_PATH.endswith('.csv'):
            price_df = pd.read_csv(PRICE_DATA_PATH, encoding='utf-8')
        else:
            price_df = pd.read_excel(PRICE_DATA_PATH)

        print(f"è‚¡ä»·æ•°æ®åŠ è½½æˆåŠŸ: {price_df.shape}")
        print(f"åˆ—å: {list(price_df.columns)}")

        # æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®
        print("\nè‚¡ä»·æ•°æ®æ ·ä¾‹ï¼ˆå‰3è¡Œï¼‰:")
        print(price_df.head(3))

    except Exception as e:
        print(f"è‚¡ä»·æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

    # 2. æ ‡å‡†åŒ–åˆ—å
    print("æ ‡å‡†åŒ–åˆ—å...")
    column_mapping = {
        'stock_id': 'stock_code', 'stock_code': 'stock_code', 'symbol': 'stock_code', 'number': 'stock_code',
        'date': 'date', 'Date': 'date', 'äº¤æ˜“æ—¥': 'date',
        'close': 'close', 'Close': 'close', 'æ”¶ç›˜ä»·': 'close',
        'open': 'open', 'Open': 'open', 'å¼€ç›˜ä»·': 'open',
        'high': 'high', 'High': 'high', 'æœ€é«˜ä»·': 'high',
        'low': 'low', 'Low': 'low', 'æœ€ä½ä»·': 'low',
        'max': 'high', 'min': 'low',
        'volume': 'volume', 'Volume': 'volume', 'æˆäº¤é‡': 'volume', 'trading_volume': 'volume',
        'trading_money': 'amount', 'æˆäº¤é‡‘é¢': 'amount',
        'spread': 'change', 'change': 'change', 'æ¶¨è·Œ': 'change',
        'turnover_rate': 'turnover_rate', 'trading_turnover': 'turnover_rate',
    }

    # åº”ç”¨åˆ—åæ˜ å°„
    for old_col, new_col in column_mapping.items():
        if old_col in price_df.columns and new_col not in price_df.columns:
            price_df = price_df.rename(columns={old_col: new_col})
            print(f"   é‡å‘½å: {old_col} -> {new_col}")

    # æ£€æŸ¥å¿…è¦çš„åˆ—
    required_cols = ['stock_code', 'date', 'close']
    missing_cols = [col for col in required_cols if col not in price_df.columns]
    if missing_cols:
        print(f"é”™è¯¯: ç¼ºå°‘å¿…è¦åˆ— {missing_cols}")
        print(f"å¯ç”¨åˆ—: {list(price_df.columns)}")
        return None

    # 3. æ•°æ®æ¸…æ´—
    print("æ•°æ®æ¸…æ´—...")

    # è½¬æ¢æ•°æ®ç±»å‹
    price_df['stock_code'] = price_df['stock_code'].astype(str).str.strip()

    # ä¿®å¤æ—¥æœŸè½¬æ¢é—®é¢˜ - ç»Ÿä¸€ä¸ºæ— æ—¶åŒºçš„datetime
    try:
        # å°è¯•ä¸åŒçš„æ—¥æœŸæ ¼å¼
        price_df['date'] = pd.to_datetime(price_df['date'], errors='coerce')
        # ç§»é™¤æ—¶åŒºä¿¡æ¯
        if hasattr(price_df['date'].dtype, 'tz') and price_df['date'].dtype.tz is not None:
            price_df['date'] = price_df['date'].dt.tz_convert(None)
    except Exception as e:
        print(f"æ—¥æœŸè½¬æ¢å¤±è´¥: {e}")
        return None

    # ç§»é™¤æ— æ•ˆæ—¥æœŸ
    initial_rows = len(price_df)
    price_df = price_df.dropna(subset=['date'])
    print(f"ç§»é™¤æ— æ•ˆæ—¥æœŸ: {initial_rows - len(price_df):,} è¡Œ")

    # æŒ‰è‚¡ç¥¨å’Œæ—¥æœŸæ’åº
    price_df = price_df.sort_values(['stock_code', 'date'])

    # ç§»é™¤é‡å¤è¡Œ
    initial_rows = len(price_df)
    price_df = price_df.drop_duplicates(subset=['stock_code', 'date'])
    print(f"ç§»é™¤é‡å¤è¡Œ: {initial_rows - len(price_df):,} è¡Œ")

    # å¤„ç†æ•°å€¼åˆ—
    numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount', 'change', 'turnover_rate']
    numeric_cols = [col for col in numeric_cols if col in price_df.columns]

    for col in numeric_cols:
        price_df[col] = pd.to_numeric(price_df[col], errors='coerce')

    # æŒ‰è‚¡ç¥¨åˆ†ç»„å¡«å……ç¼ºå¤±å€¼
    print("æŒ‰è‚¡ç¥¨å¡«å……ç¼ºå¤±å€¼...")
    for stock_code in tqdm(price_df['stock_code'].unique(), desc="å¡«å……ç¼ºå¤±å€¼"):
        stock_mask = price_df['stock_code'] == stock_code
        for col in numeric_cols:
            if col in price_df.columns:
                # å‰å‘å¡«å……ç„¶ååå‘å¡«å……
                price_df.loc[stock_mask, col] = price_df.loc[stock_mask, col].ffill().bfill()

    # ç§»é™¤ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ
    initial_size = len(price_df)
    price_df = price_df.dropna(subset=numeric_cols)
    print(f"ç§»é™¤ç¼ºå¤±å€¼è¡Œ: {initial_size - len(price_df):,} è¡Œ")

    # å‡å°‘å†…å­˜ä½¿ç”¨
    price_df = reduce_memory_usage(price_df)

    print(f"è‚¡ä»·æ•°æ®å¤„ç†å®Œæˆ!")
    print(f"å¤„ç†åçš„æ•°æ®å½¢çŠ¶: {price_df.shape}")
    print(f"æ—¶é—´èŒƒå›´: {price_df['date'].min()} åˆ° {price_df['date'].max()}")
    print(f"è‚¡ç¥¨æ•°é‡: {price_df['stock_code'].nunique()}")

    # 4. åŠ è½½è´¢æŠ¥æ•°æ®
    financial_df = None
    if os.path.exists(REPORTS_DATA_PATH):
        print(f"\nåŠ è½½è´¢æŠ¥æ•°æ®: {REPORTS_DATA_PATH}")
        try:
            if REPORTS_DATA_PATH.endswith('.csv'):
                financial_df = pd.read_csv(REPORTS_DATA_PATH, encoding='utf-8')
            else:
                financial_df = pd.read_excel(REPORTS_DATA_PATH)

            print(f"è´¢æŠ¥æ•°æ®åŠ è½½æˆåŠŸ: {financial_df.shape}")

            # åˆ é™¤åŸå§‹è‚¡ä»·å¹¶é‡å‘½åå¤æƒåè‚¡ä»·
            cols_to_drop = ['open', 'max', 'min', 'close', 'daily_return']
            rename_mapping = {
                'adj_open': 'open',
                'adj_high': 'max',
                'adj_low': 'min',
                'adj_close': 'close',
                'adj_return': 'daily_return'
            }

            # ç¬¬äºŒæ­¥ï¼šå®‰å…¨åˆ é™¤åˆ—ï¼ˆä½¿ç”¨errors='ignore'é¿å…åˆ—ä¸å­˜åœ¨æ—¶æŠ¥é”™ï¼‰
            financial_df = financial_df.drop(columns=cols_to_drop, errors='ignore')

            # ç¬¬ä¸‰æ­¥ï¼šé‡å‘½åæŒ‡å®šåˆ—ï¼ˆåŒæ ·ä½¿ç”¨errors='ignore'å¢å¼ºé²æ£’æ€§ï¼‰
            financial_df = financial_df.rename(columns=rename_mapping, errors='ignore')

            # æŸ¥çœ‹æœ€ç»ˆçš„åˆ—åï¼ŒéªŒè¯æ“ä½œæ˜¯å¦æˆåŠŸ
            print("æœ€ç»ˆçš„åˆ—åï¼š", financial_df.columns.tolist())

            # ä¼˜åŒ–è´¢æŠ¥æ•°æ®å¤„ç†
            if not financial_df.empty:
                print("ä½¿ç”¨ä¼˜åŒ–ç‰ˆå¤„ç†è´¢æŠ¥æ•°æ®...")
                financial_wide = process_financial_data(financial_df)

                if financial_wide is not None and not financial_wide.empty:
                    print("ä½¿ç”¨ä¼˜åŒ–ç‰ˆåˆå¹¶è´¢æŠ¥æ•°æ®...")
                    price_df = merge_financial_data_optimized(price_df, financial_wide)

        except Exception as e:
            print(f"è´¢æŠ¥æ•°æ®åŠ è½½å¤±è´¥: {e}")
            financial_df = None

    # ==================== 5. è°ƒç”¨æŠ€æœ¯æŒ‡æ ‡è®¡ç®— ====================
    print("\nè®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆä½¿ç”¨ä¿®å¤ç‰ˆå‡½æ•°ï¼‰...")

    # éªŒè¯ä»·æ ¼æ•°æ®è´¨é‡
    print("ğŸ” éªŒè¯ä»·æ ¼æ•°æ®è´¨é‡...")
    if not validate_price_data(price_df):
        print("ä»·æ ¼æ•°æ®éªŒè¯å¤±è´¥")
        return None

    # è°ƒç”¨æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å‡½æ•°
    try:
        price_df = calculate_technical_indicators(price_df)
        print(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ!")

        # éªŒè¯æŠ€æœ¯ç‰¹å¾ç”Ÿæˆæƒ…å†µ
        tech_cols = [col for col in price_df.columns
                     if any(pattern in col for pattern in
                            ['ma_', 'ema_', 'volatility_', 'momentum_', 'rsi_',
                             'macd_', 'bb_', 'atr_', 'obv_', 'volume_ratio_',
                             'price_vs_', 'return_', 'log_return', 'price_change'])]

        print(f"ç”ŸæˆæŠ€æœ¯ç‰¹å¾: {len(tech_cols)} ä¸ª")
        if tech_cols:
            print(f"æŠ€æœ¯ç‰¹å¾ç¤ºä¾‹: {tech_cols[:10]}...")

    except Exception as e:
        print(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==================== 6. è®¡ç®—æœªæ¥æ”¶ç›Šç‡å’Œæ ‡ç­¾ ====================
    print("\nè®¡ç®—æœªæ¥æ”¶ç›Šç‡å’Œæ ‡ç­¾ï¼ˆé€‚åˆ20å¤©é¢„æµ‹ï¼‰...")

    # åœ¨è®¡ç®—æœªæ¥æ”¶ç›Šç‡ä¹‹å‰æ·»åŠ éªŒè¯
    print("éªŒè¯ä»·æ ¼æ•°æ®è´¨é‡...")
    if not validate_price_data(price_df):
        print("ä»·æ ¼æ•°æ®éªŒè¯å¤±è´¥")
        return None

    # æœªæ¥æ”¶ç›Šç‡è®¡ç®—å‡½æ•°
    try:
        price_df = calculate_future_returns_and_labels(price_df, days=FUTURE_DAYS)

        if price_df.empty:
            print("è®¡ç®—æœªæ¥æ”¶ç›Šç‡åæ•°æ®ä¸ºç©º")
            return None

        # éªŒè¯æ”¶ç›Šç‡è®¡ç®—
        if 'future_return' in price_df.columns:
            future_returns = price_df['future_return'].dropna()
            print(f"æœªæ¥æ”¶ç›Šç‡è®¡ç®—å®Œæˆ!")
            print(f"æœ‰æ•ˆæ”¶ç›Šç‡æ ·æœ¬: {len(future_returns):,}")
            print(f"æ”¶ç›Šç‡èŒƒå›´: {future_returns.min():.4f} åˆ° {future_returns.max():.4f}")
            print(f"å¹³å‡æ”¶ç›Šç‡: {future_returns.mean():.4f}")

    except Exception as e:
        print(f"âŒ æœªæ¥æ”¶ç›Šç‡è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==================== 7. ç‰¹å¾å·¥ç¨‹ ====================
    print("\nç‰¹å¾å·¥ç¨‹...")
    try:
        price_df, feature_cols = create_features(price_df)

        if price_df is None or len(feature_cols) < 5:
            print("ç‰¹å¾æ•°é‡ä¸è¶³")
            return None

        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ!")
        print(f"æ€»ç‰¹å¾æ•°é‡: {len(feature_cols)} ä¸ª")

        # ç»Ÿè®¡ç‰¹å¾ç±»å‹
        tech_features = [col for col in feature_cols if not col.startswith('fin_')]
        fin_features = [col for col in feature_cols if col.startswith('fin_')]
        other_features = [col for col in feature_cols if col not in tech_features and col not in fin_features]

        print(f"æŠ€æœ¯ç‰¹å¾: {len(tech_features)} ä¸ª")
        print(f"è´¢åŠ¡ç‰¹å¾: {len(fin_features)} ä¸ª")
        print(f"å…¶ä»–ç‰¹å¾: {len(other_features)} ä¸ª")
        print(f"ç‰¹å¾å¹³è¡¡æ¯”ä¾‹: {len(tech_features)}:{len(fin_features)}")

    except Exception as e:
        print(f"ç‰¹å¾å·¥ç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==================== 8. ä¿å­˜é¢„åˆå¹¶æ–‡ä»¶ ====================
    print("\nä¿å­˜é¢„åˆå¹¶æ–‡ä»¶ä¾›åç»­å¿«é€ŸåŠ è½½...")
    try:
        with open(PRE_MERGED_FILE, 'wb') as f:
            pickle.dump((price_df, feature_cols), f, protocol=4)
        print(f"é¢„åˆå¹¶æ•°æ®å·²ä¿å­˜: {PRE_MERGED_FILE}")
        print("ä¸‹æ¬¡è¿è¡Œå°†ç›´æ¥åŠ è½½æ­¤æ–‡ä»¶ï¼Œé€Ÿåº¦æå‡10-100å€ï¼")
    except Exception as e:
        print(f"é¢„åˆå¹¶ä¿å­˜å¤±è´¥: {e}")

    return price_df, feature_cols


def emergency_fix_returns_simple(df, days=FUTURE_DAYS):
    """ä¿®å¤æ”¶ç›Šç‡è®¡ç®— """
    print_section("ä¿®å¤æ”¶ç›Šç‡è®¡ç®—")

    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    df_fixed = df.copy()

    # 1. ç§»é™¤é›¶ä»·æ ¼å’Œæ— æ•ˆæ•°æ®
    print("1. æ¸…ç†æ— æ•ˆæ•°æ®...")
    zero_mask = df_fixed['close'] <= 0
    print(f"   ç§»é™¤é›¶ä»·æ ¼: {zero_mask.sum()} è¡Œ")
    df_fixed = df_fixed[~zero_mask]

    # 2. æŒ‰è‚¡ç¥¨å’Œæ—¥æœŸæ’åº
    df_fixed = df_fixed.sort_values(['stock_code', 'date'])

    # 3. é‡æ–°è®¡ç®—æœªæ¥æ”¶ç›Šç‡
    print("2. é‡æ–°è®¡ç®—æœªæ¥æ”¶ç›Šç‡...")

    def simple_recalculate(group):
        group = group.sort_values('date')
        # ä½¿ç”¨shiftè®¡ç®—æœªæ¥ä»·æ ¼
        group['future_price'] = group['close'].shift(-days)
        # è®¡ç®—æ”¶ç›Šç‡ï¼ˆæ·»åŠ å®‰å…¨æ€§æ£€æŸ¥ï¼‰
        valid_mask = (group['close'] > 0) & (group['future_price'] > 0)
        group['future_return_new'] = np.nan
        group.loc[valid_mask, 'future_return_new'] = (
                group.loc[valid_mask, 'future_price'] / group.loc[valid_mask, 'close'] - 1
        )
        return group

    try:
        df_fixed = df_fixed.groupby('stock_code', group_keys=False).apply(simple_recalculate)
        # ä½¿ç”¨æ–°è®¡ç®—çš„æ”¶ç›Šç‡
        df_fixed['future_return'] = df_fixed['future_return_new']
        print("æ”¶ç›Šç‡é‡æ–°è®¡ç®—å®Œæˆ")
    except Exception as e:
        print(f"åˆ†ç»„è®¡ç®—å¤±è´¥: {e}")
        return df  # å¤±è´¥æ—¶è¿”å›åŸæ•°æ®

    # 4. å¤„ç†ç‰¹æ®Šå€¼
    print("3. å¤„ç†ç‰¹æ®Šå€¼...")
    inf_mask = np.isinf(df_fixed['future_return'])
    if inf_mask.any():
        print(f"   ä¿®å¤ {inf_mask.sum()} ä¸ªinfå€¼...")
        df_fixed.loc[inf_mask, 'future_return'] = np.nan

    # 5. ç§»é™¤æ— æ•ˆè¡Œ
    initial_size = len(df_fixed)
    df_fixed = df_fixed.dropna(subset=['future_return'])
    final_size = len(df_fixed)
    print(f"æœ‰æ•ˆæ•°æ®: {final_size:,}/{initial_size:,} ({final_size / initial_size:.1%})")

    # 6. éªŒè¯ä¿®å¤ç»“æœ
    future_returns = df_fixed['future_return'].dropna()
    if len(future_returns) > 0:
        print(f"ç´§æ€¥ä¿®å¤å®Œæˆ!")
        print(f"æœ‰æ•ˆæ”¶ç›Šç‡: {len(future_returns):,}")
        print(f"èŒƒå›´: {future_returns.min():.6f} åˆ° {future_returns.max():.6f}")
        print(f"å‡å€¼: {future_returns.mean():.6f}")
        print(f"infå€¼: {np.isinf(future_returns).sum()}")
    else:
        print("ç´§æ€¥ä¿®å¤åæ²¡æœ‰æœ‰æ•ˆæ”¶ç›Šç‡!")

    return df_fixed


@timer_decorator
def merge_financial_data_optimized(price_df, financial_df):
    """ä¼˜åŒ–è´¢æŠ¥æ•°æ®åˆå¹¶-ä½¿ç”¨å‘é‡åŒ–æ“ä½œæå‡æ€§èƒ½"""
    if financial_df is None or financial_df.empty:
        return price_df

    print_section("ä¼˜åŒ–åˆå¹¶è´¢æŠ¥æ•°æ®")
    start_time = time.time()

    try:
        # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
        price_df = price_df.copy()
        financial_df = financial_df.copy()

        # ç¡®ä¿è‚¡ç¥¨ä»£ç æ ¼å¼ä¸€è‡´
        price_df['stock_code'] = price_df['stock_code'].astype(str).str.strip()
        financial_df['stock_code'] = financial_df['stock_code'].astype(str).str.strip()

        # å¤„ç†æ—¥æœŸ - ç¡®ä¿datetimeæ ¼å¼ä¸”æ— æ—¶åŒº
        price_df['date'] = pd.to_datetime(price_df['date'])
        if hasattr(price_df['date'].dtype, 'tz') and price_df['date'].dtype.tz is not None:
            price_df['date'] = price_df['date'].dt.tz_convert(None)

        if 'report_date' in financial_df.columns:
            financial_df['report_date'] = pd.to_datetime(financial_df['report_date'])
            # ç§»é™¤æ—¶åŒºä¿¡æ¯
            if hasattr(financial_df['report_date'].dtype, 'tz') and financial_df['report_date'].dtype.tz is not None:
                financial_df['report_date'] = financial_df['report_date'].dt.tz_convert(None)

        # æ‰¾å‡ºå…±åŒè‚¡ç¥¨
        common_stocks = set(price_df['stock_code'].unique()) & set(financial_df['stock_code'].unique())
        print(f"å…±åŒè‚¡ç¥¨æ•°é‡: {len(common_stocks)}")

        if len(common_stocks) == 0:
            print("æ²¡æœ‰å…±åŒè‚¡ç¥¨,ä»…ä½¿ç”¨è‚¡ä»·æ•°æ®")
            return price_df

        # æ–¹æ³•1: ä½¿ç”¨merge_asofè¿›è¡Œå¿«é€Ÿåˆå¹¶(æ€§èƒ½æœ€ä½³)
        try:
            print("ä½¿ç”¨merge_asofè¿›è¡Œå¿«é€Ÿåˆå¹¶...")

            # åªå¤„ç†å…±åŒè‚¡ç¥¨çš„æ•°æ®
            price_common = price_df[price_df['stock_code'].isin(common_stocks)].copy()
            financial_common = financial_df[financial_df['stock_code'].isin(common_stocks)].copy()

            # ä¿®å¤1: ç¡®ä¿æ•°æ®æ’åº - è¿™æ˜¯merge_asofçš„å…³é”®è¦æ±‚
            price_common = price_common.sort_values(['stock_code', 'date'])
            financial_common = financial_common.sort_values(['stock_code', 'report_date'])

            # ä¿®å¤2: ç¡®ä¿æ—¥æœŸåˆ—æ•°æ®ç±»å‹ä¸€è‡´
            price_common['date'] = pd.to_datetime(price_common['date'])
            financial_common['report_date'] = pd.to_datetime(financial_common['report_date'])

            # ä¿®å¤3: ç§»é™¤æ— ç©·å¤§å€¼å’ŒNaNå€¼
            financial_common = financial_common.replace([np.inf, -np.inf], np.nan)

            # ä¿®å¤4: ç§»é™¤é‡å¤çš„è´¢æŠ¥æ—¥æœŸï¼ˆæ¯ä¸ªè‚¡ç¥¨æ¯ä¸ªæŠ¥å‘Šæ—¥åªä¿ç•™ä¸€æ¡ï¼‰
            financial_common = financial_common.drop_duplicates(subset=['stock_code', 'report_date'], keep='last')

            # ä¿®å¤5: å¯¹å¤§æ•°æ®é›†è¿›è¡Œåˆ†å—å¤„ç†
            if len(price_common) > 100000:  # å¦‚æœæ•°æ®é‡å¾ˆå¤§
                print(f"å¤§æ•°æ®é‡æ£€æµ‹: {len(price_common):,}è¡Œï¼Œä½¿ç”¨åˆ†å—åˆå¹¶...")
                chunk_size = 50000
                merged_chunks = []

                # æŒ‰è‚¡ç¥¨ä»£ç åˆ†å—å¤„ç†
                stock_chunks = np.array_split(list(common_stocks), max(1, len(common_stocks) // 10))

                for i, stock_chunk in enumerate(tqdm(stock_chunks, desc="åˆ†å—åˆå¹¶")):
                    price_chunk = price_common[price_common['stock_code'].isin(stock_chunk)]
                    financial_chunk = financial_common[financial_common['stock_code'].isin(stock_chunk)]

                    if not price_chunk.empty and not financial_chunk.empty:
                        # ç¡®ä¿æ’åº
                        price_chunk = price_chunk.sort_values(['stock_code', 'date'])
                        financial_chunk = financial_chunk.sort_values(['stock_code', 'report_date'])

                        chunk_merged = pd.merge_asof(
                            price_chunk,
                            financial_chunk,
                            left_on='date',
                            right_on='report_date',
                            by='stock_code',
                            direction='backward',
                            allow_exact_matches=True
                        )
                        merged_chunks.append(chunk_merged)

                if merged_chunks:
                    merged_df = pd.concat(merged_chunks, ignore_index=True)
                else:
                    raise ValueError("åˆ†å—åˆå¹¶ç»“æœä¸ºç©º")
            else:
                # å°æ•°æ®é‡ç›´æ¥åˆå¹¶
                merged_df = pd.merge_asof(
                    price_common,
                    financial_common,
                    left_on='date',
                    right_on='report_date',
                    by='stock_code',
                    direction='backward',
                    allow_exact_matches=True
                )

            # å¤„ç†æ²¡æœ‰è´¢æŠ¥æ•°æ®çš„è‚¡ç¥¨
            price_other = price_df[~price_df['stock_code'].isin(common_stocks)].copy()

            # åˆå¹¶æ‰€æœ‰æ•°æ®
            final_merged = pd.concat([merged_df, price_other], ignore_index=True)

            # ä¿®å¤6: è´¢åŠ¡ç‰¹å¾åˆ—åå‰ç¼€å¤„ç†
            # ç¡®ä¿è´¢åŠ¡ç‰¹å¾åˆ—åæœ‰'fin_'å‰ç¼€
            financial_cols = [col for col in final_merged.columns
                              if col not in ['stock_code', 'date', 'report_date']
                              and col not in price_df.columns]
            for col in financial_cols:
                if not col.startswith('fin_'):
                    final_merged = final_merged.rename(columns={col: f'fin_{col}'})

            end_time = time.time()
            print(f"merge_asofåˆå¹¶å®Œæˆ! å½¢çŠ¶: {final_merged.shape}")
            print(f"åˆå¹¶æ—¶é—´: {end_time - start_time:.2f}ç§’")
            return final_merged

        except Exception as e:
            print(f"merge_asofå¤±è´¥: {e}")
            print("ä½¿ç”¨åˆ†ç»„ä¼˜åŒ–æ–¹æ³•...")
            # å›é€€åˆ°åˆ†ç»„ä¼˜åŒ–æ–¹æ³•
            return merge_financial_data_grouped(price_df, financial_df, common_stocks)

    except Exception as e:
        print(f"ä¼˜åŒ–åˆå¹¶å¤±è´¥: {e}")
        return price_df


def merge_financial_data_grouped(price_df, financial_df, common_stocks):
    """ä¼˜åŒ–ç‰ˆåˆ†ç»„åˆå¹¶æ–¹æ³• - æ›¿ä»£åŸæœ‰çš„groupedå‡½æ•°"""
    print("ä½¿ç”¨ä¼˜åŒ–ç‰ˆåˆ†ç»„åˆå¹¶æ–¹æ³•...")
    start_time = time.time()

    # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼åŠ é€Ÿ
    merged_chunks = []

    for stock_code in tqdm(common_stocks, desc="ä¼˜åŒ–åˆå¹¶è´¢æŠ¥"):
        try:
            # è·å–è‚¡ç¥¨æ•°æ®
            stock_prices = price_df[price_df['stock_code'] == stock_code].copy().sort_values('date')
            stock_financials = financial_df[financial_df['stock_code'] == stock_code].sort_values('report_date')

            if stock_financials.empty:
                merged_chunks.append(stock_prices)
                continue

            # ä½¿ç”¨å‘é‡åŒ–æ“ä½œåŠ é€Ÿ
            price_dates = stock_prices['date'].values
            financial_dates = stock_financials['report_date'].values

            # ä½¿ç”¨searchsortedè¿›è¡Œå¿«é€ŸæŸ¥æ‰¾
            indices = np.searchsorted(financial_dates, price_dates, side='right') - 1

            # æ‰¹é‡å¤„ç†
            valid_indices = indices >= 0
            valid_price_indices = np.where(valid_indices)[0]

            if len(valid_price_indices) > 0:
                # æ‰¹é‡å¤„ç†æœ‰æ•ˆç´¢å¼•
                for i in valid_price_indices:
                    idx = indices[i]
                    latest_financial = stock_financials.iloc[idx]
                    price_row = stock_prices.iloc[i:i + 1].copy()

                    # æ·»åŠ è´¢åŠ¡æŒ‡æ ‡ï¼ˆåªæ·»åŠ æ•°å€¼å‹æŒ‡æ ‡ï¼‰
                    for col, value in latest_financial.items():
                        if col not in ['stock_code', 'report_date'] and pd.api.types.is_numeric_dtype(
                                type(value)) and pd.notna(value):
                            price_row[f'fin_{col}'] = value

                    merged_chunks.append(price_row)

                # å¤„ç†æ²¡æœ‰è´¢æŠ¥æ•°æ®çš„æ—¥æœŸ
                invalid_indices = np.where(~valid_indices)[0]
                if len(invalid_indices) > 0:
                    for i in invalid_indices:
                        merged_chunks.append(stock_prices.iloc[i:i + 1])
            else:
                # æ‰€æœ‰æ—¥æœŸéƒ½æ²¡æœ‰è´¢æŠ¥æ•°æ®
                merged_chunks.append(stock_prices)

        except Exception as e:
            print(f"è‚¡ç¥¨ {stock_code} åˆå¹¶å¤±è´¥: {e}")
            # å³ä½¿å¤±è´¥ä¹Ÿæ·»åŠ åŸºç¡€æ•°æ®
            merged_chunks.append(price_df[price_df['stock_code'] == stock_code])

    # åˆå¹¶æ‰€æœ‰å—
    if merged_chunks:
        result_df = pd.concat(merged_chunks, ignore_index=True)
        end_time = time.time()
        print(f"ä¼˜åŒ–åˆ†ç»„åˆå¹¶å®Œæˆ! å½¢çŠ¶: {result_df.shape}")
        print(f"åˆå¹¶æ—¶é—´: {end_time - start_time:.2f}ç§’")
        return result_df

    return price_df


@timer_decorator
def process_financial_data(financial_df):
    """å¤„ç†è´¢æŠ¥æ•°æ®"""
    if financial_df.empty:
        return pd.DataFrame()

    print_section("å¤„ç†è´¢æŠ¥æ•°æ®")

    print(f"è´¢æŠ¥æ•°æ®å½¢çŠ¶: {financial_df.shape}")
    print(f"è´¢æŠ¥åˆ—å: {list(financial_df.columns)}")

    # æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®
    print("\nè´¢æŠ¥æ•°æ®æ ·ä¾‹ï¼ˆå‰5è¡Œï¼‰:")
    print(financial_df.head())

    # åˆ›å»ºå‰¯æœ¬
    df = financial_df.copy()

    # å»é‡
    df = df.drop_duplicates()
    print(f"å»é‡åå½¢çŠ¶: {df.shape}")

    # å¤„ç†è‚¡ç¥¨ä»£ç 
    if 'number' in df.columns:
        df['stock_code'] = df['number'].astype(str).str.strip()
    elif 'symbol' in df.columns:
        df['stock_code'] = df['symbol'].astype(str).str.strip()
    else:
        print("ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºè‚¡ç¥¨ä»£ç ")
        df['stock_code'] = df.iloc[:, 0].astype(str).str.strip()

    print(f"è‚¡ç¥¨æ•°é‡: {df['stock_code'].nunique()}")

    # è´¢åŠ¡æŒ‡æ ‡æ˜ å°„
    financial_mapping = {
        'ç¾é‡‘åŠç´„ç•¶ç¾é‡‘': 'cash',
        'Cash and cash equivalents': 'cash',
        'æµå‹•è³‡ç”¢åˆè¨ˆ': 'current_assets',
        'Total current assets': 'current_assets',
        'è³‡ç”¢ç¸½è¨ˆ': 'total_assets',
        'Total assets': 'total_assets',
        'æµå‹•è² å‚µåˆè¨ˆ': 'current_liabilities',
        'Total current liabilities': 'current_liabilities',
        'è² å‚µåˆè¨ˆ': 'total_liabilities',
        'Total liabilities': 'total_liabilities',
        'è‚¡æ±æ¬Šç›Šåˆè¨ˆ': 'equity',
        'Total equity': 'equity',
        'æ‡‰æ”¶å¸³æ¬¾æ·¨é¡': 'accounts_receivable',
        'Accounts receivable, net': 'accounts_receivable',
        'å­˜è²¨': 'inventory',
        'Current inventories': 'inventory',
        'ç‡Ÿæ¥­æ”¶å…¥åˆè¨ˆ': 'revenue',
        'Total operating revenue': 'revenue',
        'ç‡Ÿæ¥­æˆæœ¬åˆè¨ˆ': 'operating_costs',
        'Total operating costs': 'operating_costs',
        'ç‡Ÿæ¥­æ¯›åˆ©ï¼ˆæ¯›æï¼‰': 'gross_profit',
        'Gross profit (loss)': 'gross_profit',
        'ç‡Ÿæ¥­åˆ©ç›Šï¼ˆæå¤±ï¼‰': 'operating_profit',
        'Operating profit (loss)': 'operating_profit',
        'æœ¬æœŸç¨…å¾Œæ·¨åˆ©ï¼ˆæ·¨æï¼‰': 'net_profit',
        'Profit (loss)': 'net_profit',
        'åŸºæœ¬æ¯è‚¡ç›ˆé¤˜åˆè¨ˆ': 'eps',
        'Total basic earnings per share': 'eps',
        'ç‡Ÿæ¥­æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰': 'operating_cash_flow',
        'Net cash flows from (used in) operating activities': 'operating_cash_flow',
        'æŠ•è³‡æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰': 'investing_cash_flow',
        'Net cash flows from (used in) investing activities': 'investing_cash_flow',
        'ç±Œè³‡æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰': 'financing_cash_flow',
        'Net cash flows from (used in) financing activities': 'financing_cash_flow'
    }

    def map_financial_indicator(key, key_en):
        if pd.isna(key) and pd.isna(key_en):
            return None

        key_str = str(key) if pd.notna(key) else ''
        key_en_str = str(key_en) if pd.notna(key_en) else ''

        # å…ˆå°è¯•ä¸­æ–‡åŒ¹é…
        for chinese_name, std_name in financial_mapping.items():
            if chinese_name in key_str:
                return std_name

        # å†å°è¯•è‹±æ–‡åŒ¹é…
        for english_name, std_name in financial_mapping.items():
            if english_name.lower() in key_en_str.lower():
                return std_name

        return None

    # æŸ¥æ‰¾æŒ‡æ ‡åç§°åˆ—
    indicator_col = None
    for col in ['key', 'key_en', 'indicator', 'account', 'item']:
        if col in df.columns:
            indicator_col = col
            break

    if indicator_col is None:
        print("ä½¿ç”¨ç¬¬ä¸€åˆ—éè‚¡ç¥¨ä»£ç åˆ—ä½œä¸ºæŒ‡æ ‡")
        indicator_col = df.columns[1] if len(df.columns) > 1 else None

    if indicator_col:
        # å¤„ç†æ•°å€¼
        if 'value' in df.columns:
            df['value'] = pd.to_numeric(df['value'], errors='coerce')

        # å¤„ç†æ—¥æœŸ
        if 'year' in df.columns and 'period' in df.columns:
            # å°æ¹¾è´¢æŠ¥æ—¥æœŸé€šå¸¸: Q1(5/15), Q2(8/14), Q3(11/14), Q4(æ¬¡å¹´3/31)
            def get_report_date(row):
                try:
                    year = int(row['year'])
                    period = int(row['period'])

                    if period == 1:  # ç¬¬ä¸€å­£åº¦
                        return pd.Timestamp(f"{year}-05-15")
                    elif period == 2:  # ç¬¬äºŒå­£åº¦
                        return pd.Timestamp(f"{year}-08-14")
                    elif period == 3:  # ç¬¬ä¸‰å­£åº¦
                        return pd.Timestamp(f"{year}-11-14")
                    elif period == 4:  # ç¬¬å››å­£åº¦
                        return pd.Timestamp(f"{year + 1}-03-31")
                    else:
                        return pd.NaT
                except:
                    return pd.NaT

            df['report_date'] = df.apply(get_report_date, axis=1)
        elif 'date' in df.columns:
            df['report_date'] = pd.to_datetime(df['date'], errors='coerce')
        else:
            print("æ— æ³•ç¡®å®šè´¢æŠ¥æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ")
            df['report_date'] = datetime.now()

        # ç§»é™¤æ— æ•ˆæ—¥æœŸ
        df = df[df['report_date'].notna()]

        # è·å–æ˜ å°„åˆ—
        key_col = indicator_col
        key_en_col = None
        for col in ['key_en', 'account_en', 'item_en']:
            if col in df.columns:
                key_en_col = col
                break

        if key_en_col:
            df['mapped_indicator'] = df.apply(
                lambda x: map_financial_indicator(x[key_col], x[key_en_col]), axis=1
            )
        else:
            df['mapped_indicator'] = df[key_col].apply(
                lambda x: map_financial_indicator(x, None)
            )

        # ç»Ÿè®¡æ˜ å°„ç»“æœ
        mapped_count = df['mapped_indicator'].notna().sum()
        print(f"è´¢åŠ¡æŒ‡æ ‡æ˜ å°„æˆåŠŸç‡: {mapped_count / len(df):.2%} ({mapped_count}/{len(df)})")

        if mapped_count > 0:
            # è½¬æ¢ä¸ºå®½è¡¨æ ¼å¼
            financial_wide = df.pivot_table(
                index=['stock_code', 'report_date'],
                columns='mapped_indicator',
                values='value',
                aggfunc='first'
            ).reset_index()

            financial_wide.columns.name = None

            # ç§»é™¤æ—¶åŒºä¿¡æ¯
            if hasattr(financial_wide['report_date'].dtype, 'tz') and financial_wide[
                'report_date'].dtype.tz is not None:
                financial_wide['report_date'] = financial_wide['report_date'].dt.tz_convert(None)

            # è®¡ç®—è´¢åŠ¡æ¯”ç‡
            print("è®¡ç®—è´¢åŠ¡æ¯”ç‡...")

            # é€šç”¨é™¤0è®¡ç®—å‡½æ•°ï¼ˆå¯é€‰ï¼Œç®€åŒ–ä»£ç ï¼‰
            def safe_divide(numerator, denominator, default=np.nan):
                """å®‰å…¨é™¤æ³•ï¼šé¿å…é™¤0ï¼Œè¿”å›é»˜è®¤å€¼"""
                return np.where(
                    (denominator != 0) & ~np.isnan(denominator),
                    numerator / denominator,
                    default
                )

            if all(col in financial_wide.columns for col in ['revenue', 'operating_costs']):
                # æ¯›åˆ©ç‡ï¼ˆåŸæœ‰é™¤0ä¿ç•™ï¼Œå¯æ”¹ç”¨é€šç”¨å‡½æ•°ï¼‰
                financial_wide['gross_margin'] = safe_divide(
                    (financial_wide['revenue'] - financial_wide['operating_costs']),
                    financial_wide['revenue']
                )
                print("  âœ“ è®¡ç®—æ¯›åˆ©ç‡ï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            if all(col in financial_wide.columns for col in ['revenue', 'operating_profit']):
                # è¥ä¸šåˆ©æ¶¦ç‡ï¼ˆæ–°å¢é™¤0ä¿æŠ¤ï¼‰
                financial_wide['operating_margin'] = safe_divide(
                    financial_wide['operating_profit'],
                    financial_wide['revenue']
                )
                print("  âœ“ è®¡ç®—è¥ä¸šåˆ©æ¶¦ç‡ï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            if all(col in financial_wide.columns for col in ['revenue', 'net_profit']):
                # å‡€åˆ©ç‡ï¼ˆæ–°å¢é™¤0ä¿æŠ¤ï¼‰
                financial_wide['net_margin'] = safe_divide(
                    financial_wide['net_profit'],
                    financial_wide['revenue']
                )
                print("  âœ“ è®¡ç®—å‡€åˆ©ç‡ï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            if all(col in financial_wide.columns for col in ['current_assets', 'current_liabilities']):
                # æµåŠ¨æ¯”ç‡ï¼ˆæ–°å¢é™¤0ä¿æŠ¤ï¼‰
                financial_wide['current_ratio'] = safe_divide(
                    financial_wide['current_assets'],
                    financial_wide['current_liabilities']
                )
                print("  âœ“ è®¡ç®—æµåŠ¨æ¯”ç‡ï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            if all(col in financial_wide.columns for col in ['total_assets', 'total_liabilities']):
                # èµ„äº§è´Ÿå€ºç‡/æƒç›Šæ¯”ç‡ï¼ˆæ–°å¢é™¤0ä¿æŠ¤ï¼‰
                financial_wide['debt_to_assets'] = safe_divide(
                    financial_wide['total_liabilities'],
                    financial_wide['total_assets']
                )
                financial_wide['equity_ratio'] = 1 - financial_wide['debt_to_assets'].fillna(0)
                print("  âœ“ è®¡ç®—èµ„äº§è´Ÿå€ºç‡å’Œæƒç›Šæ¯”ç‡ï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            if all(col in financial_wide.columns for col in ['equity', 'net_profit']):
                # ROEï¼ˆæ–°å¢é™¤0ä¿æŠ¤ï¼‰
                financial_wide['roe'] = safe_divide(
                    financial_wide['net_profit'],
                    financial_wide['equity']
                )
                print("  âœ“ è®¡ç®—ROEï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            if all(col in financial_wide.columns for col in ['total_assets', 'net_profit']):
                # ROAï¼ˆæ–°å¢é™¤0ä¿æŠ¤ï¼‰
                financial_wide['roa'] = safe_divide(
                    financial_wide['net_profit'],
                    financial_wide['total_assets']
                )
                print("  âœ“ è®¡ç®—ROAï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            if all(col in financial_wide.columns for col in ['operating_cash_flow', 'total_liabilities']):
                # ç»è¥ç°é‡‘æµ/è´Ÿå€ºï¼ˆæ–°å¢é™¤0ä¿æŠ¤ï¼‰
                financial_wide['ocf_to_debt'] = safe_divide(
                    financial_wide['operating_cash_flow'],
                    financial_wide['total_liabilities']
                )
                print("  âœ“ è®¡ç®—ç»è¥æ´»åŠ¨ç°é‡‘æµ/è´Ÿå€ºæ¯”ç‡ï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            if all(col in financial_wide.columns for col in ['operating_cash_flow', 'revenue']):
                # ç»è¥ç°é‡‘æµ/æ”¶å…¥ï¼ˆæ–°å¢é™¤0ä¿æŠ¤ï¼‰
                financial_wide['ocf_margin'] = safe_divide(
                    financial_wide['operating_cash_flow'],
                    financial_wide['revenue']
                )
                print("  âœ“ è®¡ç®—ç»è¥æ´»åŠ¨ç°é‡‘æµ/æ”¶å…¥æ¯”ç‡ï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            if 'revenue' in financial_wide.columns:
                # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—è¥æ”¶å¢é•¿ç‡ï¼ˆå½“æœŸ/ä¸ŠæœŸ -1ï¼‰
                financial_wide['revenue_growth'] = financial_wide.groupby('stock_code')['revenue'].pct_change()
                print("  âœ“ è®¡ç®—è¥æ”¶å¢é•¿ç‡")

            if 'net_profit' in financial_wide.columns:
                # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—åˆ©æ¶¦å¢é•¿ç‡
                financial_wide['profit_growth'] = financial_wide.groupby('stock_code')['net_profit'].pct_change()
                print("  âœ“ è®¡ç®—åˆ©æ¶¦å¢é•¿ç‡")

            # ç›ˆåˆ©å› å­ = (ROE + å‡€åˆ©ç‡ + æ¯›åˆ©ç‡)/3
            if all(col in financial_wide.columns for col in ['roe', 'net_margin', 'gross_margin']):
                financial_wide['profit_factor'] = safe_divide(
                    financial_wide['roe'] + financial_wide['net_margin'] + financial_wide['gross_margin'],
                    3,
                    default=np.nan
                )
                print("  âœ“ è®¡ç®—ç›ˆåˆ©å› å­ï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            # æˆé•¿å› å­ = (è¥æ”¶å¢é•¿ç‡ + å‡€åˆ©æ¶¦å¢é•¿ç‡)/2
            if all(col in financial_wide.columns for col in ['revenue_growth', 'profit_growth']):
                financial_wide['growth_factor'] = safe_divide(
                    financial_wide['revenue_growth'] + financial_wide['profit_growth'],
                    2,
                    default=np.nan
                )
                print("  âœ“ è®¡ç®—æˆé•¿å› å­ï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            # è´¨é‡å› å­ = (ç°é‡‘æµ/è¥æ”¶ + ROA)/2ï¼ˆocf_margin å³ ç°é‡‘æµ/è¥æ”¶ï¼‰
            if all(col in financial_wide.columns for col in ['ocf_margin', 'roa']):
                financial_wide['quality_factor'] = safe_divide(
                    financial_wide['ocf_margin'] + financial_wide['roa'],
                    2,
                    default=np.nan
                )
                print("  âœ“ è®¡ç®—è´¨é‡å› å­ï¼ˆå«é™¤0ä¿æŠ¤ï¼‰")

            # å¤„ç†ç¼ºå¤±å€¼
            numeric_cols = [col for col in financial_wide.columns
                            if col not in ['stock_code', 'report_date'] and pd.api.types.is_numeric_dtype(
                    financial_wide[col])]

            for col in numeric_cols:
                if col in financial_wide.columns:
                    financial_wide[col] = financial_wide.groupby('stock_code')[col].transform(
                        lambda x: x.ffill().bfill().fillna(x.median())
                    )

            print("è¿‡æ»¤æ— æ•ˆè´¢åŠ¡å› å­...")
            numeric_cols = [col for col in financial_wide.columns
                            if col not in ['stock_code', 'report_date']]
            invalid_cols = []

            for col in numeric_cols:
                # è§„åˆ™1ï¼šç¼ºå¤±å€¼å æ¯”>50% â†’ æ— æ•ˆ
                missing_ratio = 1 - financial_wide[col].notna().mean()
                if missing_ratio > 0.5:
                    invalid_cols.append(col)
                    continue
                # è§„åˆ™2ï¼šæ–¹å·®<0.001ï¼ˆå‡ ä¹æ— æ³¢åŠ¨ï¼‰â†’ æ— æ•ˆ
                col_var = financial_wide[col].var()
                if pd.isna(col_var) or col_var < 0.001:
                    invalid_cols.append(col)

            # åˆ é™¤æ— æ•ˆå› å­
            if invalid_cols:
                financial_wide = financial_wide.drop(columns=invalid_cols)
                print(f"  åˆ é™¤æ— æ•ˆå› å­: {invalid_cols}")
            else:
                print(f"  æ— æ— æ•ˆå› å­ï¼Œä¿ç•™æ‰€æœ‰{len(numeric_cols)}ä¸ªè´¢åŠ¡å› å­")

            # å®šä¹‰è¦ä¿ç•™çš„åˆ—
            core_cols = ['stock_code', 'report_date']  # æ ¸å¿ƒæ ‡è¯†åˆ—
            absolute_indicators = ['cash', 'total_assets', 'revenue']  # 3ä¸ªç»å¯¹å€¼æŒ‡æ ‡
            relative_indicators = [  # 15ä¸ªç›¸å¯¹å€¼æŒ‡æ ‡ï¼‰
                'gross_margin', 'operating_margin', 'net_margin',
                'current_ratio', 'debt_to_assets', 'equity_ratio',
                'roe', 'roa', 'ocf_to_debt', 'ocf_margin', 'revenue_growth', 'profit_growth',
                'profit_factor', 'growth_factor', 'quality_factor'
            ]

            # åªä¿ç•™å­˜åœ¨çš„åˆ—
            keep_cols = core_cols.copy()
            keep_cols += [col for col in absolute_indicators if col in financial_wide.columns]
            keep_cols += [col for col in relative_indicators if col in financial_wide.columns]

            # ç­›é€‰åˆ—
            financial_wide = financial_wide[keep_cols]

            # å…³é”®ä¼˜åŒ–ï¼šæ¸…ç†å¼‚å¸¸å€¼ï¼ˆinf/-inf/NaNï¼‰
            numeric_cols = [col for col in financial_wide.columns if col not in core_cols]
            for col in numeric_cols:
                # æ›¿æ¢æ— ç©·å¤§å€¼ä¸ºNaNï¼Œå†ç”¨ä¸­ä½æ•°å¡«å……
                financial_wide[col] = financial_wide[col].replace([np.inf, -np.inf], np.nan)
                # æŒ‰è‚¡ç¥¨åˆ†ç»„å¡«å……ï¼Œä¿è¯åŒè‚¡ç¥¨æ•°æ®çš„ä¸€è‡´æ€§
                financial_wide[col] = financial_wide.groupby('stock_code')[col].transform(
                    lambda x: x.fillna(x.median())
                )

            factor_cols = [col for col in financial_wide.columns if col not in core_cols]

            print(f"è´¢æŠ¥å¤„ç†å®Œæˆ: {financial_wide.shape}")
            print(f"æ—¶é—´èŒƒå›´: {financial_wide['report_date'].min()} åˆ° {financial_wide['report_date'].max()}")

            return financial_wide

    print("è´¢æŠ¥æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¿”å›ç©ºDataFrame")
    return pd.DataFrame()


# ==================== æ–°å¢å› å­å¤„ç†å‡½æ•° ====================
def winsorize_factor(df, factor_cols, limits=WINSORIZE_LIMITS):
    """
    å› å­å»æå€¼ï¼ˆWinsorizeï¼‰
    :param df: æ•°æ®æ¡†
    :param factor_cols: å› å­åˆ—åˆ—è¡¨
    :param limits: å»æå€¼åˆ†ä½æ•°ï¼Œ(ä¸‹é™, ä¸Šé™)
    :return: å»æå€¼åçš„æ•°æ®æ¡†
    """
    print_section("å› å­å»æå€¼å¤„ç†")
    df_copy = df.copy()

    for col in tqdm(factor_cols, desc="å»æå€¼å¤„ç†"):
        if col in df_copy.columns and pd.api.types.is_numeric_dtype(df_copy[col]):
            # ä¿ç•™éç©ºå€¼è¿›è¡Œå»æå€¼
            non_na_mask = df_copy[col].notna()
            if non_na_mask.sum() > 0:
                # å»æå€¼
                df_copy.loc[non_na_mask, col] = winsorize(
                    df_copy.loc[non_na_mask, col].values,
                    limits=limits
                )

    print(f"å®Œæˆ {len(factor_cols)} ä¸ªå› å­çš„å»æå€¼å¤„ç†")
    return df_copy


def market_cap_neutralization(df, factor_cols, market_cap_col='market_cap'):
    """
    å¸‚å€¼ä¸­æ€§åŒ–ï¼ˆå¯¹å› å­è¿›è¡Œå¸‚å€¼å›å½’ï¼Œå–æ®‹å·®ä½œä¸ºä¸­æ€§åŒ–åçš„å› å­ï¼‰
    :param df: æ•°æ®æ¡†
    :param factor_cols: éœ€è¦ä¸­æ€§åŒ–çš„å› å­åˆ—
    :param market_cap_col: å¸‚å€¼åˆ—å
    :return: ä¸­æ€§åŒ–åçš„æ•°æ®æ¡†
    """
    print_section("å¸‚å€¼ä¸­æ€§åŒ–å¤„ç†")

    # å¦‚æœæ²¡æœ‰å¸‚å€¼åˆ—ï¼Œå°è¯•ä»ç°æœ‰æ•°æ®è®¡ç®—
    if market_cap_col not in df.columns:
        print("æœªæ‰¾åˆ°å¸‚å€¼åˆ—ï¼Œå°è¯•ä»ä»·æ ¼å’Œæˆäº¤é‡ä¼°ç®—...")
        if 'close' in df.columns and 'volume' in df.columns:
            df['market_cap'] = df['close'] * df.groupby('stock_code')['volume'].rolling(window=20,
                                                                                        min_periods=5).mean().reset_index(
                0, drop=True)
            market_cap_col = 'market_cap'
        else:
            print("æ— æ³•ä¼°ç®—å¸‚å€¼ï¼Œè·³è¿‡å¸‚å€¼ä¸­æ€§åŒ–")
            return df

    df_copy = df.copy()

    for col in tqdm(factor_cols, desc="å¸‚å€¼ä¸­æ€§åŒ–"):
        if col in df_copy.columns and pd.api.types.is_numeric_dtype(df_copy[col]):
            # å‡†å¤‡å›å½’æ•°æ®
            reg_data = df_copy[[col, market_cap_col]].dropna()

            if len(reg_data) > 10:  # è‡³å°‘éœ€è¦10ä¸ªæ ·æœ¬
                X = reg_data[[market_cap_col]]
                y = reg_data[col]

                # çº¿æ€§å›å½’
                lr = LinearRegression()
                lr.fit(X, y)

                # è®¡ç®—æ®‹å·®ï¼ˆä¸­æ€§åŒ–åçš„å› å­ï¼‰
                residuals = y - lr.predict(X)

                # æ›¿æ¢åŸå› å­å€¼
                df_copy.loc[reg_data.index, col] = residuals

    print(f"å®Œæˆ {len(factor_cols)} ä¸ªå› å­çš„å¸‚å€¼ä¸­æ€§åŒ–")
    return df_copy


def standardize_factor(df, factor_cols):
    """
    å› å­æ ‡å‡†åŒ–ï¼ˆZ-scoreæ ‡å‡†åŒ–ï¼‰
    :param df: æ•°æ®æ¡†
    :param factor_cols: å› å­åˆ—åˆ—è¡¨
    :return: æ ‡å‡†åŒ–åçš„æ•°æ®æ¡†
    """
    print_section("å› å­æ ‡å‡†åŒ–å¤„ç†")
    df_copy = df.copy()

    scaler = StandardScaler()
    for col in tqdm(factor_cols, desc="æ ‡å‡†åŒ–å¤„ç†"):
        if col in df_copy.columns and pd.api.types.is_numeric_dtype(df_copy[col]):
            non_na_mask = df_copy[col].notna()
            if non_na_mask.sum() > 0:
                # æŒ‰æ—¥æœŸåˆ†ç»„æ ‡å‡†åŒ–ï¼ˆæ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼‰
                def standardize_group(group):
                    if group.notna().sum() > 1:
                        return (group - group.mean()) / group.std()
                    return group

                df_copy.loc[non_na_mask, col] = df_copy.loc[non_na_mask].groupby('date')[col].transform(
                    standardize_group)

    print(f"å®Œæˆ {len(factor_cols)} ä¸ªå› å­çš„æ ‡å‡†åŒ–å¤„ç†")
    return df_copy


def calculate_ic_measures(df, factor_cols, target_col='future_return', date_col='date'):
    """
    è®¡ç®—å› å­çš„ICæŒ‡æ ‡ï¼ˆä¿¡æ¯ç³»æ•°ï¼‰
    :param df: æ•°æ®æ¡†
    :param factor_cols: å› å­åˆ—åˆ—è¡¨
    :param target_col: ç›®æ ‡æ”¶ç›Šç‡åˆ—
    :param date_col: æ—¥æœŸåˆ—
    :return: ICç»Ÿè®¡ç»“æœå­—å…¸
    """
    print_section("è®¡ç®—å› å­ICæŒ‡æ ‡")

    ic_results = {
        'factor': [],
        'ic_mean': [],
        'ic_std': [],
        'icir': [],
        'winrate': [],
        'rolling_std': []
    }

    # æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—æ¯æ—¥IC
    daily_ic = {}
    dates = sorted(df[date_col].unique())

    for col in tqdm(factor_cols, desc="è®¡ç®—IC"):
        if col not in df.columns:
            continue

        # å­˜å‚¨æ¯æ—¥IC
        ic_values = []
        ic_signs = []

        for date in dates:
            daily_data = df[df[date_col] == date]
            valid_data = daily_data[[col, target_col]].dropna()

            if len(valid_data) >= 20:  # è‡³å°‘20ä¸ªæ ·æœ¬
                # è®¡ç®—Spearmanç›¸å…³ç³»æ•°ï¼ˆICï¼‰
                ic = valid_data[col].corr(valid_data[target_col], method='spearman')
                if not np.isnan(ic):
                    ic_values.append(ic)
                    ic_signs.append(1 if ic > 0 else 0)

        if len(ic_values) > 0:
            # è®¡ç®—ICå‡å€¼
            ic_mean = np.mean(ic_values)
            # è®¡ç®—ICæ ‡å‡†å·®
            ic_std = np.std(ic_values)
            # è®¡ç®—ICIRï¼ˆä¿¡æ¯ç³»æ•°ä¿¡æ¯æ¯”ç‡ï¼‰
            icir = ic_mean / ic_std if ic_std != 0 else 0
            # è®¡ç®—èƒœç‡ï¼ˆICä¸ºæ­£çš„æ¯”ä¾‹ï¼‰
            winrate = np.mean(ic_signs) if len(ic_signs) > 0 else 0

            # è®¡ç®—æ»šåŠ¨æ ‡å‡†å·®ï¼ˆç¨³å®šæ€§æ£€æŸ¥ï¼‰
            ic_series = pd.Series(ic_values, index=dates[:len(ic_values)])
            rolling_std = ic_series.rolling(window=ROLLING_WINDOW_MONTHS).std().mean()

            # ä¿å­˜ç»“æœ
            ic_results['factor'].append(col)
            ic_results['ic_mean'].append(ic_mean)
            ic_results['ic_std'].append(ic_std)
            ic_results['icir'].append(icir)
            ic_results['winrate'].append(winrate)
            ic_results['rolling_std'].append(rolling_std)

    # è½¬æ¢ä¸ºDataFrame
    ic_df = pd.DataFrame(ic_results)
    print(f"å®Œæˆ {len(ic_df)} ä¸ªå› å­çš„ICæŒ‡æ ‡è®¡ç®—")
    return ic_df


def filter_factors_by_ic(ic_df, factor_type='financial'):
    """
    æ ¹æ®ICæŒ‡æ ‡ç­›é€‰å› å­
    :param ic_df: ICç»Ÿè®¡ç»“æœDataFrame
    :param factor_type: å› å­ç±»å‹ 'financial' æˆ– 'technical'
    :return: ç­›é€‰åçš„å› å­åˆ—è¡¨
    """
    print_section(f"{factor_type.upper()}å› å­ICç­›é€‰")

    if factor_type == 'financial':
        ic_mean_thresh = FIN_IC_MEAN_THRESHOLD
        icir_thresh = FIN_ICIR_THRESHOLD
        winrate_thresh = FIN_WINRATE_THRESHOLD
    else:
        ic_mean_thresh = TECH_IC_MEAN_THRESHOLD
        icir_thresh = TECH_ICIR_THRESHOLD
        winrate_thresh = TECH_WINRATE_THRESHOLD

    # åˆå§‹ç­›é€‰
    filtered = ic_df[
        (abs(ic_df['ic_mean']) >= ic_mean_thresh) &
        (abs(ic_df['icir']) >= icir_thresh) &
        (ic_df['winrate'] >= winrate_thresh)
        ].copy()

    # ç¨³å®šæ€§æ£€æŸ¥ï¼šå‰”é™¤æ»šåŠ¨æ ‡å‡†å·®è¿‡å¤§çš„å› å­
    stable_filtered = filtered[filtered['rolling_std'] <= ROLLING_STD_THRESHOLD].copy()

    # æŒ‰ICIRæ’åº
    stable_filtered = stable_filtered.sort_values('icir', ascending=False)

    print(f"åˆå§‹ç­›é€‰åå‰©ä½™: {len(filtered)} ä¸ªå› å­")
    print(f"ç¨³å®šæ€§æ£€æŸ¥åå‰©ä½™: {len(stable_filtered)} ä¸ªå› å­")

    # æ˜¾ç¤ºç­›é€‰ç»“æœ
    if len(stable_filtered) > 0:
        print(f"\n{factor_type.upper()}å› å­ç­›é€‰ç»“æœï¼ˆæŒ‰ICIRæ’åºï¼‰:")
        print(stable_filtered[['factor', 'ic_mean', 'icir', 'winrate', 'rolling_std']].round(4))

    return stable_filtered['factor'].tolist()


def remove_high_correlation_factors(df, factor_cols, factor_type='financial'):
    """
    ç§»é™¤é«˜ç›¸å…³æ€§å› å­ï¼Œä¿ç•™ICæ›´é«˜çš„å› å­
    :param df: æ•°æ®æ¡†
    :param factor_cols: å› å­åˆ—è¡¨
    :param factor_type: å› å­ç±»å‹ 'financial' æˆ– 'technical'
    :return: å»é‡åçš„å› å­åˆ—è¡¨
    """
    print_section(f"{factor_type.upper()}å› å­ç›¸å…³æ€§å»é‡")

    if factor_type == 'financial':
        corr_thresh = FIN_CORR_THRESHOLD
    else:
        corr_thresh = TECH_CORR_THRESHOLD

    # è®¡ç®—å› å­ç›¸å…³æ€§çŸ©é˜µ
    corr_matrix = df[factor_cols].corr().abs()

    # ç”Ÿæˆä¸Šä¸‰è§’çŸ©é˜µï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    # è®¡ç®—æ¯ä¸ªå› å­çš„å¹³å‡ICï¼ˆç”¨äºé€‰æ‹©ä¿ç•™å“ªä¸ªï¼‰
    factor_ic = {}
    for col in factor_cols:
        if col in df.columns and 'future_return' in df.columns:
            ic = df[[col, 'future_return']].corr(method='spearman').iloc[0, 1]
            factor_ic[col] = abs(ic)

    # éœ€è¦ç§»é™¤çš„å› å­åˆ—è¡¨
    to_drop = []

    # éå†ç›¸å…³æ€§çŸ©é˜µ
    for col in upper.columns:
        if col in to_drop:
            continue

        # æ‰¾åˆ°é«˜ç›¸å…³æ€§çš„å› å­
        high_corr = [idx for idx in upper.index if upper.loc[idx, col] > corr_thresh and idx not in to_drop]

        if high_corr:
            # åŒ…æ‹¬å½“å‰åˆ—
            candidates = [col] + high_corr

            # è·å–å€™é€‰å› å­çš„IC
            candidate_ics = {f: factor_ic.get(f, 0) for f in candidates}

            # æ‰¾åˆ°ICæœ€é«˜çš„å› å­
            best_factor = max(candidate_ics, key=candidate_ics.get)

            # ç§»é™¤å…¶ä»–å› å­
            for f in candidates:
                if f != best_factor:
                    to_drop.append(f)

    # æœ€ç»ˆä¿ç•™çš„å› å­
    final_factors = [f for f in factor_cols if f not in to_drop]

    print(f"é«˜ç›¸å…³æ€§å› å­æ•°é‡: {len(to_drop)}")
    print(f"å»é‡åå‰©ä½™å› å­æ•°é‡: {len(final_factors)}")

    if len(to_drop) > 0:
        print(f"\nç§»é™¤çš„é«˜ç›¸å…³æ€§å› å­: {to_drop}")
        print(f"ä¿ç•™çš„å› å­: {final_factors}")

    return final_factors


def process_factors_pipeline(df, financial_cols, technical_cols):
    """
    å› å­å¤„ç†å®Œæ•´æµç¨‹
    :param df: åŸå§‹æ•°æ®æ¡†
    :param financial_cols: è´¢åŠ¡å› å­åˆ—
    :param technical_cols: æŠ€æœ¯å› å­åˆ—
    :return: å¤„ç†åçš„æ•°æ®æ¡†ã€æœ€ç»ˆç­›é€‰çš„è´¢åŠ¡å› å­ã€æœ€ç»ˆç­›é€‰çš„æŠ€æœ¯å› å­
    """
    print_section("=== å› å­å¤„ç†å®Œæ•´æµç¨‹ ===")

    # 1. å»æå€¼
    df_processed = winsorize_factor(df, financial_cols + technical_cols)

    # 2. åˆ†åˆ«å¤„ç†è´¢åŠ¡å› å­å’ŒæŠ€æœ¯å› å­
    ## è´¢åŠ¡å› å­ï¼šå¸‚å€¼ä¸­æ€§åŒ– + æ ‡å‡†åŒ–  æŠ€æœ¯å› å­ï¼šä¸åšä¸­æ€§åŒ–
    if financial_cols:
        df_processed = market_cap_neutralization(df_processed, financial_cols)
    df_processed = standardize_factor(df_processed, financial_cols + technical_cols)

    # 3. è®¡ç®—ICæŒ‡æ ‡
    ic_df = calculate_ic_measures(df_processed, financial_cols + technical_cols)

    # 4. æŒ‰ICç­›é€‰å› å­
    filtered_financial = filter_factors_by_ic(ic_df[ic_df['factor'].isin(financial_cols)], 'financial')
    filtered_technical = filter_factors_by_ic(ic_df[ic_df['factor'].isin(technical_cols)], 'technical')

    # 5. ç›¸å…³æ€§å»é‡
    final_financial = remove_high_correlation_factors(df_processed, filtered_financial, 'financial')
    final_technical = remove_high_correlation_factors(df_processed, filtered_technical, 'technical')

    # åˆå¹¶æœ€ç»ˆå› å­åˆ—è¡¨
    final_factors = final_financial + final_technical

    print(f"\nå› å­å¤„ç†å®Œæˆ:")
    print(f"  è´¢åŠ¡å› å­æœ€ç»ˆä¿ç•™: {len(final_financial)} ä¸ª")
    print(f"  æŠ€æœ¯å› å­æœ€ç»ˆä¿ç•™: {len(final_technical)} ä¸ª")
    print(f"  æ€»å› å­æ•°é‡: {len(final_factors)} ä¸ª")

    if final_factors:
        print(f"  æœ€ç»ˆå› å­åˆ—è¡¨: {final_factors}")

    return df_processed, final_financial, final_technical, final_factors, ic_df


# ==================== ä¿®å¤ï¼šå°†å‡½æ•°ç§»å‡ºåµŒå¥— ====================
@timer_decorator
def calculate_technical_indicators(df):
    """ç²¾ç®€ç‰ˆæŠ€æœ¯æŒ‡æ ‡è®¡ç®— - åªç”Ÿæˆ5ä¸ªæ ¸å¿ƒå› å­"""
    print_section("ç²¾ç®€ç‰ˆæŠ€æœ¯æŒ‡æ ‡è®¡ç®—ï¼ˆ5ä¸ªæ ¸å¿ƒå› å­ï¼‰")

    if df.empty or 'close' not in df.columns:
        print("æ•°æ®ä¸ºç©ºæˆ–ç¼ºå°‘closeåˆ—")
        return df

    df_tech = df.copy()
    close_prices = df_tech['close']

    # éªŒè¯closeåˆ—çš„æ•°æ®ç±»å‹
    if not pd.api.types.is_numeric_dtype(df_tech['close']):
        print("âš ï¸ closeåˆ—ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œå°è¯•è½¬æ¢...")
        df_tech['close'] = pd.to_numeric(df_tech['close'], errors='coerce')

    # ç§»é™¤closeä¸­çš„æ— æ•ˆå€¼
    initial_len = len(df_tech)
    df_tech = df_tech.dropna(subset=['close'])
    if initial_len - len(df_tech) > 0:
        print(f"ç§»é™¤{initial_len - len(df_tech)}ä¸ªæ— æ•ˆçš„closeå€¼")

    # é‡æ–°è·å–close_prices
    close_prices = df_tech['close']

    try:
        # ==================== å› å­1: çŸ­æœŸä»·æ ¼åŠ¨é‡ï¼ˆ5æ—¥æ”¶ç›Šç‡ï¼‰ ====================
        print("è®¡ç®—å› å­1: 5æ—¥æ”¶ç›Šç‡...")
        shifted_5 = close_prices.shift(5)
        valid_5_mask = (shifted_5 > 0) & shifted_5.notna() & close_prices.notna()
        df_tech['price_change_5d'] = 0.0
        df_tech.loc[valid_5_mask, 'price_change_5d'] = (
                (close_prices[valid_5_mask] - shifted_5[valid_5_mask]) /
                shifted_5[valid_5_mask]
        )
        print("âœ“ å› å­1ç”Ÿæˆå®Œæˆ")

        # ==================== å› å­2: ä»·æ ¼ç›¸å¯¹20æ—¥å‡çº¿ä½ç½® ====================
        print("è®¡ç®—å› å­2: ä»·æ ¼ç›¸å¯¹20æ—¥å‡çº¿ä½ç½®...")
        # è®¡ç®—20æ—¥ç§»åŠ¨å¹³å‡
        window = 20
        ma_col = f'ma_{window}'
        df_tech[ma_col] = close_prices.rolling(
            window=window, min_periods=max(1, window // 2)
        ).mean()

        # ä»·æ ¼ç›¸å¯¹äºç§»åŠ¨å¹³å‡çš„ä½ç½®
        valid_ma_mask = df_tech[ma_col] > 0
        df_tech['price_vs_ma20'] = 0.0
        df_tech.loc[valid_ma_mask, 'price_vs_ma20'] = (
                close_prices[valid_ma_mask] / df_tech.loc[valid_ma_mask, ma_col] - 1
        )
        print("âœ“ å› å­2ç”Ÿæˆå®Œæˆ")

        # ==================== å› å­3: RSIï¼ˆ14æ—¥ï¼‰ ====================
        print("è®¡ç®—å› å­3: RSI(14)...")
        period = 14

        try:
            delta = close_prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()

            # é¿å…é™¤é›¶
            rs = gain / (loss.replace(0, np.nan).fillna(1e-10))
            df_tech['rsi_14'] = 100 - (100 / (1 + rs))
        except Exception as e:
            print(f"RSIè®¡ç®—å¤±è´¥: {e}")
            df_tech['rsi_14'] = 50.0  # é»˜è®¤å€¼

        print("âœ“ å› å­3ç”Ÿæˆå®Œæˆ")

        # ==================== å› å­4: 20æ—¥æ³¢åŠ¨ç‡ ====================
        print("è®¡ç®—å› å­4: 20æ—¥æ³¢åŠ¨ç‡...")

        # æ‰‹åŠ¨è®¡ç®—æ—¥æ”¶ç›Šç‡
        daily_returns = np.zeros(len(close_prices))
        for i in range(1, len(close_prices)):
            if close_prices.iloc[i - 1] > 0 and not np.isnan(close_prices.iloc[i - 1]) and not np.isnan(
                    close_prices.iloc[i]):
                daily_returns[i] = (close_prices.iloc[i] - close_prices.iloc[i - 1]) / close_prices.iloc[i - 1]
            else:
                daily_returns[i] = np.nan

        daily_returns_series = pd.Series(daily_returns, index=close_prices.index)

        # è®¡ç®—20æ—¥æ³¢åŠ¨ç‡
        window = 20
        df_tech['volatility_20d'] = daily_returns_series.rolling(
            window=window, min_periods=max(1, window // 2)
        ).std()

        print("âœ“ å› å­4ç”Ÿæˆå®Œæˆ")

        # ==================== å› å­5: æˆäº¤é‡æ¯”ç‡ï¼ˆå¦‚æœæœ‰æˆäº¤é‡ï¼‰ ====================
        if 'volume' in df_tech.columns:
            print("è®¡ç®—å› å­5: 5æ—¥æˆäº¤é‡æ¯”ç‡...")
            volume = df_tech['volume']

            # éªŒè¯æˆäº¤é‡æ•°æ®
            if not pd.api.types.is_numeric_dtype(volume):
                print("âš ï¸ volumeåˆ—ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œå°è¯•è½¬æ¢...")
                volume = pd.to_numeric(volume, errors='coerce')
                df_tech['volume'] = volume

            # 5æ—¥æˆäº¤é‡ç§»åŠ¨å¹³å‡
            window = 5
            vol_ma_col = f'volume_ma_{window}'
            df_tech[vol_ma_col] = volume.rolling(
                window=window, min_periods=max(1, window // 2)
            ).mean()

            # æˆäº¤é‡æ¯”ç‡
            valid_vol_mask = (df_tech[vol_ma_col] > 0) & volume.notna()
            df_tech['volume_ratio_5'] = 1.0
            df_tech.loc[valid_vol_mask, 'volume_ratio_5'] = (
                    volume[valid_vol_mask] / df_tech.loc[valid_vol_mask, vol_ma_col]
            )

            print("âœ“ å› å­5ç”Ÿæˆå®Œæˆ")
        else:
            print("âš ï¸ ç¼ºå°‘æˆäº¤é‡æ•°æ®ï¼Œä½¿ç”¨ä»·æ ¼å¼ºåº¦ä½œä¸ºæ›¿ä»£å› å­...")
            # å¦‚æœæœ‰high/lowæ•°æ®ï¼Œè®¡ç®—ä»·æ ¼å¼ºåº¦
            if all(col in df_tech.columns for col in ['high', 'low']):
                high = df_tech['high']
                low = df_tech['low']

                # éªŒè¯high, lowæ•°æ®
                for col in ['high', 'low']:
                    if not pd.api.types.is_numeric_dtype(df_tech[col]):
                        print(f"âš ï¸ {col}åˆ—ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œå°è¯•è½¬æ¢...")
                        df_tech[col] = pd.to_numeric(df_tech[col], errors='coerce')

                # å½“æ—¥ä»·æ ¼å¼ºåº¦
                range_mask = (high != low) & high.notna() & low.notna() & close_prices.notna()
                df_tech['price_strength'] = 0.5  # é»˜è®¤å€¼
                df_tech.loc[range_mask, 'price_strength'] = (
                        (close_prices[range_mask] - low[range_mask]) /
                        (high[range_mask] - low[range_mask])
                )

                # é‡å‘½åä¸ºvolume_ratio_5ä»¥ä¿æŒä¸€è‡´æ€§
                df_tech['volume_ratio_5'] = df_tech['price_strength']
                df_tech = df_tech.drop(columns=['price_strength'])
                print("âœ“ å› å­5ï¼ˆä»·æ ¼å¼ºåº¦ï¼‰ç”Ÿæˆå®Œæˆ")
            else:
                print("âš ï¸ ä¹Ÿç¼ºå°‘high/lowæ•°æ®ï¼Œä½¿ç”¨åŠ é€Ÿåº¦ä½œä¸ºæ›¿ä»£å› å­...")
                # è®¡ç®—ä»·æ ¼åŠ é€Ÿåº¦
                df_tech['price_velocity'] = close_prices.diff()
                df_tech['volume_ratio_5'] = df_tech['price_velocity'].diff()
                print("âœ“ å› å­5ï¼ˆä»·æ ¼åŠ é€Ÿåº¦ï¼‰ç”Ÿæˆå®Œæˆ")

        # ==================== æ•°æ®æ¸…ç†å’ŒéªŒè¯ ====================
        print("æ¸…ç†å’ŒéªŒè¯ç”Ÿæˆçš„æŠ€æœ¯å› å­...")

        # ç¡®ä¿æ²¡æœ‰æ— ç©·å¤§å€¼
        for col in ['price_change_5d', 'price_vs_ma20', 'rsi_14', 'volatility_20d', 'volume_ratio_5']:
            if col in df_tech.columns:
                inf_count = np.isinf(df_tech[col]).sum()
                if inf_count > 0:
                    print(f"æ¸…ç† {col} ä¸­çš„ {inf_count} ä¸ªinfå€¼...")
                    df_tech[col] = df_tech[col].replace([np.inf, -np.inf], np.nan)

        # éªŒè¯ç”Ÿæˆçš„æŠ€æœ¯ç‰¹å¾
        tech_cols = ['price_change_5d', 'price_vs_ma20', 'rsi_14', 'volatility_20d', 'volume_ratio_5']
        valid_tech_cols = []

        for col in tech_cols:
            if col in df_tech.columns:
                # æ£€æŸ¥éç©ºæ¯”ä¾‹å’Œå”¯ä¸€å€¼æ•°é‡
                non_na_ratio = df_tech[col].notna().mean()
                unique_vals = df_tech[col].nunique()
                if non_na_ratio > 0.3 and unique_vals > 1:
                    valid_tech_cols.append(col)
                else:
                    print(f"âš ï¸ ç‰¹å¾ {col} è´¨é‡è¾ƒä½: éç©ºæ¯”ä¾‹={non_na_ratio:.2%}, å”¯ä¸€å€¼æ•°={unique_vals}")

        print_section("æŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆç»Ÿè®¡")
        print(f"ç›®æ ‡ç”ŸæˆæŠ€æœ¯å› å­: 5ä¸ª")
        print(f"å®é™…ç”Ÿæˆæœ‰æ•ˆæŠ€æœ¯å› å­: {len(valid_tech_cols)}ä¸ª")

        if len(valid_tech_cols) > 0:
            print(f"ç”Ÿæˆçš„å› å­: {valid_tech_cols}")

            # æ·»åŠ ç®€è¦ç»Ÿè®¡ä¿¡æ¯
            print("\nå› å­ç®€è¦ç»Ÿè®¡:")
            for col in valid_tech_cols:
                if col in df_tech.columns:
                    mean_val = df_tech[col].mean()
                    std_val = df_tech[col].std()
                    non_na = df_tech[col].notna().sum()
                    print(f"  {col}: å‡å€¼={mean_val:.4f}, æ ‡å‡†å·®={std_val:.4f}, éç©ºæ•°={non_na}")

        return df_tech

    except Exception as e:
        print(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print("è¿”å›åŸå§‹æ•°æ®...")
        return df


# ==================== è¾…åŠ©å‡½æ•°ä¿æŒä¸å˜ ====================

@timer_decorator
def calculate_future_returns_and_labels(df, days=FUTURE_DAYS):
    """æœªæ¥æ”¶ç›Šç‡è®¡ç®— """
    print_section("æ”¶ç›Šç‡è®¡ç®—")

    if df.empty or 'close' not in df.columns:
        print("æ•°æ®ä¸ºç©ºæˆ–ç¼ºå°‘closeåˆ—")
        return df

    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    df_fixed = df.copy()
    df_fixed = df_fixed.sort_values(['stock_code', 'date'])

    print(f"ä½¿ç”¨æ”¶ç›Šç‡è®¡ç®—ï¼Œé¢„æœŸé—´éš”: {days}ä¸ªäº¤æ˜“æ—¥")
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df_fixed.shape}")
    print(f"è‚¡ç¥¨æ•°é‡: {df_fixed['stock_code'].nunique()}")

    # ==================== å…³é”®ä¿®å¤å¼€å§‹ ====================
    def safe_calculate_returns(group):
        """å®‰å…¨è®¡ç®—æ”¶ç›Šç‡ - é¿å…infå’Œé™¤é›¶é”™è¯¯"""
        group = group.sort_values('date')

        if len(group) < days + 1:
            group['future_return'] = np.nan
            return group

        close_prices = group['close'].values
        returns = np.full(len(group), np.nan)

        for i in range(len(group)):
            if i + days < len(group):
                current_price = close_prices[i]
                future_price = close_prices[i + days]

                # å…³é”®ä¿®å¤ï¼šä¸¥æ ¼çš„ä»·æ ¼æœ‰æ•ˆæ€§æ£€æŸ¥
                if (current_price > 0 and future_price > 0 and
                        not np.isnan(current_price) and not np.isnan(future_price) and
                        not np.isinf(current_price) and not np.isinf(future_price)):

                    # è®¡ç®—æ”¶ç›Šç‡
                    return_val = (future_price / current_price) - 1

                    # ä¿®å¤ï¼šé™åˆ¶æ”¶ç›Šç‡èŒƒå›´ï¼Œé¿å…æç«¯å€¼
                    if return_val < -0.9:  # é™åˆ¶æœ€å¤§äºæŸ90%
                        return_val = -0.9
                    elif return_val > 10.0:  # é™åˆ¶æœ€å¤§æ”¶ç›Š1000%
                        return_val = 10.0

                    # ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰é™å€¼
                    if np.isfinite(return_val):
                        returns[i] = return_val

        group['future_return'] = returns
        return group

    # åº”ç”¨ä¿®å¤è®¡ç®—
    print("åº”ç”¨åˆ†ç»„è®¡ç®—æ”¶ç›Šç‡...")
    df_fixed = df_fixed.groupby('stock_code', group_keys=False).apply(safe_calculate_returns)

    # ç§»é™¤æ— æ•ˆè¡Œ
    initial_size = len(df_fixed)
    df_fixed = df_fixed.dropna(subset=['future_return'])
    removed_count = initial_size - len(df_fixed)
    print(f"æ”¶ç›Šç‡è®¡ç®—å®Œæˆï¼Œç§»é™¤æ— æ•ˆæ•°æ®: {removed_count:,}è¡Œ")

    # ä¿®å¤ï¼šé¢å¤–æ£€æŸ¥å¹¶å¤„ç†æ— ç©·å€¼
    if 'future_return' in df_fixed.columns:
        future_returns = df_fixed['future_return']

        # æ£€æŸ¥æ— ç©·å¤§å€¼
        inf_mask = np.isinf(future_returns)
        inf_count = inf_mask.sum()

        if inf_count > 0:
            print(f"å‘ç°æ— ç©·å¤§æ”¶ç›Šç‡: {inf_count}ä¸ªï¼Œå°†å…¶è®¾ç½®ä¸ºNaN")
            df_fixed.loc[inf_mask, 'future_return'] = np.nan

        # æ£€æŸ¥NaNå€¼
        nan_count = future_returns.isna().sum()
        if nan_count > 0:
            print(f"ç§»é™¤NaNæ”¶ç›Šç‡: {nan_count}ä¸ª")
            df_fixed = df_fixed.dropna(subset=['future_return'])

    # éªŒè¯ä¿®å¤ç»“æœ
    if 'future_return' in df_fixed.columns and len(df_fixed) > 0:
        future_returns = df_fixed['future_return'].dropna()

        if len(future_returns) > 0:
            print(f"å½»åº•ä¿®å¤åæ”¶ç›Šç‡ç»Ÿè®¡:")
            print(f"æœ‰æ•ˆæ ·æœ¬: {len(future_returns):,}")
            print(f"èŒƒå›´: {future_returns.min():.6f} åˆ° {future_returns.max():.6f}")
            print(f"å‡å€¼: {future_returns.mean():.6f}")

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ— æ•ˆå€¼
            if np.isinf(future_returns).any() or np.isnan(future_returns).any():
                print("ä»ç„¶å­˜åœ¨æ— æ•ˆæ”¶ç›Šç‡ï¼Œè¿›è¡Œç´§æ€¥å¤„ç†...")
                median_return = future_returns.replace([np.inf, -np.inf], np.nan).median()
                df_fixed['future_return'] = df_fixed['future_return'].replace(
                    [np.inf, -np.inf], median_return
                )
        else:
            print("ä¿®å¤åæ²¡æœ‰æœ‰æ•ˆæ”¶ç›Šç‡ï¼")

    # ==================== æ ‡ç­¾è®¡ç®—éƒ¨åˆ† ====================
    print("è®¡ç®—å¸‚åœºå¹³å‡æ”¶ç›Šç‡å’Œæ ‡ç­¾...")

    # è®¡ç®—å¸‚åœºå¹³å‡æ”¶ç›Šç‡
    daily_avg_return = df_fixed.groupby('date')['future_return'].mean().reset_index()
    daily_avg_return.columns = ['date', 'market_avg_return']
    df_fixed = pd.merge(df_fixed, daily_avg_return, on='date', how='left')

    # ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•å®šä¹‰æ ‡ç­¾ï¼ˆæ›´ç¨³å¥ï¼‰
    def calculate_smart_labels(group):
        if len(group) < 10:
            group['label'] = 0
            return group

        future_returns = group['future_return']

        # æ–¹æ³•1ï¼šä½¿ç”¨åˆ†ä½æ•°
        try:
            quantile_threshold = future_returns.quantile(0.6)  # å‰40%ä¸ºæ­£æ ·æœ¬
            group['label'] = (future_returns > quantile_threshold).astype(int)
        except:
            # å›é€€æ–¹æ³•ï¼šä½¿ç”¨å¸‚åœºå¹³å‡
            market_avg = group['market_avg_return'].mean()
            group['label'] = (future_returns > market_avg).astype(int)

        return group

    df_fixed = df_fixed.groupby('date', group_keys=False).apply(calculate_smart_labels)

    # éªŒè¯æ ‡ç­¾æœ‰æ•ˆæ€§
    print("éªŒè¯æ ‡ç­¾æœ‰æ•ˆæ€§...")
    if 'label' in df_fixed.columns and 'future_return' in df_fixed.columns:
        positive_mask = df_fixed['label'] == 1
        negative_mask = df_fixed['label'] == 0

        if positive_mask.any() and negative_mask.any():
            positive_return = df_fixed[positive_mask]['future_return'].mean()
            negative_return = df_fixed[negative_mask]['future_return'].mean()
            return_diff = positive_return - negative_return

            print("âœ… æ ‡ç­¾æœ‰æ•ˆæ€§éªŒè¯:")
            print(f"  æ­£æ ·æœ¬å¹³å‡æ”¶ç›Š: {positive_return:.6f} ({positive_return:.4%})")
            print(f"  è´Ÿæ ·æœ¬å¹³å‡æ”¶ç›Š: {negative_return:.6f} ({negative_return:.4%})")
            print(f"  æ”¶ç›Šå·®å¼‚: {return_diff:.6f} ({return_diff:.4%})")
            print(f"  æ­£æ ·æœ¬æ¯”ä¾‹: {df_fixed['label'].mean():.2%}")

            if return_diff < 0.01:
                print("âŒ æ ‡ç­¾åŒºåˆ†åº¦ä¸è¶³ï¼Œå°è¯•è°ƒæ•´...")
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„åˆ†ä½æ•°
                try:
                    df_fixed = df_fixed.groupby('date', group_keys=False).apply(
                        lambda x: x.assign(label=(x['future_return'] > x['future_return'].quantile(0.7)).astype(int))
                    )
                    # é‡æ–°éªŒè¯
                    positive_return = df_fixed[df_fixed['label'] == 1]['future_return'].mean()
                    negative_return = df_fixed[df_fixed['label'] == 0]['future_return'].mean()
                    return_diff = positive_return - negative_return
                    print(f"è°ƒæ•´åæ”¶ç›Šå·®å¼‚: {return_diff:.4f} ({return_diff:.2%})")
                except Exception as e:
                    print(f"è°ƒæ•´å¤±è´¥: {e}")
        else:
            print("âŒ æ— æ³•éªŒè¯æ ‡ç­¾æœ‰æ•ˆæ€§ï¼šç¼ºå°‘æ­£æ ·æœ¬æˆ–è´Ÿæ ·æœ¬")

    print(f"æ ‡ç­¾è®¡ç®—å®Œæˆ! æ­£æ ·æœ¬æ¯”ä¾‹: {df_fixed['label'].mean():.2%}")
    return df_fixed


def filter_financial_features_by_importance(df, financial_features, target_count):
    """ç­›é€‰è´¢åŠ¡ç‰¹å¾"""
    if len(financial_features) <= target_count:
        return financial_features

    print(f"ç­›é€‰è´¢åŠ¡ç‰¹å¾: {len(financial_features)} -> {target_count}ä¸ª")

    financial_features_filtered = []

    # æ–¹æ³•1ï¼šä½¿ç”¨ä¸labelçš„ç›¸å…³æ€§è¿›è¡Œç­›é€‰
    if 'label' in df.columns:
        financial_correlations = []
        for col in financial_features:
            try:
                if df[col].notna().sum() > 100:
                    corr = abs(df[col].corr(df['label']))
                    if not np.isnan(corr):
                        financial_correlations.append((col, corr))
            except:
                continue

        if financial_correlations:
            financial_correlations.sort(key=lambda x: x[1], reverse=True)
            selected_financial = [col for col, corr in financial_correlations[:target_count]]
            financial_features_filtered = selected_financial
            print(f"åŸºäºç›¸å…³æ€§ç­›é€‰: {len(selected_financial)}ä¸ªè´¢åŠ¡ç‰¹å¾")
        else:
            financial_features_filtered = financial_features[:target_count]
            print(f"ä½¿ç”¨ç®€å•æˆªå–: {len(financial_features_filtered)}ä¸ªè´¢åŠ¡ç‰¹å¾")
    else:
        # å¦‚æœæ²¡æœ‰labelï¼Œä½¿ç”¨æ–¹å·®ç­›é€‰
        financial_variances = []
        for col in financial_features:
            try:
                variance = df[col].var()
                if not np.isnan(variance):
                    financial_variances.append((col, variance))
            except:
                continue

        if financial_variances:
            financial_variances.sort(key=lambda x: x[1], reverse=True)
            financial_features_filtered = [col for col, var in financial_variances[:target_count]]
            print(f"åŸºäºæ–¹å·®ç­›é€‰: {len(financial_features_filtered)}ä¸ªè´¢åŠ¡ç‰¹å¾")
        else:
            financial_features_filtered = financial_features[:target_count]
            print(f"ä½¿ç”¨ç®€å•æˆªå–: {len(financial_features_filtered)}ä¸ªè´¢åŠ¡ç‰¹å¾")

    return financial_features_filtered


@timer_decorator
def create_features(df):
    """ç‰¹å¾å·¥ç¨‹ - ç¡®ä¿æŠ€æœ¯ç‰¹å¾å’Œè´¢åŠ¡ç‰¹å¾å¹³è¡¡"""
    print_section("ç‰¹å¾å¹³è¡¡ä¼˜åŒ–")

    if df.empty:
        return df, []

    # åŸºç¡€åˆ—ï¼ˆä¸åŒ…å«åœ¨ç‰¹å¾ä¸­ï¼‰
    base_cols = ['date', 'stock_code', 'close', 'volume', 'open', 'high', 'low',
                 'future_return', 'market_avg_return', 'label']

    # 1. æ”¶é›†æ‰€æœ‰æ•°å€¼å‹ç‰¹å¾
    all_numeric_cols = []
    for col in df.columns:
        if (col not in base_cols and
                pd.api.types.is_numeric_dtype(df[col]) and
                df[col].nunique() > 1 and
                df[col].notna().mean() > 0.3):  # é™ä½éç©ºé˜ˆå€¼åˆ°30%
            all_numeric_cols.append(col)

    print(f"æ‰€æœ‰æ•°å€¼å‹ç‰¹å¾: {len(all_numeric_cols)}ä¸ª")

    if len(all_numeric_cols) == 0:
        print("æ²¡æœ‰æ‰¾åˆ°æ•°å€¼å‹ç‰¹å¾")
        return df, []

    # 2. é‡æ–°å®šä¹‰ç‰¹å¾åˆ†ç±»æ¨¡å¼ - æ›´å…¨é¢çš„åŒ¹é…
    tech_patterns = [
        'ma_', 'ema_', 'volatility_', 'momentum_', 'rsi_', 'macd_', 'bb_', 'atr_', 'obv_',
        'volume_ratio_', 'price_vs_', 'return_', 'log_return', 'price_change', 'change_',
        'breakout_', 'strength_', 'position_', 'ratio_', 'signal_', 'index_', 'oscillator_'
    ]

    # 3. åˆ†ç±»ç‰¹å¾
    tech_features = []
    financial_features = []
    other_features = []

    for col in all_numeric_cols:
        # ä¼˜å…ˆè¯†åˆ«è´¢åŠ¡ç‰¹å¾
        if any(col.startswith(pattern) for pattern in ['fin_', 'financial_']):
            financial_features.append(col)
        # è¯†åˆ«æŠ€æœ¯ç‰¹å¾
        elif any(pattern in col for pattern in tech_patterns):
            tech_features.append(col)
        # è¯†åˆ«å…¶ä»–è´¢åŠ¡ç‰¹å¾ï¼ˆåŸºäºå…³é”®è¯ï¼‰
        elif any(keyword in col.lower() for keyword in
                 ['cash', 'asset', 'liability', 'equity', 'revenue', 'profit',
                  'margin', 'debt', 'flow', 'eps', 'roe', 'roa']):
            financial_features.append(col)
        else:
            other_features.append(col)

    print(f"åˆå§‹ç‰¹å¾ç»Ÿè®¡:")
    print(f"æŠ€æœ¯ç‰¹å¾: {len(tech_features)}ä¸ª")
    print(f"è´¢åŠ¡ç‰¹å¾: {len(financial_features)}ä¸ª")
    print(f"å…¶ä»–ç‰¹å¾: {len(other_features)}ä¸ª")

    # 4. ç›®æ ‡å¹³è¡¡æ¯”ä¾‹
    target_tech = 25  # æŠ€æœ¯ç‰¹å¾ç›®æ ‡
    target_fin = 35  # è´¢åŠ¡ç‰¹å¾ç›®æ ‡

    # 6. ç®€åŒ–å¹³è¡¡ç­–ç•¥
    print("æ‰§è¡Œç®€åŒ–å¹³è¡¡ç­–ç•¥...")

    # 6.1 å¦‚æœæŠ€æœ¯ç‰¹å¾ä»ç„¶ä¸è¶³ï¼Œä»å…¶ä»–ç‰¹å¾ä¸­å€Ÿç”¨
    if len(tech_features) < target_tech and len(other_features) > 0:
        print(f"æŠ€æœ¯ç‰¹å¾ä»ä¸è¶³({len(tech_features)}ä¸ª)ï¼Œä»å…¶ä»–ç‰¹å¾ä¸­å€Ÿç”¨...")

        # è®¡ç®—å…¶ä»–ç‰¹å¾ä¸æ ‡ç­¾çš„ç›¸å…³æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        correlations = []
        if 'label' in df.columns:
            for col in other_features:
                try:
                    if df[col].notna().sum() > 50:  # é™ä½æ ·æœ¬æ•°é‡è¦æ±‚
                        corr = abs(df[col].corr(df['label']))
                        if not np.isnan(corr):
                            correlations.append((col, corr))
                except:
                    continue

            if correlations:
                correlations.sort(key=lambda x: x[1], reverse=True)
                # å€Ÿç”¨ç›¸å…³æ€§æœ€é«˜çš„ç‰¹å¾
                borrow_count = min(target_tech - len(tech_features), len(correlations), 10)  # æœ€å¤šå€Ÿ10ä¸ª
                borrowed_features = [col for col, corr in correlations[:borrow_count]]
                tech_features.extend(borrowed_features)
                # ä»å…¶ä»–ç‰¹å¾ä¸­ç§»é™¤
                other_features = [col for col in other_features if col not in borrowed_features]
                print(f"  å€Ÿç”¨ {len(borrowed_features)} ä¸ªé«˜ç›¸å…³æ€§ç‰¹å¾ç»™æŠ€æœ¯ç‰¹å¾")

    # 6.2 å¦‚æœè´¢åŠ¡ç‰¹å¾è¿‡å¤šï¼Œè¿›è¡Œç­›é€‰
    if len(financial_features) > target_fin:
        print(f"è´¢åŠ¡ç‰¹å¾è¿‡å¤š({len(financial_features)}ä¸ª)ï¼Œè¿›è¡Œç­›é€‰...")

        # ä½¿ç”¨ç›¸å…³æ€§ç­›é€‰
        fin_correlations = []
        if 'label' in df.columns:
            for col in financial_features:
                try:
                    if df[col].notna().sum() > 50:  # é™ä½æ ·æœ¬æ•°é‡è¦æ±‚
                        corr = abs(df[col].corr(df['label']))
                        if not np.isnan(corr):
                            fin_correlations.append((col, corr))
                except:
                    continue

            if fin_correlations:
                fin_correlations.sort(key=lambda x: x[1], reverse=True)
                financial_features = [col for col, corr in fin_correlations[:target_fin]]
                print(f"  åŸºäºç›¸å…³æ€§ç­›é€‰åˆ° {len(financial_features)} ä¸ªè´¢åŠ¡ç‰¹å¾")
            else:
                # ä½¿ç”¨æ–¹å·®ç­›é€‰
                variances = []
                for col in financial_features:
                    try:
                        variance = df[col].var()
                        if not np.isnan(variance):
                            variances.append((col, variance))
                    except:
                        continue

                if variances:
                    variances.sort(key=lambda x: x[1], reverse=True)
                    financial_features = [col for col, var in variances[:target_fin]]
                    print(f"  åŸºäºæ–¹å·®ç­›é€‰åˆ° {len(financial_features)} ä¸ªè´¢åŠ¡ç‰¹å¾")
                else:
                    # ç®€å•æˆªå–
                    financial_features = financial_features[:target_fin]
                    print(f"  ç®€å•æˆªå–åˆ° {len(financial_features)} ä¸ªè´¢åŠ¡ç‰¹å¾")

    # 6.3 æœ€ç»ˆç‰¹å¾åˆå¹¶
    selected_features = tech_features + financial_features

    # ç¡®ä¿ç‰¹å¾æ•°é‡åœ¨åˆç†èŒƒå›´å†…
    total_target = target_tech + target_fin
    if len(selected_features) > total_target * 1.5:
        print(f"ç‰¹å¾æ•°é‡è¶…é¢({len(selected_features)}ä¸ª)ï¼Œè¿›è¡Œæœ€ç»ˆç²¾ç®€...")
        # ä¼˜å…ˆä¿ç•™æŠ€æœ¯ç‰¹å¾
        tech_keep = min(len(tech_features), int(total_target * 0.4))
        fin_keep = min(len(financial_features), total_target - tech_keep)
        selected_features = tech_features[:tech_keep] + financial_features[:fin_keep]
        print(f"ç²¾ç®€åˆ°: {len(selected_features)}ä¸ªç‰¹å¾")

    # 7. æœ€ç»ˆç»Ÿè®¡
    tech_selected = [col for col in selected_features if col in tech_features]
    fin_selected = [col for col in selected_features if col in financial_features]
    other_selected = [col for col in selected_features if col in other_features]

    print(f"ç‰¹å¾å¹³è¡¡å®Œæˆ!")
    print(f"æœ€ç»ˆæŠ€æœ¯ç‰¹å¾: {len(tech_selected)}ä¸ª")
    print(f"æœ€ç»ˆè´¢åŠ¡ç‰¹å¾: {len(fin_selected)}ä¸ª")
    print(f"å…¶ä»–ç‰¹å¾: {len(other_selected)}ä¸ª")
    print(f"å¹³è¡¡æ¯”ä¾‹: {len(tech_selected)}:{len(fin_selected)} (ç›®æ ‡: {target_tech}:{target_fin})")
    print(f"æ€»ç‰¹å¾æ•°é‡: {len(selected_features)}ä¸ª")

    # æ˜¾ç¤ºç‰¹å¾ç¤ºä¾‹
    if len(selected_features) > 0:
        print(f"æŠ€æœ¯ç‰¹å¾ç¤ºä¾‹: {tech_selected[:5] if tech_selected else 'æ— '}")
        print(f"è´¢åŠ¡ç‰¹å¾ç¤ºä¾‹: {fin_selected[:5] if fin_selected else 'æ— '}")

    return df, selected_features


@timer_decorator
def prepare_modeling_data(df, feature_cols):
    """å‡†å¤‡å»ºæ¨¡æ•°æ®"""
    print_section("å‡†å¤‡å»ºæ¨¡æ•°æ®")

    if df.empty or len(feature_cols) == 0:
        print("æ•°æ®ä¸ºç©ºæˆ–æ— ç‰¹å¾")
        return pd.DataFrame()

    # åŸºç¡€åˆ—
    base_cols = ['date', 'stock_code', 'close', 'volume', 'future_return', 'market_avg_return', 'label']

    # æ·»åŠ open, high, lowå¦‚æœå­˜åœ¨
    for col in ['open', 'high', 'low', 'spread', 'turnover_rate', 'change', 'amount']:
        if col in df.columns and col not in base_cols:
            base_cols.append(col)

    # åˆå¹¶æ‰€æœ‰éœ€è¦çš„åˆ—
    all_cols = base_cols + feature_cols
    all_cols = [col for col in all_cols if col in df.columns]

    modeling_df = df[all_cols].copy()

    # å¤„ç†ç¼ºå¤±å€¼
    print(f"å¤„ç†å‰æ•°æ®å½¢çŠ¶: {modeling_df.shape}")

    # ç§»é™¤æ ‡ç­¾ç¼ºå¤±çš„è¡Œ
    initial_size = len(modeling_df)
    modeling_df = modeling_df.dropna(subset=['future_return', 'market_avg_return', 'label'])
    print(f"ç§»é™¤æ ‡ç­¾ç¼ºå¤±è¡Œ: {initial_size - len(modeling_df):,} è¡Œ")

    # å¤„ç†ç‰¹å¾ç¼ºå¤±å€¼
    for col in feature_cols:
        if col in modeling_df.columns:
            modeling_df[col] = modeling_df[col].fillna(modeling_df[col].median())

    # å¤„ç†æ— ç©·å€¼
    numeric_cols = modeling_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col in modeling_df.columns:
            # æ›¿æ¢inf/-infä¸ºNaNï¼Œå†ç”¨ä¸­ä½æ•°å¡«å……
            modeling_df[col] = modeling_df[col].replace([np.inf, -np.inf], np.nan)
            modeling_df[col] = modeling_df[col].fillna(modeling_df[col].median())

    # ç§»é™¤ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ
    modeling_df = modeling_df.dropna()
    print(f"å¤„ç†åæ•°æ®å½¢çŠ¶: {modeling_df.shape}")

    print(f"å»ºæ¨¡æ•°æ®å‡†å¤‡å®Œæˆ!")
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"æ­£æ ·æœ¬æ¯”ä¾‹: {modeling_df['label'].mean():.2%}")
    print(f"æ—¶é—´èŒƒå›´: {modeling_df['date'].min()} åˆ° {modeling_df['date'].max()}")
    print(f"è‚¡ç¥¨æ•°é‡: {modeling_df['stock_code'].nunique()}")

    return modeling_df


@timer_decorator
def split_train_val_test_data(df, feature_cols, test_ratio=0.2, val_ratio=0.1):
    """æ—¶é—´åºåˆ—æ•°æ®é›†åˆ’åˆ† - ä¿®æ”¹ä¸ºæ»šåŠ¨äº¤å‰éªŒè¯"""
    print_section("æ•°æ®é›†åˆ’åˆ†ï¼ˆæ»šåŠ¨äº¤å‰éªŒè¯ï¼‰")

    if df.empty or len(feature_cols) == 0:
        print("æ•°æ®ä¸ºç©ºæˆ–æ— ç‰¹å¾")
        return None, None, None, None, None, None, None, None, None

    # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values('date')

    if USE_ROLLING_CV:
        print("ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯æ¨¡å¼")
        # è·å–å”¯ä¸€æ—¥æœŸå¹¶æ’åº
        dates = np.sort(df['date'].unique())
        n_dates = len(dates)

        # è®¡ç®—åˆ†å‰²ç‚¹ - åˆ’åˆ†æµ‹è¯•é›†
        test_start_idx = int(n_dates * (1 - test_ratio))

        # æµ‹è¯•é›†
        test_dates = dates[test_start_idx:]
        test_df = df[df['date'].isin(test_dates)]

        # è®­ç»ƒ+éªŒè¯é›†
        train_val_dates = dates[:test_start_idx]
        train_val_df = df[df['date'].isin(train_val_dates)]

        # ä»è®­ç»ƒéªŒè¯é›†ä¸­å†åˆ’åˆ†éªŒè¯é›†ï¼ˆç”¨äºæ—©åœç­‰ï¼‰
        train_val_dates_sorted = np.sort(train_val_df['date'].unique())
        n_train_val_dates = len(train_val_dates_sorted)
        val_start_idx = int(n_train_val_dates * (1 - val_ratio))

        # éªŒè¯é›†æ—¥æœŸ
        val_dates = train_val_dates_sorted[val_start_idx:]
        val_df = train_val_df[train_val_df['date'].isin(val_dates)]

        # è®­ç»ƒé›†æ—¥æœŸ
        train_dates = train_val_dates_sorted[:val_start_idx]
        train_df = train_val_df[train_val_df['date'].isin(train_dates)]

        print(f"è®­ç»ƒé›†: {train_df['date'].min().date()} åˆ° {train_df['date'].max().date()}, å¤§å°: {len(train_df):,}")
        print(f"éªŒè¯é›†: {val_df['date'].min().date()} åˆ° {val_df['date'].max().date()}, å¤§å°: {len(val_df):,}")
        print(f"æµ‹è¯•é›†: {test_df['date'].min().date()} åˆ° {test_df['date'].max().date()}, å¤§å°: {len(test_df):,}")
    else:
        # åŸæ¥çš„é™æ€åˆ’åˆ†é€»è¾‘
        dates = np.sort(df['date'].unique())
        n_dates = len(dates)

        train_end_idx = int(n_dates * (1 - test_ratio - val_ratio))
        val_end_idx = int(n_dates * (1 - test_ratio))

        train_dates = dates[:train_end_idx]
        val_dates = dates[train_end_idx:val_end_idx]
        test_dates = dates[val_end_idx:]

        train_df = df[df['date'].isin(train_dates)]
        val_df = df[df['date'].isin(val_dates)]
        test_df = df[df['date'].isin(test_dates)]

        print(f"è®­ç»ƒé›†: {train_df['date'].min().date()} åˆ° {train_df['date'].max().date()}, å¤§å°: {len(train_df):,}")
        print(f"éªŒè¯é›†: {val_df['date'].min().date()} åˆ° {val_df['date'].max().date()}, å¤§å°: {len(val_df):,}")
        print(f"æµ‹è¯•é›†: {test_df['date'].min().date()} åˆ° {test_df['date'].max().date()}, å¤§å°: {len(test_df):,}")

    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    X_train = train_df[feature_cols]
    X_val = val_df[feature_cols]
    X_test = test_df[feature_cols]

    y_train = train_df['label']
    y_val = val_df['label']
    y_test = test_df['label']

    print(f"ç‰¹å¾å½¢çŠ¶: X_train{X_train.shape}, X_val{X_val.shape}, X_test{X_test.shape}")

    return X_train, X_val, X_test, y_train, y_val, y_test, train_df, val_df, test_df


@timer_decorator
def hyperparameter_tuning(X_train, y_train, X_val, y_val, n_trials=5):
    """éªŒè¯é›†è¶…å‚æ•°è°ƒä¼˜"""
    # å¿«é€Ÿæ¨¡å¼ï¼šå‡å°‘è°ƒä¼˜æ¬¡æ•°
    if QUICK_MODE:
        n_trials = HYPERPARAM_TRIALS
        print_section(f"å¿«é€Ÿè¶…å‚æ•°è°ƒä¼˜ (n_trials={n_trials})")
    else:
        print_section("éªŒè¯é›†è¶…å‚æ•°è°ƒä¼˜")

    # å¦‚æœæ•°æ®é‡å¤§ï¼Œè¿›è¡Œé‡‡æ ·ä»¥åŠ é€Ÿè°ƒä¼˜
    if len(X_train) > SAMPLE_SIZE_TUNING:
        from sklearn.model_selection import train_test_split
        X_train_sample, _, y_train_sample, _ = train_test_split(
            X_train, y_train,
            train_size=SAMPLE_SIZE_TUNING,
            stratify=y_train,  # ä¿æŒæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
            random_state=RANDOM_STATE
        )

    best_params = {}

    # 1. éšæœºæ£®æ—è°ƒå‚ï¼ˆç®€åŒ–å‚æ•°ç½‘æ ¼ï¼‰
    print("1. éšæœºæ£®æ—è¶…å‚æ•°è°ƒä¼˜...")
    rf_param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [10, 15],
        'min_samples_split': [5, 10],
        'min_samples_leaf': [2, 5],
        'max_features': ['sqrt']
    }

    rf_model = RandomForestClassifier(
        random_state=RANDOM_STATE,
        n_jobs=N_JOBS
    )

    # ä½¿ç”¨éšæœºæœç´¢
    if USE_ROLLING_CV:
        # ä½¿ç”¨æ»šåŠ¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        from sklearn.model_selection import TimeSeriesSplit
        tscv = TimeSeriesSplit(n_splits=ROLLING_CV_SPLITS)
        cv_method = tscv
    else:
        cv_method = 2  # åŸæ¥çš„2æŠ˜äº¤å‰éªŒè¯

    rf_search = RandomizedSearchCV(
        rf_model,
        rf_param_grid,
        n_iter=n_trials,
        cv=cv_method,  # ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯
        scoring='f1',
        n_jobs=-1,
        verbose=1,
        random_state=RANDOM_STATE
    )

    rf_search.fit(X_train_sample, y_train_sample)
    best_params['rf'] = rf_search.best_params_
    print(f" æœ€ä½³å‚æ•°: {rf_search.best_params_}")
    print(f" æœ€ä½³éªŒè¯åˆ†æ•°: {rf_search.best_score_:.4f}")

    # 2. XGBoostè°ƒå‚ï¼ˆç®€åŒ–å‚æ•°ç½‘æ ¼ï¼‰- ä¿®å¤ï¼šè½¬æ¢ä¸ºnumpyæ•°ç»„
    print("\n2. XGBoostè¶…å‚æ•°è°ƒä¼˜...")

    if hasattr(X_train, 'values'):
        X_train_sample = X_train.values
    else:
        X_train_sample = X_train

    if hasattr(y_train, 'values'):
        y_train_sample = y_train.values
    else:
        y_train_sample = y_train

    # ä¿®å¤ï¼šè½¬æ¢æ•°æ®ç±»å‹
    X_train_sample = X_train_sample.astype(np.float32)
    y_train_sample = y_train_sample.astype(np.int32)

    xgb_param_grid = {
        'n_estimators': [50, 100, 150],
        'max_depth': [3, 4, 5],
        'learning_rate': [0.05, 0.1, 0.15],
        'subsample': [0.6, 0.7, 0.8],
        'colsample_bytree': [0.6, 0.7, 0.8]
    }

    # ä¿®å¤ï¼šä½¿ç”¨æ›´å…¼å®¹çš„XGBoostå‚æ•°
    xgb_model = xgb.XGBClassifier(
        random_state=RANDOM_STATE,
        n_jobs=1,  # é¿å…å¹¶è¡Œé—®é¢˜
        use_label_encoder=False,
        eval_metric='logloss',
        verbosity=0  # å‡å°‘è¾“å‡º
    )

    try:
        # ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if USE_ROLLING_CV:
            # ä½¿ç”¨æ»šåŠ¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            from sklearn.model_selection import TimeSeriesSplit
            tscv = TimeSeriesSplit(n_splits=ROLLING_CV_SPLITS)
            cv_method = tscv
        else:
            cv_method = 2  # åŸæ¥çš„2æŠ˜äº¤å‰éªŒè¯

        xgb_search = RandomizedSearchCV(
            xgb_model, xgb_param_grid,
            n_iter=n_trials, cv=cv_method, scoring='f1', n_jobs=1,  # æ”¹ä¸ºcv=cv_method
            verbose=1, random_state=RANDOM_STATE, error_score='raise'
        )
        xgb_search.fit(X_train_sample, y_train_sample)  # ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
        best_params['xgb'] = xgb_search.best_params_
        print(f"   æœ€ä½³å‚æ•°: {xgb_search.best_params_}")
        print(f"   æœ€ä½³éªŒè¯åˆ†æ•°: {xgb_search.best_score_:.4f}")

    except Exception as e:
        print(f"  XGBoostè°ƒä¼˜å¤±è´¥: {e}")
        print("   ä½¿ç”¨é»˜è®¤XGBoostå‚æ•°")
        best_params['xgb'] = {
            'n_estimators': 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8
        }

    return best_params


def train_models(X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, best_params=None):
    """ä¿®å¤ç‰ˆæ¨¡å‹è®­ç»ƒ - è§£å†³XGBoost dtypeé”™è¯¯å’Œæ”¶ç›Šç‡infé—®é¢˜"""
    print_section("ä¿®å¤ç‰ˆæ¨¡å‹è®­ç»ƒ")

    # å¼ºåˆ¶ä½¿ç”¨æ»šåŠ¨CVï¼ˆå¦‚æœå¯ç”¨ï¼‰
    global USE_ROLLING_CV  # æ·»åŠ è¿™ä¸€è¡Œå£°æ˜å…¨å±€å˜é‡
    if ENFORCE_ROLLING_CV_FOR_ALL_MODELS:
        USE_ROLLING_CV = True
        print("å¼ºåˆ¶æ‰§è¡Œæ»šåŠ¨äº¤å‰éªŒè¯æ¨¡å¼...")

    # ==================== 1. æ•°æ®éªŒè¯å’Œç‰¹å¾æ•°é‡æ£€æŸ¥ ====================
    print("æ•°æ®éªŒè¯å’Œç‰¹å¾æ•°é‡æ£€æŸ¥...")

    if X_train.empty or X_val.empty or X_test.empty:
        print("è¾“å…¥æ•°æ®ä¸ºç©º")
        return {}, None, {}, {}, {}

    # éªŒè¯ç‰¹å¾æ•°é‡ä¸€è‡´æ€§
    print(f"ç‰¹å¾æ•°é‡éªŒè¯:")
    print(f"  ç‰¹å¾åˆ—è¡¨: {len(feature_cols)} ä¸ªç‰¹å¾")
    print(f"  X_train å½¢çŠ¶: {X_train.shape} -> {X_train.shape[1]} ä¸ªç‰¹å¾")
    print(f"  X_val å½¢çŠ¶: {X_val.shape} -> {X_val.shape[1]} ä¸ªç‰¹å¾")
    print(f"  X_test å½¢çŠ¶: {X_test.shape} -> {X_test.shape[1]} ä¸ªç‰¹å¾")

    # æ£€æŸ¥ç‰¹å¾æ•°é‡æ˜¯å¦åŒ¹é…
    if len(feature_cols) != X_train.shape[1]:
        print(f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…: ç‰¹å¾åˆ—è¡¨{len(feature_cols)} vs è®­ç»ƒæ•°æ®{X_train.shape[1]}")
        if hasattr(X_train, 'columns'):
            actual_features = list(X_train.columns)
            print(f"  ä½¿ç”¨å®é™…ç‰¹å¾åç§°: {len(actual_features)} ä¸ª")
            feature_cols = actual_features
        else:
            feature_cols = [f'feature_{i}' for i in range(X_train.shape[1])]
            print(f"  åˆ›å»ºæ–°ç‰¹å¾åç§°: {len(feature_cols)} ä¸ª")

    # ==================== 2. æ ‡å‡†åŒ–ç‰¹å¾ ====================
    print("ç‰¹å¾æ ‡å‡†åŒ–...")
    scaler = StandardScaler()

    try:
        # ç¡®ä¿æ•°æ®æ˜¯numpyæ•°ç»„æ ¼å¼
        X_train_array = X_train.values if hasattr(X_train, 'values') else X_train
        X_val_array = X_val.values if hasattr(X_val, 'values') else X_val
        X_test_array = X_test.values if hasattr(X_test, 'values') else X_test

        X_train_scaled = scaler.fit_transform(X_train_array)
        X_val_scaled = scaler.transform(X_val_array)
        X_test_scaled = scaler.transform(X_test_array)

        print(f"ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
        print(f"æ ‡å‡†åŒ–åå½¢çŠ¶: X_train{X_train_scaled.shape}, X_val{X_val_scaled.shape}, X_test{X_test_scaled.shape}")
    except Exception as e:
        print(f"ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥: {e}")
        X_train_scaled = X_train_array
        X_val_scaled = X_val_array
        X_test_scaled = X_test_array
        print("ä½¿ç”¨æœªæ ‡å‡†åŒ–æ•°æ®ç»§ç»­è®­ç»ƒ")

    # ==================== 3. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ ====================
    print("å¤„ç†ç±»åˆ«ä¸å¹³è¡¡...")
    try:
        smote = SMOTE(random_state=RANDOM_STATE, sampling_strategy=0.8)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)
        print(f"å¹³è¡¡åè®­ç»ƒé›†: {X_train_balanced.shape}")

        # ç¡®ä¿y_train_balancedæ˜¯æ­£ç¡®æ ¼å¼
        if hasattr(y_train_balanced, 'values'):
            y_train_balanced = y_train_balanced.values
        elif hasattr(y_train_balanced, 'to_numpy'):
            y_train_balanced = y_train_balanced.to_numpy()

        # ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        if hasattr(X_train_balanced, 'dtype') and X_train_balanced.dtype != np.float32:
            X_train_balanced = X_train_balanced.astype(np.float32)
        if hasattr(y_train_balanced, 'dtype') and y_train_balanced.dtype != np.int32:
            y_train_balanced = y_train_balanced.astype(np.int32)

    except Exception as e:
        print(f"SMOTEå¤„ç†å¤±è´¥: {e}")
        print("ä½¿ç”¨åŸå§‹ä¸å¹³è¡¡æ•°æ®")
        X_train_balanced, y_train_balanced = X_train_scaled, y_train
        if hasattr(y_train_balanced, 'values'):
            y_train_balanced = y_train_balanced.values
        elif hasattr(y_train_balanced, 'to_numpy'):
            y_train_balanced = y_train_balanced.to_numpy()

    # ==================== 4. åˆå§‹åŒ–ç»“æœå­—å…¸ ====================
    models = {}
    results = {}
    predictions = {}
    probabilities = {}

    # ==================== 5. æ¨¡å‹é»˜è®¤å‚æ•° ====================
    if best_params is None:
        print("ä½¿ç”¨ä¿å®ˆæ¨¡å‹å‚æ•°é˜²æ­¢è¿‡æ‹Ÿåˆ...")
        best_params = {
            'rf': {
                'n_estimators': 100,
                'max_depth': 10,
                'min_samples_split': 10,
                'min_samples_leaf': 4,
                'max_features': 'sqrt',
                'random_state': RANDOM_STATE,
                'n_jobs': N_JOBS
            },
            'xgb': {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': RANDOM_STATE,
                'n_jobs': 1,
                'use_label_encoder': False,
                'eval_metric': 'logloss'
            }
        }

    # ==================== 6. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ï¼ˆç»Ÿä¸€ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯ï¼‰ ====================
    print("\n1. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
    try:
        rf_params = best_params.get('rf', {})

        # ç¡®ä¿rf_paramsåŒ…å«å¿…è¦çš„å‚æ•°
        default_rf_params = {
            'n_estimators': 100,
            'max_depth': 10,
            'min_samples_split': 10,
            'min_samples_leaf': 4,
            'max_features': 'sqrt',
            'random_state': RANDOM_STATE,
            'n_jobs': 1,  # æ¯æŠ˜æ¨¡å‹ä½¿ç”¨1ä¸ªæ ¸ï¼Œé¿å…å†…å­˜é—®é¢˜
            'class_weight': 'balanced'  # æ·»åŠ ç±»åˆ«å¹³è¡¡
        }

        # æ›´æ–°é»˜è®¤å‚æ•°
        for key, value in default_rf_params.items():
            if key not in rf_params:
                rf_params[key] = value

        print("éšæœºæ£®æ—å‚æ•°:")
        for key, value in rf_params.items():
            if key in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features', 'n_jobs']:
                print(f"   {key}: {value}")

        # å§‹ç»ˆä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if USE_ROLLING_CV:
            print("ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯è®­ç»ƒéšæœºæ£®æ—...")
            from sklearn.model_selection import TimeSeriesSplit
            tscv = TimeSeriesSplit(n_splits=ROLLING_CV_SPLITS)

            # å­˜å‚¨æ¯æŠ˜çš„æ¨¡å‹å’Œåˆ†æ•°
            rf_models = []
            fold_scores = []

            for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled), 1):
                print(f"  è®­ç»ƒæŠ˜ {fold}/{ROLLING_CV_SPLITS}...")

                X_fold_train = X_train_scaled[train_idx]
                y_fold_train = y_train.iloc[train_idx] if hasattr(y_train, 'iloc') else y_train[train_idx]

                # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆå¯¹æ¯ä¸€æŠ˜å•ç‹¬åšSMOTEï¼‰ - ä¿®å¤ç‚¹2ï¼šæ·»åŠ ç±»åˆ«æ£€æŸ¥
                # å…ˆæ£€æŸ¥ç±»åˆ«æ•°é‡
                unique_classes = np.unique(y_fold_train)
                if len(unique_classes) < 2:
                    print(f"    æŠ˜ {fold}: è®­ç»ƒé›†åªæœ‰{len(unique_classes)}ä¸ªç±»åˆ«ï¼Œè·³è¿‡SMOTE")
                    X_fold_train_bal, y_fold_train_bal = X_fold_train, y_fold_train
                else:
                    try:
                        smote_fold = SMOTE(random_state=RANDOM_STATE + fold, sampling_strategy=0.8)
                        X_fold_train_bal, y_fold_train_bal = smote_fold.fit_resample(X_fold_train, y_fold_train)
                    except Exception as e:
                        print(f"    æŠ˜ {fold} SMOTEå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                        X_fold_train_bal, y_fold_train_bal = X_fold_train, y_fold_train

                # è®­ç»ƒå½“å‰æŠ˜çš„æ¨¡å‹
                fold_model = RandomForestClassifier(**rf_params)
                fold_model.fit(X_fold_train_bal, y_fold_train_bal)
                rf_models.append(fold_model)

                # åœ¨éªŒè¯æŠ˜ä¸Šè¯„ä¼°
                X_fold_val = X_train_scaled[val_idx]
                y_fold_val = y_train.iloc[val_idx] if hasattr(y_train, 'iloc') else y_train[val_idx]
                y_fold_val_pred = fold_model.predict(X_fold_val)
                fold_score = f1_score(y_fold_val, y_fold_val_pred, zero_division=0)
                fold_scores.append(fold_score)

                print(f"    æŠ˜ {fold}: éªŒè¯é›†F1 = {fold_score:.4f}")

            # ä½¿ç”¨æ‰€æœ‰æŠ˜çš„å¹³å‡æ¨¡å‹ï¼ˆé€šè¿‡å¹³å‡é¢„æµ‹æ¦‚ç‡ï¼‰
            print(f"æ»šåŠ¨äº¤å‰éªŒè¯å¹³å‡F1: {np.mean(fold_scores):.4f}")

            rf_model = EnsembleRF(rf_models)
        else:
            # å¦‚æœä¸ä½¿ç”¨æ»šåŠ¨CVï¼Œä½¿ç”¨åŸæ¥çš„è®­ç»ƒé€»è¾‘
            print("ä½¿ç”¨æ™®é€šè®­ç»ƒæ¨¡å¼...")
            rf_model = RandomForestClassifier(**rf_params)
            rf_model.fit(X_train_balanced, y_train_balanced)

        models['rf'] = rf_model
        print("éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒå®Œæˆ")

    except Exception as e:
        print(f"éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        # è®¾ç½®é»˜è®¤ç»“æœ
        y_test_array = y_test.values if hasattr(y_test, 'values') else y_test
        results['rf'] = {
            'val_accuracy': 0.5, 'val_precision': 0.5, 'val_recall': 0.5, 'val_f1': 0.5, 'val_roc_auc': 0.5,
            'test_accuracy': 0.5, 'test_precision': 0.5, 'test_recall': 0.5, 'test_f1': 0.5, 'test_roc_auc': 0.5
        }
        predictions['rf'] = np.zeros(len(y_test_array))
        probabilities['rf'] = np.ones(len(y_test_array)) * 0.5
        models['rf'] = None
        return {}, None, {}, {}, {}  # å¦‚æœéšæœºæ£®æ—è®­ç»ƒå¤±è´¥ï¼Œç›´æ¥è¿”å›

    # åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šè¯„ä¼°
    y_val_pred_rf = rf_model.predict(X_val_scaled)
    y_val_proba_rf = rf_model.predict_proba(X_val_scaled)[:, 1]
    y_test_pred_rf = rf_model.predict(X_test_scaled)
    y_test_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]

    # ç¡®ä¿y_trueæ˜¯numpyæ•°ç»„æ ¼å¼
    y_val_array = y_val.values if hasattr(y_val, 'values') else y_val
    y_test_array = y_test.values if hasattr(y_test, 'values') else y_test

    # ä¿®å¤ç‚¹3ï¼šåœ¨è®¡ç®—æŒ‡æ ‡å‰æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦æœ‰ç‰¹æ®Šå€¼
    # å¤„ç†éªŒè¯é›†é¢„æµ‹
    if isinstance(y_val_pred_rf, (int, float)):
        # å¦‚æœy_val_pred_rfæ˜¯å•ä¸ªå€¼ï¼ˆæ¯”å¦‚é”™è¯¯å€¼ï¼‰ï¼Œè½¬æ¢ä¸ºæ•°ç»„
        y_val_pred_rf_fixed = np.full(len(y_val_array), int(y_val_pred_rf))
    else:
        y_val_pred_rf_fixed = np.array(y_val_pred_rf)

    # ä¿®å¤ï¼šæ›¿æ¢ç‰¹æ®Šå€¼
    if np.any(y_val_pred_rf_fixed == -2147483648):
        print("è­¦å‘Šï¼šéªŒè¯é›†å‘ç°ç‰¹æ®Šå€¼-2147483648ï¼Œæ›¿æ¢ä¸º0")
        y_val_pred_rf_fixed = np.where(y_val_pred_rf_fixed == -2147483648, 0, y_val_pred_rf_fixed)

    # å¤„ç†æµ‹è¯•é›†é¢„æµ‹
    if isinstance(y_test_pred_rf, (int, float)):
        y_test_pred_rf_fixed = np.full(len(y_test_array), int(y_test_pred_rf))
    else:
        y_test_pred_rf_fixed = np.array(y_test_pred_rf)

    if np.any(y_test_pred_rf_fixed == -2147483648):
        print("è­¦å‘Šï¼šæµ‹è¯•é›†å‘ç°ç‰¹æ®Šå€¼-2147483648ï¼Œæ›¿æ¢ä¸º0")
        y_test_pred_rf_fixed = np.where(y_test_pred_rf_fixed == -2147483648, 0, y_test_pred_rf_fixed)

    results['rf'] = {
        'val_accuracy': accuracy_score(y_val_array, y_val_pred_rf_fixed),
        'val_precision': precision_score(y_val_array, y_val_pred_rf_fixed, zero_division=0),
        'val_recall': recall_score(y_val_array, y_val_pred_rf_fixed, zero_division=0),
        'val_f1': f1_score(y_val_array, y_val_pred_rf_fixed, zero_division=0),
        'val_roc_auc': roc_auc_score(y_val_array, y_val_proba_rf),
        'test_accuracy': accuracy_score(y_test_array, y_test_pred_rf_fixed),
        'test_precision': precision_score(y_test_array, y_test_pred_rf_fixed, zero_division=0),
        'test_recall': recall_score(y_test_array, y_test_pred_rf_fixed, zero_division=0),
        'test_f1': f1_score(y_test_array, y_test_pred_rf_fixed, zero_division=0),
        'test_roc_auc': roc_auc_score(y_test_array, y_test_proba_rf)
    }

    predictions['rf'] = y_test_pred_rf
    probabilities['rf'] = y_test_proba_rf

    print("éšæœºæ£®æ—æ¨¡å‹éªŒè¯é›†ç»“æœ:")
    print(f"  å‡†ç¡®ç‡: {results['rf']['val_accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {results['rf']['val_precision']:.4f}")
    print(f"  å¬å›ç‡: {results['rf']['val_recall']:.4f}")
    print(f"  F1åˆ†æ•°: {results['rf']['val_f1']:.4f}")
    print(f"  ROC-AUC: {results['rf']['val_roc_auc']:.4f}")

    print("éšæœºæ£®æ—æ¨¡å‹æµ‹è¯•é›†ç»“æœ:")
    print(f"  å‡†ç¡®ç‡: {results['rf']['test_accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {results['rf']['test_precision']:.4f}")
    print(f"  å¬å›ç‡: {results['rf']['test_recall']:.4f}")
    print(f"  F1åˆ†æ•°: {results['rf']['test_f1']:.4f}")
    print(f"  ROC-AUC: {results['rf']['test_roc_auc']:.4f}")

    # ==================== 7. è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆå…³é”®ä¿®å¤éƒ¨åˆ†ï¼‰ ====================
    print("\n2. è®­ç»ƒXGBoostæ¨¡å‹...")
    try:
        xgb_params = best_params.get('xgb', {})

        if not xgb_params:
            # è®¡ç®—æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
            scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1]) if len(
                y_train[y_train == 1]) > 0 else 1
            xgb_params = {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_alpha': 0.1,
                'reg_lambda': 0.1,
                'scale_pos_weight': scale_pos_weight,
                'random_state': RANDOM_STATE,
                'n_jobs': 1,
                'use_label_encoder': False,
                'eval_metric': 'logloss',
                'verbosity': 0
            }

        print(f"ä½¿ç”¨XGBoostå‚æ•°:")
        for key, value in xgb_params.items():
            if key in ['n_estimators', 'max_depth', 'learning_rate', 'subsample',
                       'colsample_bytree', 'scale_pos_weight']:
                print(f"   {key}: {value}")

        # å…³é”®ä¿®å¤ï¼šXGBoostæ•°æ®æ ¼å¼å…¼å®¹æ€§
        print("å‡†å¤‡XGBoostè®­ç»ƒæ•°æ®...")

        def safe_convert_to_float32(data):
            """å®‰å…¨è½¬æ¢ä¸ºfloat32ï¼Œå…¼å®¹DataFrameå’Œnumpyæ•°ç»„"""
            if hasattr(data, 'values'):
                # å¦‚æœæ˜¯DataFrameæˆ–Seriesï¼Œè·å–values
                array_data = data.values
            else:
                array_data = data

            # ç¡®ä¿æ˜¯numpyæ•°ç»„
            if not isinstance(array_data, np.ndarray):
                array_data = np.array(array_data)

            # æ£€æŸ¥æ•°æ®ç±»å‹å¹¶å®‰å…¨è½¬æ¢
            try:
                if hasattr(array_data, 'dtype'):
                    if array_data.dtype != np.float32:
                        return array_data.astype(np.float32)
                return array_data
            except Exception as e:
                print(f"æ•°æ®ç±»å‹è½¬æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæ•°æ®ç±»å‹")
                return array_data

        # åº”ç”¨å®‰å…¨è½¬æ¢
        X_train_balanced_float32 = safe_convert_to_float32(X_train_balanced)
        y_train_balanced_int32 = y_train_balanced.astype(np.int32) if hasattr(y_train_balanced, 'astype') else np.array(
            y_train_balanced, dtype=np.int32)
        X_val_float32 = safe_convert_to_float32(X_val_scaled)
        X_test_float32 = safe_convert_to_float32(X_test_scaled)

        print(f"æ•°æ®æ ¼å¼æ£€æŸ¥:")
        print(f"  X_train_balanced: {type(X_train_balanced_float32)}")
        if hasattr(X_train_balanced_float32, 'dtype'):
            print(f"    dtype: {X_train_balanced_float32.dtype}")
        print(f"  y_train_balanced: {type(y_train_balanced_int32)}")
        if hasattr(y_train_balanced_int32, 'dtype'):
            print(f"    dtype: {y_train_balanced_int32.dtype}")
        print(f"  X_val: {type(X_val_float32)}")
        if hasattr(X_val_float32, 'dtype'):
            print(f"    dtype: {X_val_float32.dtype}")
        print(f"  X_test: {type(X_test_float32)}")
        if hasattr(X_test_float32, 'dtype'):
            print(f"    dtype: {X_test_float32.dtype}")

        # åˆ›å»ºXGBoostæ¨¡å‹
        xgb_model = xgb.XGBClassifier(**xgb_params)

        # è®­ç»ƒæ¨¡å‹ - ç»Ÿä¸€ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯
        print("è®­ç»ƒXGBoostæ¨¡å‹...")

        if USE_ROLLING_CV:
            print("ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯è®­ç»ƒXGBoost...")
            from sklearn.model_selection import TimeSeriesSplit

            tscv = TimeSeriesSplit(n_splits=ROLLING_CV_SPLITS)

            # å­˜å‚¨æ¯æŠ˜çš„æ¨¡å‹å’Œåˆ†æ•°
            xgb_models = []
            fold_scores = []

            for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled), 1):
                print(f"  è®­ç»ƒæŠ˜ {fold}/{ROLLING_CV_SPLITS}...")

                X_fold_train = X_train_scaled[train_idx]
                y_fold_train = y_train.iloc[train_idx] if hasattr(y_train, 'iloc') else y_train[train_idx]

                # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆå¯¹æ¯ä¸€æŠ˜å•ç‹¬åšSMOTEï¼‰ - ä¿®å¤ç‚¹2ï¼šæ·»åŠ ç±»åˆ«æ£€æŸ¥
                # å…ˆæ£€æŸ¥ç±»åˆ«æ•°é‡
                unique_classes = np.unique(y_fold_train)
                if len(unique_classes) < 2:
                    print(f"    æŠ˜ {fold}: è®­ç»ƒé›†åªæœ‰{len(unique_classes)}ä¸ªç±»åˆ«ï¼Œè·³è¿‡SMOTE")
                    X_fold_train_bal, y_fold_train_bal = X_fold_train, y_fold_train
                else:
                    try:
                        smote_fold = SMOTE(random_state=RANDOM_STATE + fold, sampling_strategy=0.8)
                        X_fold_train_bal, y_fold_train_bal = smote_fold.fit_resample(X_fold_train, y_fold_train)
                    except:
                        X_fold_train_bal, y_fold_train_bal = X_fold_train, y_fold_train

                # è½¬æ¢æ•°æ®ç±»å‹
                X_fold_train_float32 = safe_convert_to_float32(X_fold_train_bal)
                y_fold_train_int32 = y_fold_train_bal.astype(np.int32) if hasattr(y_fold_train_bal,
                                                                                  'astype') else np.array(
                    y_fold_train_bal, dtype=np.int32)

                # è®­ç»ƒå½“å‰æŠ˜çš„æ¨¡å‹
                fold_model = xgb.XGBClassifier(**xgb_params)
                fold_model.fit(X_fold_train_float32, y_fold_train_int32)
                xgb_models.append(fold_model)

                # åœ¨éªŒè¯æŠ˜ä¸Šè¯„ä¼°
                X_fold_val = X_train_scaled[val_idx]
                y_fold_val = y_train.iloc[val_idx] if hasattr(y_train, 'iloc') else y_train[val_idx]
                X_fold_val_float32 = safe_convert_to_float32(X_fold_val)
                y_fold_val_pred = fold_model.predict(X_fold_val_float32)
                fold_score = f1_score(y_fold_val, y_fold_val_pred, zero_division=0)
                fold_scores.append(fold_score)

                print(f"    æŠ˜ {fold}: éªŒè¯é›†F1 = {fold_score:.4f}")

            # ä½¿ç”¨æ‰€æœ‰æŠ˜çš„å¹³å‡æ¨¡å‹ï¼ˆé€šè¿‡å¹³å‡é¢„æµ‹æ¦‚ç‡ï¼‰
            print(f"æ»šåŠ¨äº¤å‰éªŒè¯å¹³å‡F1: {np.mean(fold_scores):.4f}")

            rf_model = EnsembleRF(rf_models)
        else:
            # å¦‚æœä¸ä½¿ç”¨æ»šåŠ¨CVï¼Œä½¿ç”¨åŸæ¥çš„è®­ç»ƒé€»è¾‘
            print("ä½¿ç”¨æ™®é€šè®­ç»ƒæ¨¡å¼...")
            rf_model = RandomForestClassifier(**rf_params)
            rf_model.fit(X_train_balanced, y_train_balanced)

        models['rf'] = rf_model
        print("éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒå®Œæˆ")

    except Exception as e:
        print(f"éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        # è®¾ç½®é»˜è®¤ç»“æœ
        y_test_array = y_test.values if hasattr(y_test, 'values') else y_test
        results['rf'] = {
            'val_accuracy': 0.5, 'val_precision': 0.5, 'val_recall': 0.5, 'val_f1': 0.5, 'val_roc_auc': 0.5,
            'test_accuracy': 0.5, 'test_precision': 0.5, 'test_recall': 0.5, 'test_f1': 0.5, 'test_roc_auc': 0.5
        }
        predictions['rf'] = np.zeros(len(y_test_array))
        probabilities['rf'] = np.ones(len(y_test_array)) * 0.5
        models['rf'] = None
        return {}, None, {}, {}, {}  # å¦‚æœéšæœºæ£®æ—è®­ç»ƒå¤±è´¥ï¼Œç›´æ¥è¿”å›

    # åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šè¯„ä¼°
    y_val_pred_rf = rf_model.predict(X_val_scaled)
    y_val_proba_rf = rf_model.predict_proba(X_val_scaled)[:, 1]
    y_test_pred_rf = rf_model.predict(X_test_scaled)
    y_test_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]

    # ç¡®ä¿y_trueæ˜¯numpyæ•°ç»„æ ¼å¼
    y_val_array = y_val.values if hasattr(y_val, 'values') else y_val
    y_test_array = y_test.values if hasattr(y_test, 'values') else y_test

    # ä¿®å¤ç‚¹3ï¼šåœ¨è®¡ç®—æŒ‡æ ‡å‰æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦æœ‰ç‰¹æ®Šå€¼
    # å¤„ç†éªŒè¯é›†é¢„æµ‹
    if isinstance(y_val_pred_rf, (int, float)):
        # å¦‚æœy_val_pred_rfæ˜¯å•ä¸ªå€¼ï¼ˆæ¯”å¦‚é”™è¯¯å€¼ï¼‰ï¼Œè½¬æ¢ä¸ºæ•°ç»„
        y_val_pred_rf_fixed = np.full(len(y_val_array), int(y_val_pred_rf))
    else:
        y_val_pred_rf_fixed = np.array(y_val_pred_rf)

    # ä¿®å¤ï¼šæ›¿æ¢ç‰¹æ®Šå€¼
    if np.any(y_val_pred_rf_fixed == -2147483648):
        print("è­¦å‘Šï¼šéªŒè¯é›†å‘ç°ç‰¹æ®Šå€¼-2147483648ï¼Œæ›¿æ¢ä¸º0")
        y_val_pred_rf_fixed = np.where(y_val_pred_rf_fixed == -2147483648, 0, y_val_pred_rf_fixed)

    # å¤„ç†æµ‹è¯•é›†é¢„æµ‹
    if isinstance(y_test_pred_rf, (int, float)):
        y_test_pred_rf_fixed = np.full(len(y_test_array), int(y_test_pred_rf))
    else:
        y_test_pred_rf_fixed = np.array(y_test_pred_rf)

    if np.any(y_test_pred_rf_fixed == -2147483648):
        print("è­¦å‘Šï¼šæµ‹è¯•é›†å‘ç°ç‰¹æ®Šå€¼-2147483648ï¼Œæ›¿æ¢ä¸º0")
        y_test_pred_rf_fixed = np.where(y_test_pred_rf_fixed == -2147483648, 0, y_test_pred_rf_fixed)

    results['rf'] = {
        'val_accuracy': accuracy_score(y_val_array, y_val_pred_rf_fixed),
        'val_precision': precision_score(y_val_array, y_val_pred_rf_fixed, zero_division=0),
        'val_recall': recall_score(y_val_array, y_val_pred_rf_fixed, zero_division=0),
        'val_f1': f1_score(y_val_array, y_val_pred_rf_fixed, zero_division=0),
        'val_roc_auc': roc_auc_score(y_val_array, y_val_proba_rf),
        'test_accuracy': accuracy_score(y_test_array, y_test_pred_rf_fixed),
        'test_precision': precision_score(y_test_array, y_test_pred_rf_fixed, zero_division=0),
        'test_recall': recall_score(y_test_array, y_test_pred_rf_fixed, zero_division=0),
        'test_f1': f1_score(y_test_array, y_test_pred_rf_fixed, zero_division=0),
        'test_roc_auc': roc_auc_score(y_test_array, y_test_proba_rf)
    }

    predictions['rf'] = y_test_pred_rf
    probabilities['rf'] = y_test_proba_rf

    print("éšæœºæ£®æ—æ¨¡å‹éªŒè¯é›†ç»“æœ:")
    print(f"  å‡†ç¡®ç‡: {results['rf']['val_accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {results['rf']['val_precision']:.4f}")
    print(f"  å¬å›ç‡: {results['rf']['val_recall']:.4f}")
    print(f"  F1åˆ†æ•°: {results['rf']['val_f1']:.4f}")
    print(f"  ROC-AUC: {results['rf']['val_roc_auc']:.4f}")

    print("éšæœºæ£®æ—æ¨¡å‹æµ‹è¯•é›†ç»“æœ:")
    print(f"  å‡†ç¡®ç‡: {results['rf']['test_accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {results['rf']['test_precision']:.4f}")
    print(f"  å¬å›ç‡: {results['rf']['test_recall']:.4f}")
    print(f"  F1åˆ†æ•°: {results['rf']['test_f1']:.4f}")
    print(f"  ROC-AUC: {results['rf']['test_roc_auc']:.4f}")

    # ==================== 7. è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆå…³é”®ä¿®å¤éƒ¨åˆ†ï¼‰ ====================
    print("\n2. è®­ç»ƒXGBoostæ¨¡å‹...")
    try:
        xgb_params = best_params.get('xgb', {})

        if not xgb_params:
            # è®¡ç®—æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
            scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1]) if len(
                y_train[y_train == 1]) > 0 else 1
            xgb_params = {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_alpha': 0.1,
                'reg_lambda': 0.1,
                'scale_pos_weight': scale_pos_weight,
                'random_state': RANDOM_STATE,
                'n_jobs': 1,
                'use_label_encoder': False,
                'eval_metric': 'logloss',
                'verbosity': 0
            }

        print(f"ä½¿ç”¨XGBoostå‚æ•°:")
        for key, value in xgb_params.items():
            if key in ['n_estimators', 'max_depth', 'learning_rate', 'subsample',
                       'colsample_bytree', 'scale_pos_weight']:
                print(f"   {key}: {value}")

        # å…³é”®ä¿®å¤ï¼šXGBoostæ•°æ®æ ¼å¼å…¼å®¹æ€§
        print("å‡†å¤‡XGBoostè®­ç»ƒæ•°æ®...")

        def safe_convert_to_float32(data):
            """å®‰å…¨è½¬æ¢ä¸ºfloat32ï¼Œå…¼å®¹DataFrameå’Œnumpyæ•°ç»„"""
            if hasattr(data, 'values'):
                # å¦‚æœæ˜¯DataFrameæˆ–Seriesï¼Œè·å–values
                array_data = data.values
            else:
                array_data = data

            # ç¡®ä¿æ˜¯numpyæ•°ç»„
            if not isinstance(array_data, np.ndarray):
                array_data = np.array(array_data)

            # æ£€æŸ¥æ•°æ®ç±»å‹å¹¶å®‰å…¨è½¬æ¢
            try:
                if hasattr(array_data, 'dtype'):
                    if array_data.dtype != np.float32:
                        return array_data.astype(np.float32)
                return array_data
            except Exception as e:
                print(f"æ•°æ®ç±»å‹è½¬æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæ•°æ®ç±»å‹")
                return array_data

        # åº”ç”¨å®‰å…¨è½¬æ¢
        X_train_balanced_float32 = safe_convert_to_float32(X_train_balanced)
        y_train_balanced_int32 = y_train_balanced.astype(np.int32) if hasattr(y_train_balanced, 'astype') else np.array(
            y_train_balanced, dtype=np.int32)
        X_val_float32 = safe_convert_to_float32(X_val_scaled)
        X_test_float32 = safe_convert_to_float32(X_test_scaled)

        print(f"æ•°æ®æ ¼å¼æ£€æŸ¥:")
        print(f"  X_train_balanced: {type(X_train_balanced_float32)}")
        if hasattr(X_train_balanced_float32, 'dtype'):
            print(f"    dtype: {X_train_balanced_float32.dtype}")
        print(f"  y_train_balanced: {type(y_train_balanced_int32)}")
        if hasattr(y_train_balanced_int32, 'dtype'):
            print(f"    dtype: {y_train_balanced_int32.dtype}")
        print(f"  X_val: {type(X_val_float32)}")
        if hasattr(X_val_float32, 'dtype'):
            print(f"    dtype: {X_val_float32.dtype}")
        print(f"  X_test: {type(X_test_float32)}")
        if hasattr(X_test_float32, 'dtype'):
            print(f"    dtype: {X_test_float32.dtype}")

        # åˆ›å»ºXGBoostæ¨¡å‹
        xgb_model = xgb.XGBClassifier(**xgb_params)

        # è®­ç»ƒæ¨¡å‹ - ç»Ÿä¸€ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯
        print("è®­ç»ƒXGBoostæ¨¡å‹...")

        if USE_ROLLING_CV:
            print("ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯è®­ç»ƒXGBoost...")
            from sklearn.model_selection import TimeSeriesSplit

            tscv = TimeSeriesSplit(n_splits=ROLLING_CV_SPLITS)

            # å­˜å‚¨æ¯æŠ˜çš„æ¨¡å‹å’Œåˆ†æ•°
            xgb_models = []
            fold_scores = []

            for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled), 1):
                print(f"  è®­ç»ƒæŠ˜ {fold}/{ROLLING_CV_SPLITS}...")

                X_fold_train = X_train_scaled[train_idx]
                y_fold_train = y_train.iloc[train_idx] if hasattr(y_train, 'iloc') else y_train[train_idx]

                # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆå¯¹æ¯ä¸€æŠ˜å•ç‹¬åšSMOTEï¼‰ - ä¿®å¤ç‚¹2ï¼šæ·»åŠ ç±»åˆ«æ£€æŸ¥
                # å…ˆæ£€æŸ¥ç±»åˆ«æ•°é‡
                unique_classes = np.unique(y_fold_train)
                if len(unique_classes) < 2:
                    print(f"    æŠ˜ {fold}: è®­ç»ƒé›†åªæœ‰{len(unique_classes)}ä¸ªç±»åˆ«ï¼Œè·³è¿‡SMOTE")
                    X_fold_train_bal, y_fold_train_bal = X_fold_train, y_fold_train
                else:
                    try:
                        smote_fold = SMOTE(random_state=RANDOM_STATE + fold, sampling_strategy=0.8)
                        X_fold_train_bal, y_fold_train_bal = smote_fold.fit_resample(X_fold_train, y_fold_train)
                    except:
                        X_fold_train_bal, y_fold_train_bal = X_fold_train, y_fold_train

                # è½¬æ¢æ•°æ®ç±»å‹
                X_fold_train_float32 = safe_convert_to_float32(X_fold_train_bal)
                y_fold_train_int32 = y_fold_train_bal.astype(np.int32) if hasattr(y_fold_train_bal,
                                                                                  'astype') else np.array(
                    y_fold_train_bal, dtype=np.int32)

                # è®­ç»ƒå½“å‰æŠ˜çš„æ¨¡å‹
                fold_model = xgb.XGBClassifier(**xgb_params)
                fold_model.fit(X_fold_train_float32, y_fold_train_int32)
                xgb_models.append(fold_model)

                # åœ¨éªŒè¯æŠ˜ä¸Šè¯„ä¼°
                X_fold_val = X_train_scaled[val_idx]
                y_fold_val = y_train.iloc[val_idx] if hasattr(y_train, 'iloc') else y_train[val_idx]
                X_fold_val_float32 = safe_convert_to_float32(X_fold_val)
                y_fold_val_pred = fold_model.predict(X_fold_val_float32)
                fold_score = f1_score(y_fold_val, y_fold_val_pred, zero_division=0)
                fold_scores.append(fold_score)

                print(f"    æŠ˜ {fold}: éªŒè¯é›†F1 = {fold_score:.4f}")

            # ä½¿ç”¨æ‰€æœ‰æŠ˜çš„å¹³å‡æ¨¡å‹ï¼ˆé€šè¿‡å¹³å‡é¢„æµ‹æ¦‚ç‡ï¼‰
            print(f"æ»šåŠ¨äº¤å‰éªŒè¯å¹³å‡F1: {np.mean(fold_scores):.4f}")

            # ä½¿ç”¨å…¨å±€å®šä¹‰çš„EnsembleXGBç±»
            xgb_model = EnsembleXGB(xgb_models)
        else:
            # åŸæ¥çš„è®­ç»ƒé€»è¾‘
            print("ä½¿ç”¨æ™®é€šè®­ç»ƒæ¨¡å¼...")
            xgb_model.fit(X_train_balanced_float32, y_train_balanced_int32)

        models['xgb'] = xgb_model
        print(" XGBoostæ¨¡å‹è®­ç»ƒå®Œæˆ")

        # åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šè¯„ä¼°
        y_val_pred_xgb = xgb_model.predict(X_val_float32)
        y_val_proba_xgb = xgb_model.predict_proba(X_val_float32)[:, 1]
        y_test_pred_xgb = xgb_model.predict(X_test_float32)
        y_test_proba_xgb = xgb_model.predict_proba(X_test_float32)[:, 1]

        # ç¡®ä¿y_trueæ ¼å¼æ­£ç¡®
        y_val_array = y_val.values if hasattr(y_val, 'values') else y_val
        y_test_array = y_test.values if hasattr(y_test, 'values') else y_test

        # ä¿®å¤ç‚¹3ï¼šåœ¨è®¡ç®—æŒ‡æ ‡å‰æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦æœ‰ç‰¹æ®Šå€¼
        # å¤„ç†éªŒè¯é›†é¢„æµ‹
        if isinstance(y_val_pred_xgb, (int, float)):
            y_val_pred_xgb_fixed = np.full(len(y_val_array), int(y_val_pred_xgb))
        else:
            y_val_pred_xgb_fixed = np.array(y_val_pred_xgb)

        if np.any(y_val_pred_xgb_fixed == -2147483648):
            print("è­¦å‘Šï¼šXGBéªŒè¯é›†å‘ç°ç‰¹æ®Šå€¼-2147483648ï¼Œæ›¿æ¢ä¸º0")
            y_val_pred_xgb_fixed = np.where(y_val_pred_xgb_fixed == -2147483648, 0, y_val_pred_xgb_fixed)

        # å¤„ç†æµ‹è¯•é›†é¢„æµ‹
        if isinstance(y_test_pred_xgb, (int, float)):
            y_test_pred_xgb_fixed = np.full(len(y_test_array), int(y_test_pred_xgb))
        else:
            y_test_pred_xgb_fixed = np.array(y_test_pred_xgb)

        if np.any(y_test_pred_xgb_fixed == -2147483648):
            print("è­¦å‘Šï¼šXGBæµ‹è¯•é›†å‘ç°ç‰¹æ®Šå€¼-2147483648ï¼Œæ›¿æ¢ä¸º0")
            y_test_pred_xgb_fixed = np.where(y_test_pred_xgb_fixed == -2147483648, 0, y_test_pred_xgb_fixed)

        results['xgb'] = {
            'val_accuracy': accuracy_score(y_val_array, y_val_pred_xgb_fixed),
            'val_precision': precision_score(y_val_array, y_val_pred_xgb_fixed, zero_division=0),
            'val_recall': recall_score(y_val_array, y_val_pred_xgb_fixed, zero_division=0),
            'val_f1': f1_score(y_val_array, y_val_pred_xgb_fixed, zero_division=0),
            'val_roc_auc': roc_auc_score(y_val_array, y_val_proba_xgb),
            'test_accuracy': accuracy_score(y_test_array, y_test_pred_xgb_fixed),
            'test_precision': precision_score(y_test_array, y_test_pred_xgb_fixed, zero_division=0),
            'test_recall': recall_score(y_test_array, y_test_pred_xgb_fixed, zero_division=0),
            'test_f1': f1_score(y_test_array, y_test_pred_xgb_fixed, zero_division=0),
            'test_roc_auc': roc_auc_score(y_test_array, y_test_proba_xgb)
        }

        predictions['xgb'] = y_test_pred_xgb
        probabilities['xgb'] = y_test_proba_xgb

        print("XGBoostæ¨¡å‹éªŒè¯é›†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {results['xgb']['val_accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡: {results['xgb']['val_precision']:.4f}")
        print(f"  å¬å›ç‡: {results['xgb']['val_recall']:.4f}")
        print(f"  F1åˆ†æ•°: {results['xgb']['val_f1']:.4f}")
        print(f"  ROC-AUC: {results['xgb']['val_roc_auc']:.4f}")

        print("XGBoostæ¨¡å‹æµ‹è¯•é›†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {results['xgb']['test_accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡: {results['xgb']['test_precision']:.4f}")
        print(f"  å¬å›ç‡: {results['xgb']['test_recall']:.4f}")
        print(f"  F1åˆ†æ•°: {results['xgb']['test_f1']:.4f}")
        print(f"  ROC-AUC: {results['xgb']['test_roc_auc']:.4f}")

    except Exception as e:
        print(f" XGBoostæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        print("è·³è¿‡XGBoostæ¨¡å‹ï¼Œä»…ä½¿ç”¨éšæœºæ£®æ—")
        # è®¾ç½®é»˜è®¤ç»“æœ
        y_test_array = y_test.values if hasattr(y_test, 'values') else y_test
        results['xgb'] = {
            'val_accuracy': 0.5, 'val_precision': 0.5, 'val_recall': 0.5, 'val_f1': 0.5, 'val_roc_auc': 0.5,
            'test_accuracy': 0.5, 'test_precision': 0.5, 'test_recall': 0.5, 'test_f1': 0.5, 'test_roc_auc': 0.5
        }
        predictions['xgb'] = np.zeros(len(y_test_array))
        probabilities['xgb'] = np.ones(len(y_test_array)) * 0.5
        models['xgb'] = None

    # ==================== 8. æœ€ç»ˆç»“æœç»Ÿè®¡ ====================
    print_section("æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # ç»Ÿè®¡æˆåŠŸè®­ç»ƒçš„æ¨¡å‹
    successful_models = [name for name, model in models.items() if model is not None]
    print(f"æˆåŠŸè®­ç»ƒçš„æ¨¡å‹: {len(successful_models)}/{len(models)}")

    for model_name in successful_models:
        test_f1 = results[model_name]['test_f1']
        test_auc = results[model_name]['test_roc_auc']
        print(f"  {model_name.upper()}: F1={test_f1:.4f}, AUC={test_auc:.4f}")

    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹
    if not any(models.values()):
        print("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå¤±è´¥!")
        return {}, None, {}, {}, {}

    return models, scaler, results, predictions, probabilities


@timer_decorator
def train_lightgbm_default(X_train, y_train, X_val, y_val, X_test, y_test, feature_cols):
    """ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒLightGBMå¹¶æå–ç‰¹å¾é‡è¦æ€§ï¼ˆæŒ‰å¢ç›Šï¼‰"""
    print_section("ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒLightGBM")

    # å¼ºåˆ¶ä½¿ç”¨æ»šåŠ¨CVï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if ENFORCE_ROLLING_CV_FOR_ALL_MODELS:
        USE_ROLLING_CV = True
        print("å¼ºåˆ¶æ‰§è¡Œæ»šåŠ¨äº¤å‰éªŒè¯æ¨¡å¼...")

    # ==================== ä¿®å¤ï¼šæ£€æŸ¥éªŒè¯é›†æ˜¯å¦ä¸ºç©º ====================
    # æ•°æ®æ ‡å‡†åŒ–ï¼ˆä¿æŒå’Œå…¶ä»–æ¨¡å‹ä¸€è‡´ï¼‰
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    # æ£€æŸ¥éªŒè¯é›†æ˜¯å¦ä¸ºç©ºï¼ˆæ»šåŠ¨äº¤å‰éªŒè¯æ¨¡å¼ä¸‹å¯èƒ½ä¸ºç©ºï¼‰
    if X_val is not None and len(X_val) > 0:
        X_val_scaled = scaler.transform(X_val)
        has_validation = True
    else:
        X_val_scaled = None
        has_validation = False
        print("æ³¨æ„ï¼šéªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯é›†æ ‡å‡†åŒ–å’Œè¯„ä¼°")

    X_test_scaled = scaler.transform(X_test)

    # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    smote = SMOTE(random_state=RANDOM_STATE, sampling_strategy=0.8)
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

    # åˆå§‹åŒ–LightGBMï¼ˆé»˜è®¤å‚æ•°ï¼‰
    lgb_model = lgb.LGBMClassifier(
        random_state=RANDOM_STATE,
        n_jobs=N_JOBS,
        verbosity=-1  # -1è¡¨ç¤ºé™é»˜
    )

    # ==================== ä¿®å¤ï¼šä¿å­˜æ»šåŠ¨äº¤å‰éªŒè¯æ•°æ® ====================
    rolling_cv_data = {
        'fold_scores': [],
        'fold_models': [],
        'X_train_scaled': X_train_scaled,
        'y_train': y_train,
        'fold_predictions': []
    }

    # ==================== ä¿®å¤ï¼šå¤„ç†éªŒè¯é›†ä¸ºç©ºçš„æƒ…å†µ ====================
    # è®­ç»ƒæ¨¡å‹ - æ”¯æŒæ»šåŠ¨äº¤å‰éªŒè¯
    if USE_ROLLING_CV and not X_train.empty:
        print("ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯è®­ç»ƒLightGBM...")
        tscv = TimeSeriesSplit(n_splits=ROLLING_CV_SPLITS)

        # å­˜å‚¨æ¯æŠ˜çš„æ¨¡å‹å’Œåˆ†æ•°
        lgb_models = []
        fold_scores = []

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled), 1):
            print(f"  è®­ç»ƒæŠ˜ {fold}/{ROLLING_CV_SPLITS}...")

            X_fold_train = X_train_scaled[train_idx]
            y_fold_train = y_train.iloc[train_idx] if hasattr(y_train, 'iloc') else y_train[train_idx]

            # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆå¯¹æ¯ä¸€æŠ˜å•ç‹¬åšSMOTEï¼‰
            try:
                smote_fold = SMOTE(random_state=RANDOM_STATE + fold, sampling_strategy=0.8)
                X_fold_train_bal, y_fold_train_bal = smote_fold.fit_resample(X_fold_train, y_fold_train)
            except:
                X_fold_train_bal, y_fold_train_bal = X_fold_train, y_fold_train

            # è®­ç»ƒå½“å‰æŠ˜çš„æ¨¡å‹
            fold_model = lgb.LGBMClassifier(
                random_state=RANDOM_STATE + fold,
                n_jobs=1,
                verbosity=-1
            )

            # ä½¿ç”¨å½“å‰æŠ˜çš„éªŒè¯é›†
            X_fold_val = X_train_scaled[val_idx]
            y_fold_val = y_train.iloc[val_idx] if hasattr(y_train, 'iloc') else y_train[val_idx]

            fold_model.fit(
                X_fold_train_bal, y_fold_train_bal,
                eval_set=[(X_fold_val, y_fold_val)],
                eval_metric='binary_logloss',
                callbacks=[
                    lgb.early_stopping(50, verbose=0),  # æ·»åŠ verboseå‚æ•°
                    lgb.log_evaluation(0)  # æ§åˆ¶æ—¥å¿—è¾“å‡º
                ]
            )

            # åœ¨éªŒè¯æŠ˜ä¸Šè¯„ä¼°
            y_fold_val_pred = fold_model.predict(X_fold_val)
            fold_score = f1_score(y_fold_val, y_fold_val_pred, zero_division=0)
            fold_scores.append(fold_score)

            # ä¿å­˜æŠ˜å æ¨¡å‹å’Œåˆ†æ•°
            lgb_models.append(fold_model)
            rolling_cv_data['fold_scores'].append(fold_score)
            rolling_cv_data['fold_models'].append(fold_model)

            print(f"    æŠ˜ {fold}: éªŒè¯é›†F1 = {fold_score:.4f}")

        # ä½¿ç”¨æ‰€æœ‰æŠ˜çš„å¹³å‡æ¨¡å‹ï¼ˆé€šè¿‡å¹³å‡é¢„æµ‹æ¦‚ç‡ï¼‰
        print(f"æ»šåŠ¨äº¤å‰éªŒè¯å¹³å‡F1: {np.mean(fold_scores):.4f}")

        lgb_model = EnsembleLGB(lgb_models, fold_scores)

        # ==================== ä¿å­˜æ»šåŠ¨äº¤å‰éªŒè¯æ•°æ® ====================
        # æµ‹è¯•é›†æˆæ¨¡å‹
        try:
            y_test_pred = lgb_model.predict(X_test_scaled)
            # ä¿å­˜æµ‹è¯•é¢„æµ‹
            rolling_cv_data['fold_predictions'] = y_test_pred
            print(f"é›†æˆæ¨¡å‹æµ‹è¯•é¢„æµ‹å®Œæˆï¼Œå½¢çŠ¶: {y_test_pred.shape}")
        except Exception as e:
            print(f"æµ‹è¯•é¢„æµ‹å¤±è´¥: {e}")
            y_test_pred = np.zeros(len(X_test_scaled), dtype=np.int32)

        # ä¿å­˜æ»šåŠ¨äº¤å‰éªŒè¯æ•°æ®åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        rolling_cv_file = f'lightgbm_rolling_cv_data_{timestamp}.pkl'

        # åªä¿å­˜å¿…è¦çš„æ•°æ®ï¼Œé¿å…ä¿å­˜æ•´ä¸ªæ¨¡å‹ï¼ˆæ¨¡å‹å¦å¤–ä¿å­˜ï¼‰
        rolling_cv_data_save = {
            'fold_scores': rolling_cv_data['fold_scores'],
            'X_train_shape': X_train_scaled.shape,
            'y_train_shape': y_train.shape if hasattr(y_train, 'shape') else len(y_train),
            'n_splits': ROLLING_CV_SPLITS,
            'timestamp': timestamp,
            'feature_names': feature_cols,
            'fold_predictions_test': rolling_cv_data.get('fold_predictions', [])
        }

        with open(rolling_cv_file, 'wb') as f:
            pickle.dump(rolling_cv_data_save, f, protocol=4)
        print(f"âœ… æ»šåŠ¨äº¤å‰éªŒè¯æ•°æ®å·²ä¿å­˜: {rolling_cv_file}")

        # å•ç‹¬ä¿å­˜é›†æˆæ¨¡å‹
        model_file = f'lightgbm_ensemble_model_{timestamp}.pkl'
        # æ³¨æ„ï¼šEnsembleLGBç±»å¯èƒ½æ— æ³•ç›´æ¥åºåˆ—åŒ–ï¼Œæˆ‘ä»¬ä¿å­˜åŸºç¡€æ¨¡å‹åˆ—è¡¨
        model_save_data = {
            'models': lgb_models,
            'fold_scores': fold_scores,
            'feature_names': feature_cols,
            'scaler': scaler,
            'ensemble_class': 'EnsembleLGB'
        }
        with open(model_file, 'wb') as f:
            pickle.dump(model_save_data, f, protocol=4)
        print(f"âœ… LightGBMé›†æˆæ¨¡å‹å·²ä¿å­˜: {model_file}")
    else:
        # åŸæ¥çš„è®­ç»ƒé€»è¾‘
        print("ä½¿ç”¨æ™®é€šè®­ç»ƒæ¨¡å¼...")
        # è®­ç»ƒæ¨¡å‹ - æ ¹æ®éªŒè¯é›†æ˜¯å¦å­˜åœ¨ä½¿ç”¨ä¸åŒçš„å‚æ•°
        if has_validation and X_val_scaled is not None:
            try:
                # ä½¿ç”¨æ–°ç‰ˆæœ¬çš„å›è°ƒå‡½æ•°
                lgb_model.fit(
                    X_train_balanced, y_train_balanced,
                    eval_set=[(X_val_scaled, y_val)],
                    eval_metric='binary_logloss',
                    callbacks=[
                        lgb.early_stopping(50),
                        lgb.log_evaluation(0)
                    ])
            except Exception as e:
                # å¦‚æœæ–°ç‰ˆæœ¬APIå¤±è´¥ï¼Œå°è¯•æ—§ç‰ˆæœ¬
                print(f"æ–°ç‰ˆæœ¬APIå¤±è´¥ï¼Œå°è¯•æ—§ç‰ˆæœ¬: {e}")
                # ç§»é™¤verboseå‚æ•°ï¼Œä½¿ç”¨callbacks
                lgb_model.fit(
                    X_train_balanced, y_train_balanced,
                    eval_set=[(X_val_scaled, y_val)],
                    eval_metric='binary_logloss',
                    callbacks=[
                        lgb.early_stopping(50),
                        lgb.log_evaluation(0)
                    ])
        else:
            # æ²¡æœ‰éªŒè¯é›†ï¼Œä¸ä½¿ç”¨æ—©åœ
            print("æ— éªŒè¯é›†ï¼Œè®­ç»ƒæ—¶ä¸ä½¿ç”¨æ—©åœ")
            lgb_model.fit(
                X_train_balanced, y_train_balanced,
                eval_metric='binary_logloss',
                callbacks=[lgb.log_evaluation(0)])

    # ==================== ä¿®å¤ï¼šéªŒè¯é›†è¯„ä¼°éƒ¨åˆ† ====================
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    # å®‰å…¨åœ°è·å–é¢„æµ‹ç»“æœ
    try:
        y_test_pred = lgb_model.predict(X_test_scaled)
        # ç¡®ä¿é¢„æµ‹ç»“æœæ˜¯æœ‰æ•ˆçš„æ•´æ•°æ•°ç»„
        if hasattr(y_test_pred, '__len__'):
            y_test_pred = y_test_pred.astype(np.int32)
            # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆå€¼
            mask_invalid = (y_test_pred != 0) & (y_test_pred != 1)
            if mask_invalid.any():
                print(f"è­¦å‘Šï¼šæµ‹è¯•é›†é¢„æµ‹åŒ…å«å¼‚å¸¸å€¼{np.unique(y_test_pred[mask_invalid])}ï¼Œä¿®æ­£ä¸º0")
                y_test_pred[mask_invalid] = 0
        else:
            # å¦‚æœæ˜¯å•ä¸ªå€¼ï¼Œæ£€æŸ¥æœ‰æ•ˆæ€§
            if y_test_pred not in [0, 1]:
                y_test_pred = 0
            y_test_pred = np.array([y_test_pred], dtype=np.int32)
    except Exception as e:
        print(f"æµ‹è¯•é›†é¢„æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨å…¨0é¢„æµ‹")
        y_test_pred = np.zeros(len(X_test_scaled), dtype=np.int32)

    # å®‰å…¨åœ°è·å–é¢„æµ‹æ¦‚ç‡
    try:
        proba_result = lgb_model.predict_proba(X_test_scaled)
        # æ£€æŸ¥è¿”å›çš„ç»“æœç»´åº¦
        if proba_result.ndim == 2 and proba_result.shape[1] >= 2:
            y_test_proba = proba_result[:, 1]
        elif proba_result.ndim == 1:
            # å¦‚æœåªæœ‰ä¸€ç»´ï¼Œå‡è®¾æ˜¯æ­£ç±»æ¦‚ç‡
            y_test_proba = proba_result
        else:
            # å…¶ä»–æƒ…å†µï¼Œä½¿ç”¨é»˜è®¤å€¼
            y_test_proba = np.ones(len(X_test_scaled)) * 0.5
    except Exception as e:
        print(f"é¢„æµ‹æ¦‚ç‡å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤æ¦‚ç‡")
        y_test_proba = np.ones(len(X_test_scaled)) * 0.5

    # ==================== ä¿®å¤ï¼šåœ¨è®¡ç®—æŒ‡æ ‡å‰éªŒè¯é¢„æµ‹å€¼ ====================
    # ç¡®ä¿y_trueæ˜¯numpyæ•°ç»„æ ¼å¼
    y_test_array = y_test.values if hasattr(y_test, 'values') else y_test

    # æ£€æŸ¥y_test_predæ˜¯å¦ä¸ºæœ‰æ•ˆæ•°ç»„
    if isinstance(y_test_pred, (int, float, np.int32, np.int64)):
        # å¦‚æœæ˜¯å•ä¸ªå€¼ï¼Œæ‰©å±•åˆ°æ•°ç»„
        y_test_pred_fixed = np.full(len(y_test_array), int(y_test_pred), dtype=np.int32)
    else:
        y_test_pred_fixed = np.array(y_test_pred, dtype=np.int32)

    # ä¿®å¤ç‰¹æ®Šå€¼
    if np.any(y_test_pred_fixed == -2147483648):
        print(f"è­¦å‘Šï¼šå‘ç°{np.sum(y_test_pred_fixed == -2147483648)}ä¸ªç‰¹æ®Šå€¼-2147483648ï¼Œæ›¿æ¢ä¸º0")
        y_test_pred_fixed[y_test_pred_fixed == -2147483648] = 0

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    results = {
        'test_accuracy': accuracy_score(y_test_array, y_test_pred_fixed),
        'test_precision': precision_score(y_test_array, y_test_pred_fixed, zero_division=0),
        'test_recall': recall_score(y_test_array, y_test_pred_fixed, zero_division=0),
        'test_f1': f1_score(y_test_array, y_test_pred_fixed, zero_division=0),
        'test_roc_auc': roc_auc_score(y_test_array, y_test_proba)
    }

    # å¦‚æœæœ‰éªŒè¯é›†ï¼Œä¹Ÿè®¡ç®—éªŒè¯é›†æŒ‡æ ‡
    if has_validation and X_val_scaled is not None and y_val is not None and len(y_val) > 0:
        y_val_pred = lgb_model.predict(X_val_scaled)
        y_val_proba = lgb_model.predict_proba(X_val_scaled)[:, 1]
        y_val_array = y_val.values if hasattr(y_val, 'values') else y_val

        results['val_accuracy'] = accuracy_score(y_val_array, y_val_pred)
        results['val_precision'] = precision_score(y_val_array, y_val_pred, zero_division=0)
        results['val_recall'] = recall_score(y_val_array, y_val_pred, zero_division=0)
        results['val_f1'] = f1_score(y_val_array, y_val_pred, zero_division=0)
        results['val_roc_auc'] = roc_auc_score(y_val_array, y_val_proba)

        print("LightGBMéªŒè¯é›†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {results['val_accuracy']:.4f}")
        print(f"  F1åˆ†æ•°: {results['val_f1']:.4f}")
        print(f"  ROC-AUC: {results['val_roc_auc']:.4f}")

    # ç¬¬ä¸€æ­¥ï¼šæ ¹æ®æ¨¡å‹ç±»å‹è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šé€‚é…è‡ªå®šä¹‰EnsembleLGBç±»ï¼‰
    def get_ensemble_feature_importance(model, feature_num):
        """
        é€‚é…å•ä¸ªæ¨¡å‹/é›†æˆæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§è·å–
        :param model: LightGBMæ¨¡å‹ï¼ˆå•ä¸ª/é›†æˆï¼‰
        :param feature_num: ç‰¹å¾æ€»æ•°ï¼ˆé¿å…ç»´åº¦ä¸åŒ¹é…ï¼‰
        :return: ç‰¹å¾é‡è¦æ€§æ•°ç»„ï¼ˆgainï¼‰
        """
        # æƒ…å†µ1ï¼šå•ä¸ªLightGBMæ¨¡å‹ï¼ˆæœ‰feature_importances_å±æ€§ï¼‰
        if hasattr(model, 'feature_importances_'):
            return model.feature_importances_

        # æƒ…å†µ2ï¼šè‡ªå®šä¹‰é›†æˆæ¨¡å‹ï¼ˆæœ‰modelså±æ€§ï¼Œå­˜å‚¨æ‰€æœ‰åŸºæ¨¡å‹ï¼‰
        elif hasattr(model, 'models'):
            all_importances = []
            for estimator in model.models:
                # ä»…å¤„ç†æœ‰ç‰¹å¾é‡è¦æ€§çš„åŸºæ¨¡å‹
                if hasattr(estimator, 'feature_importances_'):
                    imp = estimator.feature_importances_
                    # ç¡®ä¿ç»´åº¦ä¸€è‡´ï¼ˆé˜²æ­¢ä¸ªåˆ«åŸºæ¨¡å‹ç‰¹å¾æ•°å¼‚å¸¸ï¼‰
                    if len(imp) == feature_num:
                        all_importances.append(imp)

            if all_importances:
                # é›†æˆæ¨¡å‹ï¼šè®¡ç®—æ‰€æœ‰åŸºæ¨¡å‹ç‰¹å¾é‡è¦æ€§çš„å¹³å‡å€¼
                return np.mean(all_importances, axis=0)
            else:
                # æ— æœ‰æ•ˆåŸºæ¨¡å‹æ—¶ï¼Œè¿”å›å…¨0æ•°ç»„
                return np.zeros(feature_num)

        # æƒ…å†µ3ï¼šæœªçŸ¥æ¨¡å‹ç±»å‹ï¼Œè¿”å›å…¨0æ•°ç»„
        else:
            return np.zeros(feature_num)

    # ç¬¬äºŒæ­¥ï¼šè·å–ç‰¹å¾é‡è¦æ€§å¹¶æ„å»ºDataFrame
    # å…ˆç¡®è®¤ç‰¹å¾æ€»æ•°ï¼ˆé¿å…ç»´åº¦ä¸åŒ¹é…ï¼‰
    feature_num = len(feature_cols) if feature_cols else 0
    feature_importances_gain = get_ensemble_feature_importance(lgb_model, feature_num)

    # æ„å»ºç‰¹å¾é‡è¦æ€§DataFrameï¼ˆå…¼å®¹ç©ºå€¼ï¼‰
    if feature_num > 0 and len(feature_importances_gain) == feature_num:
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'gain': feature_importances_gain,  # å¢ç›Šï¼ˆæ ¸å¿ƒç‰¹å¾é‡è¦æ€§æŒ‡æ ‡ï¼‰
            'importance_type': 'gain'
        }).sort_values('gain', ascending=False)
    else:
        # æ— æœ‰æ•ˆç‰¹å¾æ—¶ï¼Œè¿”å›ç©ºDataFrameï¼ˆé¿å…æŠ¥é”™ï¼‰
        feature_importance = pd.DataFrame(columns=['feature', 'gain', 'importance_type'])

    # å¯é€‰ï¼šæ‰“å°ç‰¹å¾é‡è¦æ€§ä¿¡æ¯ï¼Œæ–¹ä¾¿è°ƒè¯•
    print(f"âœ… ç‰¹å¾é‡è¦æ€§æå–å®Œæˆï¼š")
    print(f"   - ç‰¹å¾æ€»æ•°ï¼š{feature_num}")
    print(f"   - æœ‰æ•ˆç‰¹å¾é‡è¦æ€§æ•°é‡ï¼š{len(feature_importance)}")
    print(f"   - å‰5ä¸ªé‡è¦ç‰¹å¾ï¼š\n{feature_importance.head()}")

    print("LightGBMé»˜è®¤å‚æ•°è®­ç»ƒç»“æœ:")
    print(f"  å‡†ç¡®ç‡: {results['test_accuracy']:.4f}")
    print(f"  F1åˆ†æ•°: {results['test_f1']:.4f}")
    print(f"  ROC-AUC: {results['test_roc_auc']:.4f}")

    return lgb_model, scaler, results, feature_importance

def select_top10_core_factors(feature_importance, financial_cols, technical_cols):
    """
    é€‰æ‹©å‰10ä¸ªæ ¸å¿ƒå› å­ï¼š
    - åŸºäºç‰¹å¾é‡è¦æ€§å¢ç›Šï¼ˆgainï¼‰æ’åº
    - å¦‚æœæ€»å› å­æ•°>10ï¼Œä¿ç•™å¢ç›Šæœ€é«˜çš„10ä¸ª
    - å¦‚æœæ€»å› å­æ•°<10ï¼Œä¿ç•™æ‰€æœ‰æœ‰æ•ˆå› å­
    """
    print_section("ç­›é€‰å‰10ä¸ªæ ¸å¿ƒå› å­ï¼ˆåŸºäºå¢ç›Šï¼‰")

    # åˆ†ç¦»è´¢åŠ¡/æŠ€æœ¯å› å­çš„é‡è¦æ€§å¹¶æŒ‰å¢ç›Šé™åºæ’åº
    fin_importance = feature_importance[feature_importance['feature'].isin(financial_cols)].sort_values('gain',
                                                                                                        ascending=False)
    tech_importance = feature_importance[feature_importance['feature'].isin(technical_cols)].sort_values('gain',
                                                                                                         ascending=False)

    print(f"è´¢åŠ¡å› å­æ•°é‡ï¼ˆæŒ‰å¢ç›Šæ’åºï¼‰: {len(fin_importance)}")
    print(f"æŠ€æœ¯å› å­æ•°é‡ï¼ˆæŒ‰å¢ç›Šæ’åºï¼‰: {len(tech_importance)}")

    # åˆå¹¶æ‰€æœ‰å› å­å¹¶æŒ‰å¢ç›Šé™åºæ’åº
    all_importance = pd.concat([fin_importance, tech_importance]).sort_values('gain', ascending=False)

    # ç¡®å®šæœ€ç»ˆä¿ç•™æ•°é‡ï¼šæœ€å¤š10ä¸ªï¼Œæœ€å°‘å®é™…å¯ç”¨æ•°é‡
    total_available = len(all_importance)
    target_count = min(total_available, 10)

    # é€‰æ‹©å¢ç›Šæœ€é«˜çš„å› å­
    selected_core = all_importance.head(target_count)['feature'].tolist()

    # åˆ†ç¦»é€‰ä¸­çš„è´¢åŠ¡å’ŒæŠ€æœ¯å› å­
    selected_fin = [f for f in selected_core if f in financial_cols]
    selected_tech = [f for f in selected_core if f in technical_cols]

    print(f"\næœ€ç»ˆç­›é€‰ç»“æœ:")
    print(f"  è´¢åŠ¡å› å­: {len(selected_fin)} ä¸ª")
    print(f"  æŠ€æœ¯å› å­: {len(selected_tech)} ä¸ª")
    print(f"  æ€»å› å­æ•°: {len(selected_core)} ä¸ª (æœ€å¤š10ä¸ª)")

    # æ‰“å°å…·ä½“å› å­åˆ—è¡¨ï¼ˆå¸¦å¢ç›Šå’Œç±»å‹ï¼‰
    print(f"\né€‰ä¸­çš„æ ¸å¿ƒå› å­ï¼ˆæŒ‰å¢ç›Šæ’åºï¼‰:")
    for i, factor in enumerate(selected_core, 1):
        gain = all_importance[all_importance['feature'] == factor]['gain'].values[0]
        factor_type = "è´¢åŠ¡å› å­" if factor in financial_cols else "æŠ€æœ¯å› å­"
        print(f"  {i:2d}. {factor:<30} {factor_type:<8} å¢ç›Š: {gain:.2f}")

    return selected_core, selected_fin, selected_tech

@timer_decorator
def analyze_feature_importance(models, feature_cols, n_top=20):
    """åˆ†æç‰¹å¾é‡è¦æ€§ - ä¿®å¤ç‰ˆæœ¬"""
    print_section("ç‰¹å¾é‡è¦æ€§åˆ†æ")

    # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
    feature_importance = pd.DataFrame({'feature': feature_cols})

    for model_name, model in models.items():
        if model is not None and hasattr(model, 'feature_importances_'):
            try:
                importances = model.feature_importances_

                # ä¿®å¤ï¼šè·å–æ¨¡å‹å®é™…ä½¿ç”¨çš„ç‰¹å¾åç§°
                if hasattr(model, 'feature_names_in_'):
                    # ä½¿ç”¨æ¨¡å‹è®­ç»ƒæ—¶çš„ç‰¹å¾åç§°
                    model_features = list(model.feature_names_in_)
                else:
                    # å›é€€åˆ°ä¼ å…¥çš„ç‰¹å¾åˆ—è¡¨
                    model_features = feature_cols

                # ç¡®ä¿ç‰¹å¾æ•°é‡åŒ¹é…
                if len(importances) == len(model_features):
                    # åˆ›å»ºä¸´æ—¶DataFrameæ¥åŒ¹é…ç‰¹å¾
                    temp_importance = pd.DataFrame({
                        'feature': model_features,
                        f'importance_{model_name}': importances
                    })
                    # åˆå¹¶åˆ°ä¸»DataFrame
                    feature_importance = feature_importance.merge(
                        temp_importance, on='feature', how='left'
                    )
                else:
                    print(f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…: æ¨¡å‹{model_name}")
                    # ä½¿ç”¨å¯¹é½çš„é€»è¾‘
                    min_len = min(len(importances), len(feature_cols))
                    importance_series = np.zeros(len(feature_cols))
                    importance_series[:min_len] = importances[:min_len]
                    feature_importance[f'importance_{model_name}'] = importance_series

            except Exception as e:
                print(f"æ¨¡å‹ {model_name} ç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥: {e}")
                feature_importance[f'importance_{model_name}'] = 0.0

    # è®¡ç®—å¹³å‡é‡è¦æ€§
    importance_cols = [col for col in feature_importance.columns if col.startswith('importance_')]
    if importance_cols:
        feature_importance['importance_mean'] = feature_importance[importance_cols].mean(axis=1)
        feature_importance = feature_importance.sort_values('importance_mean', ascending=False)

    print(f"Top {n_top} é‡è¦ç‰¹å¾:")
    if len(feature_importance) > 0:
        print(feature_importance.head(min(n_top, len(feature_importance))).to_string(index=False))

        # æ˜¾ç¤ºç‰¹å¾ç±»å‹ç»Ÿè®¡
        tech_features = len([col for col in feature_cols if not col.startswith('fin_')])
        fin_features = len([col for col in feature_cols if col.startswith('fin_')])
        print(f"ç‰¹å¾ç±»å‹ç»Ÿè®¡: æŠ€æœ¯ç‰¹å¾={tech_features}, è´¢åŠ¡ç‰¹å¾={fin_features}")
    else:
        print("æ²¡æœ‰ç‰¹å¾é‡è¦æ€§æ•°æ®")

    return feature_importance


def generate_daily_selected_stocks(test_df, predictions, probabilities, top_n=10):
    """ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨ - ä¿®å¤ç‰ˆæœ¬ï¼ˆåˆ é™¤æ”¶ç›Šç‡è®¡ç®—ï¼‰"""
    print_section("ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨")

    if test_df.empty or not predictions:
        print("æµ‹è¯•æ•°æ®æˆ–é¢„æµ‹ç»“æœä¸ºç©º")
        return pd.DataFrame()

    try:
        # ==================== 1. æ•°æ®å‡†å¤‡å’ŒéªŒè¯ ====================
        print("æ•°æ®å‡†å¤‡å’ŒéªŒè¯...")

        # å¤åˆ¶æµ‹è¯•é›†æ•°æ®
        required_cols = ['date', 'stock_code', 'close', 'future_return']
        missing_cols = [col for col in required_cols if col not in test_df.columns]
        if missing_cols:
            print(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return pd.DataFrame()

        selected_stocks = test_df[required_cols].copy()

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        initial_count = len(selected_stocks)
        selected_stocks = selected_stocks.dropna(subset=['future_return'])
        print(f"ç§»é™¤æœªæ¥æ”¶ç›Šç‡ç¼ºå¤±çš„æ•°æ®: {initial_count - len(selected_stocks):,} è¡Œ")

        if selected_stocks.empty:
            print("é€‰è‚¡æ•°æ®ä¸ºç©º")
            return pd.DataFrame()

        # æ·»åŠ æ¨¡å‹é¢„æµ‹æ¦‚ç‡
        for model_name in predictions.keys():
            if model_name in predictions and len(predictions[model_name]) == len(selected_stocks):
                selected_stocks[f'{model_name}_prediction'] = predictions[model_name]
                selected_stocks[f'{model_name}_probability'] = probabilities[model_name]
            else:
                print(f"æ¨¡å‹ {model_name} é¢„æµ‹ç»“æœé•¿åº¦ä¸åŒ¹é…ï¼Œè·³è¿‡")

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹è¿›è¡Œé€‰è‚¡
        available_models = [m for m in predictions.keys() if f'{m}_probability' in selected_stocks.columns]
        if available_models:
            best_model = available_models[0]
        else:
            best_model = 'rf'
            # å¦‚æœæ²¡æœ‰æ¨¡å‹æ¦‚ç‡ï¼Œä½¿ç”¨éšæœºåˆ†æ•°
            selected_stocks['selection_score'] = np.random.random(len(selected_stocks))
            print("æ— å¯ç”¨æ¨¡å‹æ¦‚ç‡ï¼Œä½¿ç”¨éšæœºé€‰è‚¡")

        print(f"ä½¿ç”¨æ¨¡å‹è¿›è¡Œé€‰è‚¡: {best_model.upper()}")
        selected_stocks['selection_score'] = selected_stocks[f'{best_model}_probability']

        # ==================== 2. ä¿®å¤é€‰è‚¡é€»è¾‘ ====================
        print("ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨...")
        daily_top_stocks = []
        valid_dates = 0

        # è·å–å”¯ä¸€æ—¥æœŸå¹¶æ’åº
        unique_dates = sorted(selected_stocks['date'].unique())
        print(f"å¤„ç† {len(unique_dates)} ä¸ªäº¤æ˜“æ—¥çš„é€‰è‚¡...")

        for date in tqdm(unique_dates, desc="ç”Ÿæˆæ¯æ—¥é€‰è‚¡"):
            date_data = selected_stocks[selected_stocks['date'] == date].copy()

            if len(date_data) == 0:
                continue

            # æŒ‰é¢„æµ‹æ¦‚ç‡æ’åº
            date_data = date_data.sort_values('selection_score', ascending=False)
            date_data = date_data.drop_duplicates(subset=['stock_code'], keep='first')

            # ä¿®å¤ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„è‚¡ç¥¨å¯é€‰
            if len(date_data) < top_n:
                if len(date_data) > 0:
                    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨è‚¡ç¥¨
                    top_n_stocks = date_data.copy()
                    print(f"æ—¥æœŸ {date.date()} åªæœ‰ {len(date_data)} åªè‚¡ç¥¨ï¼Œä½¿ç”¨å…¨éƒ¨å¯ç”¨è‚¡ç¥¨")
                else:
                    print(f"æ—¥æœŸ {date.date()} æ²¡æœ‰å¯ç”¨è‚¡ç¥¨ï¼Œè·³è¿‡")
                    continue
            else:
                # é€‰æ‹©Top N
                top_n_stocks = date_data.head(top_n).copy()

            # ç¡®ä¿æœ‰é€‰è‚¡ç»“æœ
            if len(top_n_stocks) == 0:
                print(f"æ—¥æœŸ {date.date()} é€‰è‚¡ç»“æœä¸ºç©ºï¼Œä½¿ç”¨éšæœºé€‰æ‹©")
                # å›é€€ï¼šéšæœºé€‰æ‹©top_nåªè‚¡ç¥¨
                if len(date_data) > 0:
                    top_n_stocks = date_data.sample(n=min(top_n, len(date_data)),
                                                    random_state=RANDOM_STATE)
                else:
                    continue

            top_n_stocks['rank'] = range(1, len(top_n_stocks) + 1)
            daily_top_stocks.append(top_n_stocks)
            valid_dates += 1

        print(f"æˆåŠŸå¤„ç† {valid_dates}/{len(unique_dates)} ä¸ªäº¤æ˜“æ—¥çš„é€‰è‚¡")

        if not daily_top_stocks:
            print("æ²¡æœ‰ç”Ÿæˆä»»ä½•é€‰è‚¡åˆ—è¡¨")
            return pd.DataFrame()

        # ==================== 3. åˆå¹¶ç»“æœ ====================
        result_df = pd.concat(daily_top_stocks, ignore_index=True)

        # æ·»åŠ é€‰è‚¡ç†ç”±
        result_df['selection_reason'] = result_df.apply(
            lambda
                x: f"æ¨¡å‹é¢„æµ‹æ¦‚ç‡:{x['selection_score']:.3f}, æ’å:{x['rank']}/{min(top_n, len(result_df[result_df['date'] == x['date']]))}",
            axis=1
        )

        # é‡å‘½ååˆ—
        result_df = result_df.rename(columns={
            'date': 'äº¤æ˜“æ—¥',
            'stock_code': 'è‚¡ç¥¨ä»£ç ',
            'close': 'æ”¶ç›˜ä»·',
            'future_return': 'æœªæ¥20å¤©ç»å¯¹æ”¶ç›Šç‡',
            'selection_score': 'æ¨¡å‹é¢„æµ‹æ¦‚ç‡',
            'rank': 'å½“æ—¥æ’å',
            'selection_reason': 'é€‰è‚¡ç†ç”±'
        })

        # é€‰æ‹©éœ€è¦çš„åˆ—
        final_columns = ['äº¤æ˜“æ—¥', 'è‚¡ç¥¨ä»£ç ', 'æ”¶ç›˜ä»·', 'æœªæ¥20å¤©ç»å¯¹æ”¶ç›Šç‡',
                         'æ¨¡å‹é¢„æµ‹æ¦‚ç‡', 'å½“æ—¥æ’å', 'é€‰è‚¡ç†ç”±']
        final_columns = [col for col in final_columns if col in result_df.columns]
        result_df = result_df[final_columns]

        print(f"ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨: {result_df.shape}")

        # ==================== 4. ç®€å•çš„é€‰è‚¡ç»Ÿè®¡ï¼ˆåˆ é™¤æ”¶ç›Šç‡è®¡ç®—ï¼‰ ====================
        print_section("é€‰è‚¡ç»“æœç»Ÿè®¡")

        # ç®€å•çš„ç»Ÿè®¡ï¼ˆä¸æ¶‰åŠå¤æ‚æ”¶ç›Šç‡è®¡ç®—ï¼‰
        total_stocks = len(result_df)
        unique_stocks = result_df['è‚¡ç¥¨ä»£ç '].nunique()
        avg_daily_stocks = result_df.groupby('äº¤æ˜“æ—¥').size().mean()
        avg_prob_all = result_df['æ¨¡å‹é¢„æµ‹æ¦‚ç‡'].mean()

        print(f"é€‰è‚¡ç»Ÿè®¡:")
        print(f"   æ€»é€‰è‚¡è®°å½•: {total_stocks:,} æ¡")
        print(f"   å”¯ä¸€è‚¡ç¥¨æ•°é‡: {unique_stocks} åª")
        print(f"   å¹³å‡æ¯æ—¥é€‰è‚¡: {avg_daily_stocks:.1f} åª")
        print(f"   å¹³å‡é¢„æµ‹æ¦‚ç‡: {avg_prob_all:.3f}")

        # ==================== 5. éªŒè¯é€‰è‚¡ç»“æœ ====================
        print_section("é€‰è‚¡ç»“æœéªŒè¯")

        # æ£€æŸ¥æœ€è¿‘å‡ ä¸ªäº¤æ˜“æ—¥çš„é€‰è‚¡ç»“æœ
        recent_dates = result_df['äº¤æ˜“æ—¥'].unique()[-3:]  # æœ€è¿‘3ä¸ªäº¤æ˜“æ—¥
        for test_date in recent_dates:
            daily_selection = result_df[result_df['äº¤æ˜“æ—¥'] == test_date]
            print(f"éªŒè¯ {test_date.date()} çš„é€‰è‚¡ç»“æœ:")
            print(f"   é€‰è‚¡æ•°é‡: {len(daily_selection)} åª")
            print(f"   å”¯ä¸€è‚¡ç¥¨: {len(daily_selection['è‚¡ç¥¨ä»£ç '].unique())} åª")
            if len(daily_selection) > 0:
                top_stocks = daily_selection['è‚¡ç¥¨ä»£ç '].head(3).tolist()
                avg_prob = daily_selection['æ¨¡å‹é¢„æµ‹æ¦‚ç‡'].mean()
                print(f"   å‰3åªè‚¡ç¥¨: {top_stocks}")
                print(f"   å¹³å‡é¢„æµ‹æ¦‚ç‡: {avg_prob:.3f}")
                # åˆ é™¤æ”¶ç›Šç‡è®¡ç®—ï¼Œåªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            else:
                print(" è¯¥æ—¥æ— é€‰è‚¡ç»“æœ")

        return result_df

    except Exception as e:
        print(f"ç”Ÿæˆæ¯æ—¥é€‰è‚¡åˆ—è¡¨å¤±è´¥: {e}")
        traceback.print_exc()
        return pd.DataFrame()



def emergency_recalculate_returns(df, days=FUTURE_DAYS):
    """ç´§æ€¥é‡æ–°è®¡ç®—æ”¶ç›Šç‡ - ç®€åŒ–ç‰ˆæœ¬"""
    print("æ‰§è¡Œç´§æ€¥æ”¶ç›Šç‡é‡æ–°è®¡ç®—...")

    df = df.copy().sort_values(['stock_code', 'date'])
    returns = np.full(len(df), np.nan)

    # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—
    for stock_code in df['stock_code'].unique():
        stock_data = df[df['stock_code'] == stock_code].sort_values('date')
        close_prices = stock_data['close'].values

        for i in range(len(stock_data)):
            if i + days < len(stock_data):
                current_price = close_prices[i]
                future_price = close_prices[i + days]

                # æ£€æŸ¥ä»·æ ¼æœ‰æ•ˆæ€§
                if current_price > 0 and future_price > 0 and not np.isnan(current_price) and not np.isnan(
                        future_price):
                    return_val = (future_price / current_price) - 1
                    # æ‰¾åˆ°åœ¨åŸå§‹dfä¸­çš„ç´¢å¼•
                    original_idx = stock_data.index[i]
                    returns[df.index.get_loc(original_idx)] = return_val

    df['future_return'] = returns

    # ç»Ÿè®¡ç»“æœ
    valid_returns = returns[~np.isnan(returns)]
    if len(valid_returns) > 0:
        print(f"ç´§æ€¥è®¡ç®—å®Œæˆ: {len(valid_returns):,} ä¸ªæœ‰æ•ˆæ”¶ç›Šç‡")
        print(f"  æ”¶ç›Šç‡èŒƒå›´: {valid_returns.min():.4f} åˆ° {valid_returns.max():.4f}")

        # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
        sample_count = min(3, len(valid_returns))
        sample_indices = np.random.choice(len(valid_returns), sample_count, replace=False)
        for i, idx in enumerate(sample_indices):
            print(f"  æ ·æœ¬{i + 1}: {valid_returns[idx]:.4f} ({valid_returns[idx]:.2%})")
    else:
        print("ç´§æ€¥è®¡ç®—å¤±è´¥ï¼šæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆæ”¶ç›Šç‡")

    return df


@timer_decorator
def perform_backtest_with_costs(daily_selected_df, test_df, benchmark_returns=None):
    """
    æ‰§è¡Œå¸¦äº¤æ˜“æˆæœ¬å’Œé£æ§çš„æ”¶ç›Šå›æµ‹ - ä¿®å¤ç‰ˆæœ¬
    """
    print_section("å®ç›˜è´´è¿‘åº¦å›æµ‹ï¼ˆå«äº¤æ˜“æˆæœ¬ä¸é£æ§ï¼‰")

    try:
        # å‡†å¤‡æ•°æ®
        backtest_data = daily_selected_df.copy()

        # è·å–å”¯ä¸€æ—¥æœŸå¹¶æ’åº
        unique_dates = sorted(backtest_data['date'].unique())
        if len(unique_dates) == 0:
            print("é”™è¯¯ï¼šæ²¡æœ‰å›æµ‹æ—¥æœŸ")
            return None

        print(f"å›æµ‹æœŸé—´: {unique_dates[0]} åˆ° {unique_dates[-1]}")
        print(f"æ€»äº¤æ˜“æ—¥æ•°: {len(unique_dates)}")

        # åˆå§‹åŒ–é£æ§ç®¡ç†å™¨
        risk_manager = RiskControlManager()

        # åˆ›å»ºä»·æ ¼æ•°æ®å­—å…¸ä»¥ä¾¿å¿«é€ŸæŸ¥è¯¢
        price_dict = {}
        for stock_code in test_df['stock_code'].unique():
            stock_data = test_df[test_df['stock_code'] == stock_code]
            if not stock_data.empty:
                price_dict[stock_code] = dict(zip(stock_data['date'], stock_data['close']))

        # ç¡®å®šè°ƒä»“æ—¥ï¼ˆæ¯å­£åº¦è°ƒä»“ï¼‰
        rebalance_dates = []
        current_quarter = None

        for date in unique_dates:
            quarter = f"{date.year}-Q{(date.month - 1) // 3 + 1}"
            if quarter != current_quarter:
                rebalance_dates.append(date)
                current_quarter = quarter

        print(f"è°ƒä»“æ—¥æ•°é‡: {len(rebalance_dates)}")
        print(f"è°ƒä»“æ—¥åˆ—è¡¨: {rebalance_dates[:5]}...")

        # æ‰§è¡Œå›æµ‹
        portfolio_values = []
        portfolio_returns = []
        trading_summary = []

        for i, current_date in enumerate(tqdm(unique_dates, desc="æ‰§è¡Œå›æµ‹")):

            # è·å–å½“å‰æ—¥æœŸæ‰€æœ‰è‚¡ç¥¨çš„ä»·æ ¼
            current_prices = {}
            for stock_code in price_dict:
                if current_date in price_dict[stock_code]:
                    current_prices[stock_code] = price_dict[stock_code][current_date]

            # å¦‚æœæ˜¯è°ƒä»“æ—¥ï¼Œæ‰§è¡Œè°ƒä»“
            if current_date in rebalance_dates:
                print(f"\nè°ƒä»“æ—¥: {current_date}")

                # è·å–å½“æ—¥çš„é€‰è‚¡åˆ—è¡¨
                daily_stocks = backtest_data[backtest_data['date'] == current_date]

                if daily_stocks.empty:
                    print(f"  æ—¥æœŸ{current_date}æ²¡æœ‰é€‰è‚¡æ•°æ®")
                    continue

                # æŒ‰æ¨¡å‹é¢„æµ‹æ¦‚ç‡æ’åº
                daily_stocks = daily_stocks.sort_values('æ¨¡å‹é¢„æµ‹æ¦‚ç‡', ascending=False)

                # ç¡®å®šä¹°å…¥åˆ—è¡¨ï¼ˆæœ€å¤šTOP_N_HOLDINGSåªï¼‰
                buy_list = daily_stocks.head(TOP_N_HOLDINGS)
                print(f"  é€‰è‚¡æ•°é‡: {len(buy_list)}åª")

                # å–å‡ºä¸åœ¨ä¹°å…¥åˆ—è¡¨ä¸­çš„è‚¡ç¥¨
                stocks_to_sell = []
                for stock_code in list(risk_manager.positions.keys()):
                    if stock_code not in buy_list['è‚¡ç¥¨ä»£ç '].values:
                        stocks_to_sell.append(stock_code)

                for stock_code in stocks_to_sell:
                    if stock_code in current_prices:
                        position = risk_manager.positions[stock_code]
                        risk_manager.execute_sell(stock_code, position['shares'],
                                                  current_prices[stock_code], current_date,
                                                  'rebalance')

                # ä¹°å…¥æ–°è‚¡ç¥¨
                for _, row in buy_list.iterrows():
                    stock_code = row['è‚¡ç¥¨ä»£ç ']
                    if stock_code not in current_prices:
                        continue

                    current_price = current_prices[stock_code]

                    # è®¡ç®—æ¯åªè‚¡ç¥¨çš„æƒé‡
                    target_weight = 1.0 / len(buy_list)  # ç­‰æƒé‡
                    target_weight = risk_manager.check_single_stock_limit(stock_code, target_weight)

                    # è®¡ç®—ç›®æ ‡æŒä»“ä»·å€¼
                    total_portfolio_value = risk_manager.cash
                    for code, pos in risk_manager.positions.items():
                        if code in current_prices:
                            total_portfolio_value += pos['shares'] * current_prices[code]

                    target_value = total_portfolio_value * target_weight

                    # å¦‚æœå·²æœ‰æŒä»“ï¼Œè®¡ç®—éœ€è¦è°ƒæ•´çš„æ•°é‡
                    if stock_code in risk_manager.positions:
                        position = risk_manager.positions[stock_code]
                        current_value = position['shares'] * current_price
                        value_diff = target_value - current_value

                        if value_diff > 0:  # éœ€è¦ä¹°å…¥
                            shares_to_buy = int(value_diff / current_price)
                            if shares_to_buy > 0:
                                risk_manager.execute_buy(stock_code, shares_to_buy,
                                                         current_price, current_date)
                        elif value_diff < 0:  # éœ€è¦å–å‡º
                            shares_to_sell = int(-value_diff / current_price)
                            if shares_to_sell > 0:
                                risk_manager.execute_sell(stock_code, shares_to_sell,
                                                          current_price, current_date,
                                                          'rebalance')
                    else:  # æ–°ä¹°å…¥
                        shares_to_buy = int(target_value / current_price)
                        if shares_to_buy > 0:
                            risk_manager.execute_buy(stock_code, shares_to_buy,
                                                     current_price, current_date)

            # æ›´æ–°æŒä»“å¸‚å€¼
            total_value = risk_manager.update_positions(price_dict, current_date)

            # æ£€æŸ¥ç»„åˆå›æ’¤
            stop_loss_triggered, drawdown = risk_manager.check_portfolio_drawdown(total_value)
            if stop_loss_triggered:
                print(f"\næ—¥æœŸ {current_date}: ç»„åˆå›æ’¤è¾¾åˆ°{drawdown:.2%}ï¼Œè§¦å‘å‡ä»“")
                risk_manager.reduce_positions()

            # è®°å½•æ¯æ—¥ç»„åˆä»·å€¼
            portfolio_values.append({
                'date': current_date,
                'portfolio_value': total_value,
                'cash': risk_manager.cash,
                'drawdown': drawdown,
                'total_value': total_value
            })

            # è®¡ç®—æ—¥æ”¶ç›Šç‡
            if i > 0:
                prev_value = portfolio_values[i - 1]['portfolio_value']
                if prev_value > 0:
                    daily_return = (total_value - prev_value) / prev_value
                else:
                    daily_return = 0
                portfolio_returns.append(daily_return)
            else:
                portfolio_returns.append(0)

        # è®¡ç®—å›æµ‹æŒ‡æ ‡
        metrics = calculate_backtest_metrics(portfolio_values, portfolio_returns, benchmark_returns)

        # ç”Ÿæˆäº¤æ˜“ç»Ÿè®¡
        trading_stats = generate_trading_statistics(risk_manager.trading_records, portfolio_values)

        # ç”Ÿæˆå›æµ‹æŠ¥å‘Š
        report = generate_backtest_report(metrics, trading_stats, risk_manager.positions)

        # è®¡ç®—å¹³å‡æŒä»“å¤©æ•°
        if risk_manager.position_days:
            avg_hold_days = np.mean(list(risk_manager.position_days.values()))
        else:
            avg_hold_days = 0

        # æ·»åŠ åˆ°æŠ¥å‘Š
        report['additional_stats'] = {
            'å¹³å‡æŒä»“å¤©æ•°': avg_hold_days,
            'æœ€å¤§æŒä»“å¤©æ•°': max(risk_manager.position_days.values()) if risk_manager.position_days else 0,
            'æœ€å°æŒä»“å¤©æ•°': min(risk_manager.position_days.values()) if risk_manager.position_days else 0
        }

        return {
            'portfolio_values': portfolio_values,
            'portfolio_returns': portfolio_returns,
            'metrics': metrics,
            'trading_stats': trading_stats,
            'trading_records': risk_manager.trading_records,
            'positions': risk_manager.positions,
            'report': report,
            'holdings_history': risk_manager.holdings_history
        }

    except Exception as e:
        print(f"å›æµ‹æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


def calculate_turnover_rate(trades_df, portfolio_values):
    """è®¡ç®—æ¢æ‰‹ç‡ - ä½¿ç”¨å•è¾¹æ¢æ‰‹ç‡ï¼ˆä¸šå†…æ ‡å‡†ï¼‰"""
    if len(portfolio_values) < 2 or trades_df.empty:
        return 0

    # æ–¹æ³•1ï¼šåªè®¡ç®—å–å‡ºäº¤æ˜“é‡‘é¢ï¼ˆä¸šå†…æ ‡å‡†ï¼‰
    if 'type' in trades_df.columns:
        # ä½¿ç”¨å–å‡ºäº¤æ˜“é‡‘é¢ï¼ˆå•è¾¹ï¼‰
        sell_trades = trades_df[trades_df['type'] == 'sell']
        total_trade_amount = sell_trades['total_value'].sum()

        # æˆ–è€…ä½¿ç”¨ä¹°å…¥äº¤æ˜“é‡‘é¢ï¼ˆä¸¤ç§æ–¹å¼ç­‰ä»·ï¼‰
        # buy_trades = trades_df[trades_df['type'] == 'buy']
        # total_trade_amount = buy_trades['total_value'].sum()
    else:
        # å¦‚æœæ— æ³•åŒºåˆ†ç±»å‹ï¼Œä½¿ç”¨äº¤æ˜“é‡‘é¢çš„ä¸€åŠï¼ˆè¿‘ä¼¼å•è¾¹ï¼‰
        total_trade_amount = trades_df['total_value'].sum() / 2

    # è®¡ç®—å¹³å‡èµ„äº§å‡€å€¼
    avg_portfolio_value = np.mean([
        pv.get('total_value', pv.get('portfolio_value', 0))
        for pv in portfolio_values
    ])

    if avg_portfolio_value > 0:
        # å•è¾¹æ¢æ‰‹ç‡
        turnover_rate = total_trade_amount / avg_portfolio_value

        # å¹´åŒ–è®¡ç®—
        if len(portfolio_values) > 1:
            days = (portfolio_values[-1]['date'] - portfolio_values[0]['date']).days
            if days > 0:
                trading_days = len(portfolio_values)
                # ä½¿ç”¨242ä¸ªäº¤æ˜“æ—¥ï¼ˆå°æ¹¾å¸‚åœºï¼‰
                turnover_rate = turnover_rate * (242 / trading_days)
    else:
        turnover_rate = 0

    return turnover_rate

def check_position_concentration(positions):
    """æ£€æŸ¥ä»“ä½é›†ä¸­åº¦"""
    if not positions:
        return True

    total_value = sum(pos['shares'] * pos.get('current_price', pos['avg_price']) for pos in positions.values())

    for stock_code, position in positions.items():
        position_value = position['shares'] * position.get('current_price', position['avg_price'])
        weight = position_value / total_value if total_value > 0 else 0

        if weight > RISK_CONTROL['single_stock_limit']:
            print(f"âš ï¸ è‚¡ç¥¨{stock_code}ä»“ä½{weight:.2%}è¶…è¿‡é™åˆ¶{RISK_CONTROL['single_stock_limit']:.2%}")
            return False

    return True


def check_stop_loss_execution(trading_stats):
    """æ£€æŸ¥æ­¢æŸè§„åˆ™æ‰§è¡Œæƒ…å†µ"""
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ£€æŸ¥é€»è¾‘
    return True


def check_stop_profit_execution(trading_stats):
    """æ£€æŸ¥æ­¢ç›ˆè§„åˆ™æ‰§è¡Œæƒ…å†µ"""
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ£€æŸ¥é€»è¾‘
    return True

def plot_backtest_results(backtest_results, save_path=None):
    """
    ç»˜åˆ¶å›æµ‹ç»“æœå›¾è¡¨
    """
    try:
        if 'error' in backtest_results:
            print("æ— æ³•ç»˜åˆ¶å›¾è¡¨ï¼šå›æµ‹ç»“æœåŒ…å«é”™è¯¯")
            return

        # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¦‚æœæœ‰ä¸­æ–‡å­—ç¬¦ï¼‰
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(3, 2, figsize=(16, 12))
        fig.suptitle('è‚¡ç¥¨é€‰è‚¡ç­–ç•¥å›æµ‹åˆ†æ', fontsize=16, fontweight='bold')

        # --- å­å›¾1ï¼šå‡€å€¼æ›²çº¿å¯¹æ¯” ---
        ax1 = axes[0, 0]
        strategy_nav = backtest_results['strategy_net_value']
        benchmark_nav = backtest_results['benchmark_cumulative_returns'] * 10000

        ax1.plot(strategy_nav.index, strategy_nav.values, 'b-', linewidth=2, label='ç­–ç•¥å‡€å€¼')
        ax1.plot(benchmark_nav.index, benchmark_nav.values, 'g-', linewidth=1.5, label='åŸºå‡†å‡€å€¼', alpha=0.7)
        ax1.set_title('å‡€å€¼æ›²çº¿å¯¹æ¯”')
        ax1.set_xlabel('æ—¥æœŸ')
        ax1.set_ylabel('å‡€å€¼ï¼ˆå…ƒï¼‰')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # --- å­å›¾2ï¼šç´¯è®¡æ”¶ç›Šç‡å¯¹æ¯” ---
        ax2 = axes[0, 1]
        strategy_cum_return = (backtest_results['strategy_cumulative_returns'] - 1) * 100
        benchmark_cum_return = (backtest_results['benchmark_cumulative_returns'] - 1) * 100

        ax2.plot(strategy_cum_return.index, strategy_cum_return.values, 'r-', linewidth=2, label='ç­–ç•¥ç´¯è®¡æ”¶ç›Š')
        ax2.plot(benchmark_cum_return.index, benchmark_cum_return.values, 'b-', linewidth=1.5, label='åŸºå‡†ç´¯è®¡æ”¶ç›Š',
                 alpha=0.7)
        ax2.set_title('ç´¯è®¡æ”¶ç›Šç‡å¯¹æ¯” (%)')
        ax2.set_xlabel('æ—¥æœŸ')
        ax2.set_ylabel('ç´¯è®¡æ”¶ç›Šç‡ (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # --- å­å›¾3ï¼šæ—¥æ”¶ç›Šç‡åˆ†å¸ƒ ---
        ax3 = axes[1, 0]
        daily_returns = backtest_results['daily_returns'] * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”

        ax3.hist(daily_returns, bins=50, edgecolor='black', alpha=0.7, color='skyblue')
        ax3.axvline(daily_returns.mean(), color='red', linestyle='--', linewidth=2,
                    label=f'å‡å€¼: {daily_returns.mean():.2f}%')
        ax3.set_title('ç­–ç•¥æ—¥æ”¶ç›Šç‡åˆ†å¸ƒ')
        ax3.set_xlabel('æ—¥æ”¶ç›Šç‡ (%)')
        ax3.set_ylabel('é¢‘æ¬¡')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # --- å­å›¾4ï¼šæ»šåŠ¨æ”¶ç›Šç‡ï¼ˆ20æ—¥ï¼‰ ---
        ax4 = axes[1, 1]
        rolling_return = daily_returns.rolling(window=20).mean()
        ax4.plot(rolling_return.index, rolling_return.values, 'purple', linewidth=2)
        ax4.axhline(y=0, color='gray', linestyle='-', alpha=0.5)
        ax4.set_title('20æ—¥æ»šåŠ¨å¹³å‡æ”¶ç›Šç‡ (%)')
        ax4.set_xlabel('æ—¥æœŸ')
        ax4.set_ylabel('æ»šåŠ¨æ”¶ç›Šç‡ (%)')
        ax4.grid(True, alpha=0.3)

        # --- å­å›¾5ï¼šå›æ’¤æ›²çº¿ ---
        ax5 = axes[2, 0]
        running_max = backtest_results['strategy_cumulative_returns'].expanding().max()
        drawdown = (backtest_results['strategy_cumulative_returns'] - running_max) / running_max * 100

        ax5.fill_between(drawdown.index, drawdown.values, 0, alpha=0.3, color='red', label='å›æ’¤')
        ax5.set_title('ç­–ç•¥å›æ’¤æ›²çº¿')
        ax5.set_xlabel('æ—¥æœŸ')
        ax5.set_ylabel('å›æ’¤ (%)')
        ax5.legend()
        ax5.grid(True, alpha=0.3)

        # --- å­å›¾6ï¼šå…³é”®æŒ‡æ ‡è¡¨æ ¼ ---
        ax6 = axes[2, 1]
        ax6.axis('off')

        # å‡†å¤‡è¡¨æ ¼æ•°æ®
        metrics = [
            ['æŒ‡æ ‡', 'ç­–ç•¥', 'åŸºå‡†'],
            ['æ€»æ”¶ç›Šç‡', f"{backtest_results['total_return']:.2%}",
             f"{(backtest_results['benchmark_cumulative_returns'].iloc[-1] - 1):.2%}"],
            ['å¹´åŒ–æ”¶ç›Šç‡', f"{backtest_results['annualized_return']:.2%}", 'N/A'],
            ['å¹´åŒ–æ³¢åŠ¨ç‡', f"{backtest_results['annualized_volatility']:.2%}", 'N/A'],
            ['å¤æ™®æ¯”ç‡', f"{backtest_results['sharpe_ratio']:.2f}", 'N/A'],
            ['æœ€å¤§å›æ’¤', f"{backtest_results['max_drawdown']:.2%}", 'N/A'],
            ['äº¤æ˜“æ—¥æ•°', f"{backtest_results['duration_days']}", 'N/A'],
            ['é€‰è‚¡è®°å½•', f"{len(backtest_results['backtest_data'])}", 'N/A']
        ]

        table = ax6.table(cellText=metrics, loc='center', cellLoc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.5)

        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for i in range(len(metrics)):
            for j in range(len(metrics[0])):
                cell = table[(i, j)]
                if i == 0:  # æ ‡é¢˜è¡Œ
                    cell.set_facecolor('#4C72B0')
                    cell.set_text_props(weight='bold', color='white')
                elif i % 2 == 1:  # å¥‡æ•°è¡Œ
                    cell.set_facecolor('#E3E3E3')
                else:  # å¶æ•°è¡Œ
                    cell.set_facecolor('#FFFFFF')

        ax6.set_title('å…³é”®ç»©æ•ˆæŒ‡æ ‡')

        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"å›æµ‹å›¾è¡¨å·²ä¿å­˜: {save_path}")

        plt.show()

    except Exception as e:
        print(f"ç»˜åˆ¶å›æµ‹å›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


def simple_plot_backtest(backtest_results, timestamp):
    """
    ç®€å•çš„å›æµ‹å›¾è¡¨ç»˜åˆ¶ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
    """
    try:
        plt.figure(figsize=(12, 8))

        # å‡€å€¼æ›²çº¿
        plt.subplot(2, 2, 1)
        plt.plot(backtest_results['strategy_cumulative_returns'], label='ç­–ç•¥å‡€å€¼', linewidth=2)
        plt.plot(backtest_results['benchmark_cumulative_returns'], label='åŸºå‡†å‡€å€¼', linewidth=1.5, alpha=0.7)
        plt.title('å‡€å€¼æ›²çº¿å¯¹æ¯”')
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('å‡€å€¼')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # ç´¯è®¡æ”¶ç›Šç‡
        plt.subplot(2, 2, 2)
        strategy_return = (backtest_results['strategy_cumulative_returns'] - 1) * 100
        benchmark_return = (backtest_results['benchmark_cumulative_returns'] - 1) * 100
        plt.plot(strategy_return, label='ç­–ç•¥ç´¯è®¡æ”¶ç›Š (%)', linewidth=2, color='red')
        plt.plot(benchmark_return, label='åŸºå‡†ç´¯è®¡æ”¶ç›Š (%)', linewidth=1.5, color='blue', alpha=0.7)
        plt.title('ç´¯è®¡æ”¶ç›Šç‡å¯¹æ¯”')
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('æ”¶ç›Šç‡ (%)')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # æ—¥æ”¶ç›Šç‡åˆ†å¸ƒ
        plt.subplot(2, 2, 3)
        plt.hist(backtest_results['daily_returns'] * 100, bins=50, edgecolor='black', alpha=0.7)
        plt.title('æ—¥æ”¶ç›Šç‡åˆ†å¸ƒ')
        plt.xlabel('æ—¥æ”¶ç›Šç‡ (%)')
        plt.ylabel('é¢‘æ¬¡')
        plt.grid(True, alpha=0.3)

        # å…³é”®æŒ‡æ ‡æ–‡æœ¬
        plt.subplot(2, 2, 4)
        plt.axis('off')
        metrics_text = f"""å…³é”®æŒ‡æ ‡ï¼š
        æ€»æ”¶ç›Šç‡: {backtest_results['total_return']:.2%}
        å¹´åŒ–æ”¶ç›Šç‡: {backtest_results['annualized_return']:.2%}
        å¹´åŒ–æ³¢åŠ¨ç‡: {backtest_results['annualized_volatility']:.2%}
        å¤æ™®æ¯”ç‡: {backtest_results['sharpe_ratio']:.2f}
        æœ€å¤§å›æ’¤: {backtest_results['max_drawdown']:.2%}
        äº¤æ˜“æ—¥æ•°: {backtest_results['duration_days']}
        é€‰è‚¡è®°å½•: {len(backtest_results['backtest_data']):,}"""
        plt.text(0.1, 0.5, metrics_text, fontsize=10, verticalalignment='center')

        plt.tight_layout()
        simple_plot_file = f'backtest_simple_chart_{timestamp}.png'
        plt.savefig(simple_plot_file, dpi=300, bbox_inches='tight')
        plt.show()
        print(f"ç®€å•å›æµ‹å›¾è¡¨å·²ä¿å­˜: {simple_plot_file}")

    except Exception as e:
        print(f"ç®€å•å›¾è¡¨ç»˜åˆ¶å¤±è´¥: {e}")


@timer_decorator
def stratified_backtest(df, probabilities, feature_cols, model_name='rf'):
    """
    åˆ†å±‚å›æµ‹å‡½æ•°
    :param df: åŒ…å«æ—¥æœŸã€è‚¡ç¥¨ä»£ç ã€æœªæ¥æ”¶ç›Šç‡çš„DataFrame
    :param probabilities: æ¨¡å‹é¢„æµ‹æ¦‚ç‡
    :param feature_cols: ç‰¹å¾åˆ—
    :param model_name: æ¨¡å‹åç§°
    :return: åˆ†å±‚å›æµ‹ç»“æœ
    """
    print_section(f"åˆ†å±‚å›æµ‹ - {model_name.upper()}")

    # å‡†å¤‡æ•°æ®
    backtest_df = df.copy()
    backtest_df['prediction_prob'] = probabilities

    # ç¡®ä¿æœ‰æœªæ¥æ”¶ç›Šç‡
    if 'future_return' not in backtest_df.columns:
        print("é”™è¯¯ï¼šæ•°æ®ä¸­æ²¡æœ‰future_returnåˆ—")
        return None

    # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
    backtest_df = backtest_df.sort_values(['date', 'stock_code'])

    # è·å–æ‰€æœ‰å”¯ä¸€æ—¥æœŸ
    unique_dates = sorted(backtest_df['date'].unique())
    print(f"å›æµ‹æœŸé—´ï¼š{unique_dates[0]} åˆ° {unique_dates[-1]}")
    print(f"æ€»äº¤æ˜“æ—¥æ•°ï¼š{len(unique_dates)}")

    # åˆå§‹åŒ–åˆ†å±‚ç»“æœå­˜å‚¨
    strat_results = {
        'date': [],
        'layer': [],
        'num_stocks': [],
        'layer_return': [],
        'cumulative_return': [],
        'positions': []
    }

    # åˆå§‹åŒ–åˆ†å±‚å‡€å€¼æ›²çº¿
    layer_nav = {i: [1.0] for i in range(N_STRATIFICATION)}
    layer_dates = {i: [unique_dates[0]] for i in range(N_STRATIFICATION)}

    # ç”Ÿæˆè°ƒä»“æ—¥
    if REBALANCE_MONTHLY:
        # æŒ‰æœˆè°ƒä»“
        rebalance_dates = []
        current_month = None
        for date in unique_dates:
            if date.month != current_month:
                rebalance_dates.append(date)
                current_month = date.month
    else:
        # æŒ‰æœˆè°ƒä»“ï¼ˆæœˆåˆï¼‰
        rebalance_dates = [date for date in unique_dates
                           if date.day == REBALANCE_DAY or date == unique_dates[0]]

    print(f"è°ƒä»“æ—¥æ•°é‡ï¼š{len(rebalance_dates)}")

    # æ‰§è¡Œåˆ†å±‚å›æµ‹
    for i, rebalance_date in enumerate(rebalance_dates):
        if i >= len(rebalance_dates) - 1:
            break

        next_rebalance_idx = i + 1
        if next_rebalance_idx >= len(rebalance_dates):
            break

        next_rebalance_date = rebalance_dates[next_rebalance_idx]

        # è·å–è°ƒä»“æ—¥æ•°æ®
        daily_data = backtest_df[backtest_df['date'] == rebalance_date].copy()

        if len(daily_data) < N_STRATIFICATION:
            print(f"æ—¥æœŸ {rebalance_date} è‚¡ç¥¨æ•°é‡ä¸è¶³ï¼Œè·³è¿‡")
            continue

        # æŒ‰é¢„æµ‹æ¦‚ç‡æ’åºå¹¶åˆ†å±‚
        daily_data = daily_data.sort_values('prediction_prob', ascending=False)
        daily_data['layer'] = pd.qcut(
            daily_data['prediction_prob'],
            q=N_STRATIFICATION,
            labels=False,
            duplicates='drop'
        )

        # è®¡ç®—æ¯å±‚æ”¶ç›Š
        for layer in range(N_STRATIFICATION):
            layer_stocks = daily_data[daily_data['layer'] == layer]

            if len(layer_stocks) == 0:
                continue

            # è·å–è¿™äº›è‚¡ç¥¨åœ¨æŒæœ‰æœŸçš„æ”¶ç›Š
            stock_codes = layer_stocks['stock_code'].tolist()
            hold_period_data = backtest_df[
                (backtest_df['date'] >= rebalance_date) &
                (backtest_df['date'] < next_rebalance_date) &
                (backtest_df['stock_code'].isin(stock_codes))
                ]

            if len(hold_period_data) == 0:
                continue

            # è®¡ç®—æ¯æ—¥ç­‰æƒæ”¶ç›Šï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            daily_returns = hold_period_data.groupby('date')['future_return'].mean()

            # ç´¯è®¡æŒæœ‰æœŸæ”¶ç›Š
            if len(daily_returns) > 0:
                period_return = (1 + daily_returns).prod() - 1

                # æ›´æ–°å‡€å€¼
                if layer in layer_nav:
                    current_nav = layer_nav[layer][-1]
                    new_nav = current_nav * (1 + period_return)
                    layer_nav[layer].append(new_nav)
                    layer_dates[layer].append(next_rebalance_date)

                # å­˜å‚¨ç»“æœ
                strat_results['date'].append(rebalance_date)
                strat_results['layer'].append(layer)
                strat_results['num_stocks'].append(len(layer_stocks))
                strat_results['layer_return'].append(period_return)
                strat_results['positions'].append(stock_codes)

    # è®¡ç®—æ¯å±‚ç´¯è®¡æ”¶ç›Š
    strat_results_df = pd.DataFrame(strat_results)

    if strat_results_df.empty:
        print("åˆ†å±‚å›æµ‹ç»“æœä¸ºç©º")
        return None

    # è®¡ç®—æ¯å±‚çš„ç´¯è®¡æ”¶ç›Šç‡
    cumulative_returns = {}
    for layer in range(N_STRATIFICATION):
        layer_data = strat_results_df[strat_results_df['layer'] == layer]
        if not layer_data.empty:
            cumulative_returns[layer] = (1 + layer_data['layer_return']).prod() - 1

    # è®¡ç®—æ ¸å¿ƒéªŒè¯æŒ‡æ ‡
    validation_metrics = calculate_stratification_metrics(
        strat_results_df, cumulative_returns, layer_nav, layer_dates
    )

    # ç»˜åˆ¶åˆ†å±‚å›æµ‹å›¾è¡¨
    plot_stratified_backtest(layer_nav, layer_dates, validation_metrics, model_name)

    return {
        'stratified_results': strat_results_df,
        'layer_nav': layer_nav,
        'layer_dates': layer_dates,
        'validation_metrics': validation_metrics
    }


def calculate_stratification_metrics(strat_results_df, cumulative_returns, layer_nav, layer_dates):
    """
    è®¡ç®—åˆ†å±‚å›æµ‹éªŒè¯æŒ‡æ ‡
    """
    metrics = {}

    # 1. Topå±‚ vs Bottomå±‚æ”¶ç›Šç‡å·®
    if 0 in cumulative_returns and (N_STRATIFICATION - 1) in cumulative_returns:
        top_bottom_spread = cumulative_returns[0] - cumulative_returns[N_STRATIFICATION - 1]
        metrics['top_bottom_spread'] = top_bottom_spread
        metrics['top_bottom_spread_pct'] = f"{top_bottom_spread:.2%}"

        # æ£€éªŒæ˜¯å¦æ˜¾è‘—>0
        from scipy import stats
        top_returns = strat_results_df[strat_results_df['layer'] == 0]['layer_return']
        bottom_returns = strat_results_df[strat_results_df['layer'] == N_STRATIFICATION - 1]['layer_return']

        if len(top_returns) > 1 and len(bottom_returns) > 1:
            t_stat, p_value = stats.ttest_ind(top_returns, bottom_returns, equal_var=False)
            metrics['top_bottom_t_stat'] = t_stat
            metrics['top_bottom_p_value'] = p_value
            metrics['top_bottom_significant'] = p_value < 0.05

    # 2. åˆ†å±‚å•è°ƒæ€§æ£€éªŒ
    layer_avg_returns = []
    layers = []
    for layer in range(N_STRATIFICATION):
        if layer in cumulative_returns:
            layer_avg_returns.append(cumulative_returns[layer])
            layers.append(layer)

    if len(layer_avg_returns) >= 3:
        from scipy.stats import spearmanr
        monotonicity, monotonicity_p = spearmanr(layers, layer_avg_returns)
        metrics['monotonicity'] = monotonicity
        metrics['monotonicity_p_value'] = monotonicity_p
        metrics['monotonicity_passed'] = monotonicity > MONOTONICITY_THRESHOLD

    # 3. Topå±‚å¹´åŒ–æ”¶ç›Šç‡å’Œå¤æ™®æ¯”ç‡
    if 0 in layer_nav and len(layer_nav[0]) > 1:
        nav_series = pd.Series(layer_nav[0], index=layer_dates[0])
        # è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡
        total_return = nav_series.iloc[-1] / nav_series.iloc[0] - 1
        days = (nav_series.index[-1] - nav_series.index[0]).days
        annualized_return = (1 + total_return) ** (365 / days) - 1 if days > 0 else 0
        metrics['top_annualized_return'] = annualized_return

        # è®¡ç®—æ—¥æ”¶ç›Šç‡
        daily_returns = nav_series.pct_change().dropna()
        if len(daily_returns) > 0:
            # å¹´åŒ–æ³¢åŠ¨ç‡
            annualized_vol = daily_returns.std() * np.sqrt(252)
            # å¤æ™®æ¯”ç‡ï¼ˆå‡è®¾æ— é£é™©åˆ©ç‡ä¸º0ï¼‰
            sharpe_ratio = annualized_return / annualized_vol if annualized_vol != 0 else 0
            metrics['top_sharpe_ratio'] = sharpe_ratio
            metrics['top_sharpe_passed'] = sharpe_ratio > SHARPE_THRESHOLD

    # 4. Topå±‚ vs å¸‚åœºåŸºå‡†
    # è¿™é‡Œå¸‚åœºåŸºå‡†å¯ä»¥ä½¿ç”¨æ‰€æœ‰è‚¡ç¥¨ç­‰æƒç»„åˆ
    all_stocks_return = (1 + strat_results_df['layer_return']).prod() - 1
    metrics['market_avg_return'] = all_stocks_return
    metrics['top_vs_market'] = metrics.get('top_annualized_return', 0) - all_stocks_return

    return metrics


def plot_stratified_backtest(layer_nav, layer_dates, validation_metrics, model_name):
    """
    ç»˜åˆ¶åˆ†å±‚å›æµ‹å›¾è¡¨
    """
    try:
        plt.figure(figsize=(15, 10))

        # å­å›¾1ï¼šåˆ†å±‚å‡€å€¼æ›²çº¿
        plt.subplot(2, 2, 1)
        for layer in sorted(layer_nav.keys()):
            if len(layer_nav[layer]) > 1:
                plt.plot(layer_dates[layer], layer_nav[layer],
                         label=f'Layer {layer} (Top)' if layer == 0 else f'Layer {layer}',
                         linewidth=2 if layer == 0 else 1)

        plt.title(f'Stratified Backtest - {model_name.upper()}', fontsize=14)
        plt.xlabel('Date')
        plt.ylabel('Net Asset Value')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # å­å›¾2ï¼šåˆ†å±‚å¹³å‡æ”¶ç›Šç‡æŸ±çŠ¶å›¾
        plt.subplot(2, 2, 2)
        layers = []
        returns = []
        for layer in sorted(layer_nav.keys()):
            if len(layer_nav[layer]) > 1:
                total_return = layer_nav[layer][-1] / layer_nav[layer][0] - 1
                layers.append(layer)
                returns.append(total_return)

        colors = ['green' if i == 0 else 'red' if i == len(layers) - 1 else 'blue' for i in range(len(layers))]
        bars = plt.bar([f'Layer {l}' for l in layers], returns, color=colors)
        plt.title('Layer Cumulative Returns')
        plt.ylabel('Cumulative Return')
        plt.xticks(rotation=45)

        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, ret in zip(bars, returns):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width() / 2., height,
                     f'{ret:.2%}', ha='center', va='bottom')

        # å­å›¾3ï¼šéªŒè¯æŒ‡æ ‡è¡¨æ ¼
        plt.subplot(2, 2, 3)
        plt.axis('off')

        # å‡†å¤‡è¡¨æ ¼æ•°æ®
        table_data = []
        if 'top_bottom_spread_pct' in validation_metrics:
            significance = 'âœ“' if validation_metrics.get('top_bottom_significant', False) else 'âœ—'
            table_data.append(['Top-Bottom Spread', validation_metrics['top_bottom_spread_pct'], significance])

        if 'monotonicity' in validation_metrics:
            passed = 'âœ“' if validation_metrics.get('monotonicity_passed', False) else 'âœ—'
            table_data.append(['Monotonicity', f"{validation_metrics['monotonicity']:.3f}", passed])

        if 'top_annualized_return' in validation_metrics:
            table_data.append(['Top Annualized Return', f"{validation_metrics['top_annualized_return']:.2%}", ''])

        if 'top_sharpe_ratio' in validation_metrics:
            passed = 'âœ“' if validation_metrics.get('top_sharpe_passed', False) else 'âœ—'
            table_data.append(['Top Sharpe Ratio', f"{validation_metrics['top_sharpe_ratio']:.3f}", passed])

        if 'market_avg_return' in validation_metrics:
            table_data.append(['Market Avg Return', f"{validation_metrics['market_avg_return']:.2%}", ''])

        # åˆ›å»ºè¡¨æ ¼
        if table_data:
            table = plt.table(cellText=table_data,
                              colLabels=['Metric', 'Value', 'Passed'],
                              loc='center',
                              cellLoc='center',
                              colWidths=[0.3, 0.2, 0.1])
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1, 1.5)

        # å­å›¾4ï¼šå•è°ƒæ€§æ£€éªŒæ•£ç‚¹å›¾
        plt.subplot(2, 2, 4)
        layers = []
        returns = []
        for layer in sorted(layer_nav.keys()):
            if len(layer_nav[layer]) > 1:
                total_return = layer_nav[layer][-1] / layer_nav[layer][0] - 1
                layers.append(layer)
                returns.append(total_return)

        if len(layers) >= 3:
            plt.scatter(layers, returns, s=100, alpha=0.7)

            # æ·»åŠ è¶‹åŠ¿çº¿
            z = np.polyfit(layers, returns, 1)
            p = np.poly1d(z)
            plt.plot(layers, p(layers), "r--", alpha=0.5)

            # è®¡ç®—RÂ²
            from scipy.stats import linregress
            slope, intercept, r_value, p_value, std_err = linregress(layers, returns)

            plt.title(f'Monotonicity Test (RÂ²={r_value ** 2:.3f})')
            plt.xlabel('Layer (0=Top, 4=Bottom)')
            plt.ylabel('Cumulative Return')
            plt.grid(True, alpha=0.3)

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_file = f'stratified_backtest_{model_name}_{timestamp}.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        print(f"åˆ†å±‚å›æµ‹å›¾è¡¨å·²ä¿å­˜: {plot_file}")
        plt.show()

    except Exception as e:
        print(f"ç»˜åˆ¶åˆ†å±‚å›æµ‹å›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


def perform_stratified_backtest_all_models(test_df, predictions, probabilities, feature_cols):
    """
    å¯¹æ‰€æœ‰æ¨¡å‹æ‰§è¡Œåˆ†å±‚å›æµ‹
    """
    if not STRATIFIED_BACKTEST:
        return {}

    print_section("æ‰§è¡Œåˆ†å±‚å›æµ‹")

    stratified_results = {}

    for model_name in probabilities.keys():
        if model_name in probabilities and len(probabilities[model_name]) == len(test_df):
            print(f"\nå¯¹æ¨¡å‹ {model_name.upper()} æ‰§è¡Œåˆ†å±‚å›æµ‹...")

            try:
                result = stratified_backtest(
                    test_df,
                    probabilities[model_name],
                    feature_cols,
                    model_name
                )

                if result:
                    stratified_results[model_name] = result

                    # è¾“å‡ºéªŒè¯ç»“æœ
                    print(f"\n{model_name.upper()} åˆ†å±‚å›æµ‹éªŒè¯ç»“æœ:")
                    print("-" * 50)

                    metrics = result['validation_metrics']

                    if 'top_bottom_spread_pct' in metrics:
                        print(f"Top-Bottom Spread: {metrics['top_bottom_spread_pct']}")
                        if 'top_bottom_significant' in metrics:
                            status = "âœ“ æ˜¾è‘—" if metrics['top_bottom_significant'] else "âœ— ä¸æ˜¾è‘—"
                            print(f"æ˜¾è‘—æ€§æ£€éªŒ: {status}")

                    if 'monotonicity' in metrics:
                        print(f"å•è°ƒæ€§ (Spearman): {metrics['monotonicity']:.3f}")
                        if 'monotonicity_passed' in metrics:
                            status = "âœ“ é€šè¿‡" if metrics['monotonicity_passed'] else "âœ— æœªé€šè¿‡"
                            print(f"å•è°ƒæ€§æ£€éªŒ: {status}")

                    if 'top_annualized_return' in metrics:
                        print(f"Topå±‚å¹´åŒ–æ”¶ç›Š: {metrics['top_annualized_return']:.2%}")

                    if 'top_sharpe_ratio' in metrics:
                        print(f"Topå±‚å¤æ™®æ¯”ç‡: {metrics['top_sharpe_ratio']:.3f}")
                        if 'top_sharpe_passed' in metrics:
                            status = "âœ“ é€šè¿‡" if metrics['top_sharpe_passed'] else "âœ— æœªé€šè¿‡"
                            print(f"å¤æ™®æ¯”ç‡æ£€éªŒ: {status}")

                    if 'market_avg_return' in metrics:
                        print(f"å¸‚åœºå¹³å‡æ”¶ç›Š: {metrics['market_avg_return']:.2%}")

                    print("-" * 50)

            except Exception as e:
                print(f"æ¨¡å‹ {model_name} åˆ†å±‚å›æµ‹å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

    return stratified_results
@timer_decorator
def select_stocks_with_lightgbm_unified(X_train, y_train, X_test, test_df, feature_cols, top_percent=0.4, top_k=20):
    """
    ä½¿ç”¨LightGBMç­›é€‰è‚¡ç¥¨ï¼Œä¿æŒä¸å‰40%é€»è¾‘ä¸€è‡´
    :param top_percent: é€‰æ‹©å‰ç™¾åˆ†ä¹‹å¤šå°‘çš„è‚¡ç¥¨ï¼ˆé»˜è®¤40%ï¼‰
    :param top_k: æœ€å¤šé€‰æ‹©çš„è‚¡ç¥¨æ•°é‡
    """
    print_section("LightGBMé€‰è‚¡ï¼ˆä¿æŒå‰40%é€»è¾‘ï¼‰")

    # 1. æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 2. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆä¸å‰40%æ ‡ç­¾åŒ¹é…ï¼‰
    # æ³¨æ„ï¼šSMOTEçš„sampling_strategyåº”è¯¥æ ¹æ®æ­£æ ·æœ¬æ¯”ä¾‹è°ƒæ•´
    pos_ratio = y_train.mean()
    sampling_strategy = min(0.8, (0.4 / pos_ratio) if pos_ratio > 0 else 0.8)
    smote = SMOTE(random_state=RANDOM_STATE, sampling_strategy=sampling_strategy)
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

    # 3. è®­ç»ƒLightGBM
    print("è®­ç»ƒLightGBMé€‰è‚¡æ¨¡å‹...")
    lgb_model = lgb.LGBMClassifier(
        n_estimators=100,
        max_depth=7,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=RANDOM_STATE,
        n_jobs=N_JOBS,
        verbosity=-1
    )

    # ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯
    if USE_ROLLING_CV:
        from sklearn.model_selection import TimeSeriesSplit
        tscv = TimeSeriesSplit(n_splits=ROLLING_CV_SPLITS)

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled), 1):
            X_fold_train = X_train_scaled[train_idx]
            y_fold_train = y_train.iloc[train_idx] if hasattr(y_train, 'iloc') else y_train[train_idx]

            # é‡æ–°å¹³è¡¡
            smote_fold = SMOTE(random_state=RANDOM_STATE + fold, sampling_strategy=sampling_strategy)
            X_fold_train_bal, y_fold_train_bal = smote_fold.fit_resample(X_fold_train, y_fold_train)

            lgb_model.fit(
                X_fold_train_bal, y_fold_train_bal,
                eval_set=[(X_train_scaled[val_idx], y_train.iloc[val_idx])],
                eval_metric='binary_logloss',
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
            )

    # 4. é¢„æµ‹æ‰€æœ‰è‚¡ç¥¨
    y_proba = lgb_model.predict_proba(X_test_scaled)[:, 1]

    # 5. åº”ç”¨å‰40%é€»è¾‘ï¼šæŒ‰æ¦‚ç‡æ’åºï¼Œå–å‰40%æˆ–æœ€å¤štop_kåª
    results = pd.DataFrame({
        'è‚¡ç¥¨ä»£ç ': test_df['stock_code'].values,
        'æ”¶ç›˜ä»·': test_df['close'].values,
        'é¢„æµ‹æ¦‚ç‡': y_proba
    })

    # æ’åº
    results_sorted = results.sort_values('é¢„æµ‹æ¦‚ç‡', ascending=False)

    # è®¡ç®—åº”é€‰è‚¡ç¥¨æ•°é‡ï¼šå‰40%ï¼Œä½†ä¸è¶…è¿‡top_k
    n_stocks = len(results_sorted)
    n_select = min(int(n_stocks * top_percent), top_k)
    n_select = max(n_select, 1)  # è‡³å°‘é€‰1åª

    top_stocks = results_sorted.head(n_select).copy()
    top_stocks['æ’å'] = range(1, len(top_stocks) + 1)

    print(f"âœ… LightGBMé€‰è‚¡å®Œæˆï¼ˆå‰{top_percent:.0%}é€»è¾‘ï¼‰")
    print(f"  æ€»è‚¡ç¥¨æ•°: {n_stocks}åª")
    print(f"  åº”é€‰å‰{top_percent:.0%}: {int(n_stocks * top_percent)}åª")
    print(f"  å®é™…é€‰æ‹©: {n_select}åªï¼ˆæœ€å¤š{top_k}åªï¼‰")
    print(f"  æ¦‚ç‡é˜ˆå€¼: {top_stocks['é¢„æµ‹æ¦‚ç‡'].min():.3f}")

    return top_stocks, lgb_model, scaler

@timer_decorator
def calculate_transaction_costs(trade_value, is_buy=True):
    """
    è®¡ç®—äº¤æ˜“æˆæœ¬
    :param trade_value: äº¤æ˜“é‡‘é¢
    :param is_buy: æ˜¯å¦ä¹°å…¥ï¼ˆTrue:ä¹°å…¥, False:å–å‡ºï¼‰
    :return: äº¤æ˜“æˆæœ¬
    """
    commission = trade_value * TRANSACTION_COSTS['commission']
    tax = 0
    if not is_buy:  # å–å‡ºæ—¶å¾æ”¶è¯äº¤ç¨
        tax = trade_value * TRANSACTION_COSTS['tax']
    slippage = trade_value * TRANSACTION_COSTS['slippage']

    total_cost = commission + tax + slippage
    return total_cost


# ==================== ä¿®å¤RiskControlManagerç±» ====================
class RiskControlManager:
    """é£æ§ç®¡ç†å™¨ - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self):
        self.portfolio_value = INITIAL_CAPITAL
        self.cash = INITIAL_CAPITAL  # æ·»åŠ ç°é‡‘è¿½è¸ª
        self.positions = {}  # æŒä»“å­—å…¸ {è‚¡ç¥¨ä»£ç : æŒä»“ä¿¡æ¯}
        self.trading_records = []  # äº¤æ˜“è®°å½•
        self.daily_portfolio_values = []  # æ¯æ—¥ç»„åˆå‡€å€¼
        self.max_portfolio_value = INITIAL_CAPITAL  # æœ€é«˜ç»„åˆå‡€å€¼ï¼ˆç”¨äºè®¡ç®—å›æ’¤ï¼‰
        self.trade_counts = {'buy': 0, 'sell': 0}  # äº¤æ˜“è®¡æ•°

        # æ·»åŠ æŒä»“è¿½è¸ª
        self.position_days = {}  # è‚¡ç¥¨æŒä»“å¤©æ•° {è‚¡ç¥¨ä»£ç : æŒä»“å¤©æ•°}
        self.holdings_history = []  # æŒä»“å†å²è®°å½•

    def check_single_stock_limit(self, stock_code, target_weight):
        """æ£€æŸ¥å•åªè‚¡ç¥¨ä»“ä½é™åˆ¶"""
        if target_weight > RISK_CONTROL['single_stock_limit']:
            print(
                f"âš ï¸ è‚¡ç¥¨{stock_code}ç›®æ ‡æƒé‡{target_weight:.2%}è¶…è¿‡å•åªè‚¡ç¥¨ä¸Šé™{RISK_CONTROL['single_stock_limit']:.2%}")
            return RISK_CONTROL['single_stock_limit']
        return target_weight

    def update_holding_days(self, current_date):
        """æ›´æ–°æŒä»“å¤©æ•°"""
        for stock_code in list(self.position_days.keys()):
            if stock_code in self.positions:
                self.position_days[stock_code] += 1
            else:
                # ç§»é™¤å·²æ¸…ä»“çš„è‚¡ç¥¨
                del self.position_days[stock_code]

    def record_holding_history(self, current_date):
        """è®°å½•æŒä»“å†å²"""
        total_value = self.cash
        positions_summary = {}

        for stock_code, position in self.positions.items():
            if 'current_price' in position and position['current_price'] > 0:
                position_value = position['shares'] * position['current_price']
                total_value += position_value
                positions_summary[stock_code] = {
                    'shares': position['shares'],
                    'price': position['current_price'],
                    'value': position_value,
                    'weight': position_value / total_value if total_value > 0 else 0
                }

        self.holdings_history.append({
            'date': current_date,
            'cash': self.cash,
            'total_value': total_value,
            'positions': positions_summary,
            'hold_days': dict(self.position_days)
        })

    def check_stop_loss_profit(self, stock_code, current_price, purchase_price, hold_days):
        """æ£€æŸ¥ä¸ªè‚¡æ­¢æŸæ­¢ç›ˆï¼ˆè€ƒè™‘æŒæœ‰æœŸï¼‰"""
        if purchase_price <= 0 or hold_days < 30:  # æŒæœ‰ä¸è¶³30å¤©ï¼Œä¸è§¦å‘æ­¢æŸæ­¢ç›ˆ
            return None

        return_rate = (current_price - purchase_price) / purchase_price

        # æ ¹æ®æŒæœ‰æœŸè°ƒæ•´æ­¢æŸæ­¢ç›ˆé˜ˆå€¼
        if hold_days < 60:  # æŒæœ‰30-60å¤©
            stop_loss_threshold = RISK_CONTROL['individual_stop_loss'] * 0.5  # æ”¾å®½æ­¢æŸ
            stop_profit_threshold = RISK_CONTROL['individual_stop_profit'] * 1.5  # æé«˜æ­¢ç›ˆ
        else:  # æŒæœ‰è¶…è¿‡60å¤©
            stop_loss_threshold = RISK_CONTROL['individual_stop_loss'] * 1.5  # è¿›ä¸€æ­¥æ”¾å®½
            stop_profit_threshold = RISK_CONTROL['individual_stop_profit'] * 2.0  # è¿›ä¸€æ­¥æé«˜

        if return_rate <= stop_loss_threshold:
            return 'stop_loss'
        elif return_rate >= stop_profit_threshold:
            return 'stop_profit'

        return None

    def check_portfolio_drawdown(self, current_value):
        """æ£€æŸ¥ç»„åˆå›æ’¤"""
        self.max_portfolio_value = max(self.max_portfolio_value, current_value)
        drawdown = (current_value - self.max_portfolio_value) / self.max_portfolio_value

        if drawdown <= RISK_CONTROL['portfolio_stop_loss']:
            return True, drawdown
        return False, drawdown

    def reduce_positions(self, reduction_ratio=RISK_CONTROL['reduction_ratio']):
        """å‡ä»“æ“ä½œ"""
        print(f"è§¦å‘ç»„åˆæ­¢æŸï¼Œå‡ä»“{reduction_ratio:.0%}")
        stocks_to_sell = []

        # æŒ‰æ¯”ä¾‹å‡å°‘æ‰€æœ‰æŒä»“
        for stock_code in list(self.positions.keys()):
            position = self.positions[stock_code]
            reduce_shares = int(position['shares'] * reduction_ratio)

            if reduce_shares > 0 and 'current_price' in position:
                stocks_to_sell.append((stock_code, reduce_shares, position['current_price']))

        # æ‰§è¡Œå–å‡º
        for stock_code, shares, price in stocks_to_sell:
            self.execute_sell(stock_code, shares, price, None, 'portfolio_stop_loss')

    def execute_buy(self, stock_code, shares, price, date=None):
        """æ‰§è¡Œä¹°å…¥æ“ä½œ"""
        if shares <= 0 or price <= 0:
            return 0

        trade_value = shares * price
        cost = calculate_transaction_costs(trade_value, is_buy=True)
        net_value = trade_value + cost

        if net_value > self.cash:
            # èµ„é‡‘ä¸è¶³ï¼Œè°ƒæ•´ä¹°å…¥æ•°é‡
            max_shares = int((self.cash - cost) / price)
            if max_shares <= 0:
                print(f"èµ„é‡‘ä¸è¶³è´­ä¹°{stock_code}ï¼Œç°é‡‘{self.cash:.2f}ï¼Œéœ€è¦{net_value:.2f}")
                return 0

            shares = max_shares
            trade_value = shares * price
            cost = calculate_transaction_costs(trade_value, is_buy=True)
            net_value = trade_value + cost

        # æ›´æ–°æŒä»“
        if stock_code in self.positions:
            self.positions[stock_code]['shares'] += shares
            total_shares = self.positions[stock_code]['shares']
            total_cost = (self.positions[stock_code]['avg_price'] *
                          (total_shares - shares) + price * shares)
            self.positions[stock_code]['avg_price'] = total_cost / total_shares
            self.positions[stock_code]['last_buy_date'] = date
        else:
            self.positions[stock_code] = {
                'shares': shares,
                'avg_price': price,
                'current_price': price,
                'last_buy_date': date,
                'first_buy_date': date
            }
            self.position_days[stock_code] = 1

        # æ›´æ–°èµ„é‡‘
        self.cash -= net_value

        # è®°å½•äº¤æ˜“
        self.trading_records.append({
            'date': date,
            'type': 'buy',
            'stock_code': stock_code,
            'shares': shares,
            'price': price,
            'cost': cost,
            'total_value': trade_value,
            'cash_after': self.cash
        })

        self.trade_counts['buy'] += 1

        print(f"ä¹°å…¥ {stock_code}: {shares}è‚¡ @ {price:.2f}, æˆæœ¬{cost:.2f}, ç°é‡‘å‰©ä½™{self.cash:.2f}")
        return shares

    def execute_sell(self, stock_code, shares, price, date=None, reason='normal'):
        """æ‰§è¡Œå–å‡ºæ“ä½œ"""
        if stock_code not in self.positions:
            return 0

        position = self.positions[stock_code]
        actual_shares = min(shares, position['shares'])

        if actual_shares <= 0 or price <= 0:
            return 0

        trade_value = actual_shares * price
        cost = calculate_transaction_costs(trade_value, is_buy=False)
        net_value = trade_value - cost

        # æ›´æ–°æŒä»“
        position['shares'] -= actual_shares
        if position['shares'] <= 0:
            del self.positions[stock_code]
            if stock_code in self.position_days:
                del self.position_days[stock_code]

        # æ›´æ–°èµ„é‡‘
        self.cash += net_value

        # è®¡ç®—ç›ˆäº
        purchase_value = actual_shares * position['avg_price']
        profit = net_value - purchase_value
        return_rate = profit / purchase_value if purchase_value > 0 else 0

        # è®°å½•äº¤æ˜“
        self.trading_records.append({
            'date': date,
            'type': 'sell',
            'stock_code': stock_code,
            'shares': actual_shares,
            'price': price,
            'cost': cost,
            'total_value': trade_value,
            'profit': profit,
            'return_rate': return_rate,
            'reason': reason,
            'cash_after': self.cash
        })

        self.trade_counts['sell'] += 1

        print(f"å–å‡º {stock_code}: {actual_shares}è‚¡ @ {price:.2f}, ç›ˆäº{profit:.2f}, ç°é‡‘{self.cash:.2f}")
        return actual_shares

    def update_positions(self, price_dict, date):
        """æ›´æ–°æŒä»“å¸‚å€¼"""
        total_value = self.cash  # ä»ç°é‡‘å¼€å§‹è®¡ç®—

        for stock_code, position in self.positions.items():
            # è·å–å½“å‰ä»·æ ¼
            if stock_code in price_dict and date in price_dict[stock_code]:
                current_price = price_dict[stock_code][date]
            else:
                # ä½¿ç”¨æœ€åå·²çŸ¥ä»·æ ¼
                current_price = position.get('current_price', position['avg_price'])

            position['current_price'] = current_price
            position_value = position['shares'] * current_price
            total_value += position_value

            # æ£€æŸ¥ä¸ªè‚¡æ­¢æŸæ­¢ç›ˆ
            stop_signal = self.check_stop_loss_profit(
                stock_code,
                current_price,
                position['avg_price']
            )
            if stop_signal:
                print(f"è‚¡ç¥¨{stock_code}è§¦å‘{stop_signal}ï¼Œå½“å‰ä»·æ ¼{current_price:.2f}ï¼Œæˆæœ¬{position['avg_price']:.2f}")
                self.execute_sell(
                    stock_code,
                    position['shares'],
                    current_price,
                    date,
                    stop_signal
                )

        # æ›´æ–°æŒä»“å¤©æ•°
        self.update_holding_days(date)

        # è®°å½•æŒä»“å†å²
        self.record_holding_history(date)

        return total_value


@timer_decorator
def generate_final_output(feature_cols, model_results, backtest_results, core_factors=None):
    """
    ç”Ÿæˆæœ€ç»ˆè¾“å‡ºç»“æœ
    """
    print_section("æ­¥éª¤6ï¼šç»“æœè¾“å‡ºä¸è¿­ä»£ä¼˜åŒ–")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 1. æœ€ç»ˆå› å­åˆ—è¡¨
    final_factors = {
        'final_factors': feature_cols,
        'financial_factors': [f for f in feature_cols if f.startswith('fin_')],
        'technical_factors': [f for f in feature_cols if not f.startswith('fin_')],
        'core_factors': core_factors if core_factors else feature_cols[:10]
    }

    # ä¿å­˜å› å­åˆ—è¡¨
    factors_file = f'final_factors_list_{timestamp}.json'
    with open(factors_file, 'w', encoding='utf-8') as f:
        json.dump(final_factors, f, indent=2, ensure_ascii=False)
    print(f"âœ… æœ€ç»ˆå› å­åˆ—è¡¨å·²ä¿å­˜: {factors_file}")

    # 2. æ¨¡å‹æ€§èƒ½è¾“å‡º
    model_performance = {
        'training_date': timestamp,
        'models': {}
    }

    for model_name, results in model_results.items():
        model_performance['models'][model_name] = {
            'accuracy': results.get('test_accuracy', 0),
            'precision': results.get('test_precision', 0),
            'recall': results.get('test_recall', 0),
            'f1_score': results.get('test_f1', 0),
            'roc_auc': results.get('test_roc_auc', 0)
        }

    # ä¿å­˜æ¨¡å‹æ€§èƒ½
    performance_file = f'model_performance_{timestamp}.json'
    with open(performance_file, 'w', encoding='utf-8') as f:
        json.dump(model_performance, f, indent=2)
    print(f"âœ… æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜: {performance_file}")

    # 3. å›æµ‹ç»“æœè¾“å‡º
    if backtest_results:
        backtest_summary = {
            'backtest_period': {
                'start_date': str(backtest_results['portfolio_values'][0]['date']),
                'end_date': str(backtest_results['portfolio_values'][-1]['date']),
                'days': backtest_results['metrics'].get('å›æµ‹å¤©æ•°', 0)
            },
            'performance_metrics': {
                k: (f"{v:.2%}" if isinstance(v, float) and k.endswith('ç‡') else
                    f"{v:.2f}" if isinstance(v, float) else v)
                for k, v in backtest_results['metrics'].items()
            },
            'trading_statistics': backtest_results['trading_stats'],
            'report': backtest_results.get('report', {})
        }

        # ä¿å­˜å›æµ‹ç»“æœ
        backtest_file = f'backtest_results_detailed_{timestamp}.json'
        with open(backtest_file, 'w', encoding='utf-8') as f:
            json.dump(backtest_summary, f, indent=2, ensure_ascii=False)
        print(f"âœ… è¯¦ç»†å›æµ‹ç»“æœå·²ä¿å­˜: {backtest_file}")

        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_report = generate_html_report(final_factors, model_performance, backtest_summary)
        html_file = f'final_report_{timestamp}.html'
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_report)
        print(f"âœ… HTMLç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {html_file}")

    # 4. è¿­ä»£ä¼˜åŒ–å»ºè®®
    optimization_suggestions = generate_optimization_suggestions(
        final_factors, model_performance, backtest_results
    )

    suggestions_file = f'optimization_suggestions_{timestamp}.txt'
    with open(suggestions_file, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("è¿­ä»£ä¼˜åŒ–å»ºè®®\n")
        f.write("=" * 60 + "\n\n")
        for suggestion in optimization_suggestions:
            f.write(f"â€¢ {suggestion}\n")

    print(f"âœ… è¿­ä»£ä¼˜åŒ–å»ºè®®å·²ä¿å­˜: {suggestions_file}")

    return {
        'factors_file': factors_file,
        'performance_file': performance_file,
        'backtest_file': backtest_file if backtest_results else None,
        'html_file': html_file if backtest_results else None,
        'suggestions_file': suggestions_file
    }


def generate_html_report(factors, model_performance, backtest_summary):
    """ç”ŸæˆHTMLæ ¼å¼çš„ç»¼åˆæŠ¥å‘Š"""

    html_template = '''
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>è‚¡ç¥¨é€‰è‚¡ç­–ç•¥å›æµ‹æŠ¥å‘Š</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 40px; }}
            .section {{ margin-bottom: 30px; border: 1px solid #ddd; padding: 20px; border-radius: 5px; }}
            h1, h2, h3 {{ color: #333; }}
            table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
            th {{ background-color: #f2f2f2; }}
            .metric {{ margin: 5px 0; }}
            .pass {{ color: green; font-weight: bold; }}
            .fail {{ color: red; font-weight: bold; }}
            .recommendation {{ background-color: #fffacd; padding: 10px; margin: 10px 0; border-left: 4px solid #ffd700; }}
        </style>
    </head>
    <body>
        <h1>ğŸ“Š è‚¡ç¥¨é€‰è‚¡ç­–ç•¥å›æµ‹æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {timestamp}</p>

        <div class="section">
            <h2>1. ç­–ç•¥æ¦‚è§ˆ</h2>
            <p><strong>å›æµ‹æœŸé—´:</strong> {start_date} è‡³ {end_date} ({days} å¤©)</p>
            <p><strong>åˆå§‹èµ„é‡‘:</strong> {initial_capital:,.2f} å…ƒ</p>
        </div>

        <div class="section">
            <h2>2. å› å­é…ç½®</h2>
            <h3>æ ¸å¿ƒå› å­ ({core_count}ä¸ª)</h3>
            <ul>
                {core_factors_list}
            </ul>
            <h3>è´¢åŠ¡å› å­ ({financial_count}ä¸ª)</h3>
            <ul>
                {financial_factors_list}
            </ul>
            <h3>æŠ€æœ¯å› å­ ({technical_count}ä¸ª)</h3>
            <ul>
                {technical_factors_list}
            </ul>
        </div>

        <div class="section">
            <h2>3. æ¨¡å‹æ€§èƒ½</h2>
            <table>
                <tr>
                    <th>æ¨¡å‹</th>
                    <th>å‡†ç¡®ç‡</th>
                    <th>F1åˆ†æ•°</th>
                    <th>ROC-AUC</th>
                    <th>ç²¾ç¡®ç‡</th>
                    <th>å¬å›ç‡</th>
                </tr>
                {model_rows}
            </table>
        </div>

        <div class="section">
            <h2>4. å›æµ‹ç»©æ•ˆ</h2>
            <h3>4.1 æ ¸å¿ƒæŒ‡æ ‡</h3>
            <table>
                <tr>
                    <th>æŒ‡æ ‡</th>
                    <th>æ•°å€¼</th>
                    <th>ç›®æ ‡</th>
                    <th>çŠ¶æ€</th>
                </tr>
                {metric_rows}
            </table>

            <h3>4.2 äº¤æ˜“ç»Ÿè®¡</h3>
            <table>
                {trading_rows}
            </table>
        </div>

        <div class="section">
            <h2>5. é£æ§åˆè§„</h2>
            <table>
                {compliance_rows}
            </table>
        </div>

        <div class="section">
            <h2>6. ä¼˜åŒ–å»ºè®®</h2>
            {recommendations}
        </div>

        <div class="section">
            <h2>7. è¿­ä»£è®¡åˆ’</h2>
            <ol>
                <li>å¢åŠ æ›´å¤šè´¢åŠ¡æŒ‡æ ‡ï¼Œå¦‚ç°é‡‘æµé‡æ¯”ç‡ã€è¥è¿èµ„æœ¬ç­‰</li>
                <li>ä¼˜åŒ–æŠ€æœ¯å› å­å‚æ•°ï¼Œæµ‹è¯•ä¸åŒæ—¶é—´çª—å£</li>
                <li>å¼•å…¥å¸‚åœºæƒ…ç»ªå› å­å’Œèµ„é‡‘æµå› å­</li>
                <li>æµ‹è¯•ä¸åŒæœºå™¨å­¦ä¹ ç®—æ³•çš„ç»„åˆ</li>
                <li>ä¼˜åŒ–äº¤æ˜“æˆæœ¬æ¨¡å‹ï¼Œè€ƒè™‘å®é™…äº¤æ˜“é™åˆ¶</li>
            </ol>
        </div>
    </body>
    </html>
    '''

    # å‡†å¤‡æ•°æ®
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # å› å­åˆ—è¡¨
    core_factors_list = ''.join(f'<li>{factor}</li>' for factor in factors.get('core_factors', [])[:10])
    financial_factors_list = ''.join(f'<li>{factor}</li>' for factor in factors.get('financial_factors', [])[:5])
    technical_factors_list = ''.join(f'<li>{factor}</li>' for factor in factors.get('technical_factors', [])[:5])

    # æ¨¡å‹æ€§èƒ½è¡Œ
    model_rows = ''
    for model_name, metrics in model_performance.get('models', {}).items():
        model_rows += f'''
        <tr>
            <td>{model_name.upper()}</td>
            <td>{metrics.get('accuracy', 0):.2%}</td>
            <td>{metrics.get('f1_score', 0):.4f}</td>
            <td>{metrics.get('roc_auc', 0):.4f}</td>
            <td>{metrics.get('precision', 0):.4f}</td>
            <td>{metrics.get('recall', 0):.4f}</td>
        </tr>
        '''

    # ç»©æ•ˆæŒ‡æ ‡è¡Œ
    metric_rows = ''
    target_metrics = [
        ('å¹´åŒ–æ”¶ç›Šç‡', TARGET_METRICS['annual_return']),
        ('å¤æ™®æ¯”ç‡', TARGET_METRICS['sharpe_ratio']),
        ('æœ€å¤§å›æ’¤', TARGET_METRICS['max_drawdown'])
    ]

    for metric_name, target_value in target_metrics:
        actual_value = backtest_summary['performance_metrics'].get(metric_name, '0')
        # ============ ä¿®å¤ï¼šå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµ®ç‚¹æ•° ============
        actual_value_num = 0.0
        try:
            if isinstance(actual_value, str):
                # å¤„ç†ç™¾åˆ†æ¯”å­—ç¬¦ä¸²
                if '%' in actual_value:
                    # ç§»é™¤ç™¾åˆ†å·å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                    actual_value_num = float(actual_value.replace('%', '')) / 100.0
                elif ':' in actual_value:
                    # å¤„ç†å…¶ä»–æ ¼å¼ï¼Œæš‚æ—¶è®¾ä¸º0
                    actual_value_num = 0.0
                else:
                    # å°è¯•ç›´æ¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                    actual_value_num = float(actual_value)
            else:
                # å¦‚æœä¸æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                actual_value_num = float(actual_value)
        except (ValueError, TypeError) as e:
            print(f"è­¦å‘Šï¼šæ— æ³•è½¬æ¢æŒ‡æ ‡å€¼ '{actual_value}' ä¸ºæµ®ç‚¹æ•°: {e}")
            actual_value_num = 0.0
        # ============ ä¿®å¤ç»“æŸ ============

        # æ ¹æ®æŒ‡æ ‡ç±»å‹å†³å®šæ¯”è¾ƒæ–¹å¼
        if metric_name == 'æœ€å¤§å›æ’¤':
            # æœ€å¤§å›æ’¤æ˜¯è´Ÿæ•°ï¼Œæ¯”è¾ƒæ—¶å–ç»å¯¹å€¼
            actual_for_compare = abs(actual_value_num)
            target_for_compare = abs(target_value)
            status_class = 'pass' if actual_for_compare <= target_for_compare else 'fail'
            status_text = 'âœ“ è¾¾æ ‡' if actual_for_compare <= target_for_compare else 'âœ— æœªè¾¾æ ‡'
        else:
            # å…¶ä»–æŒ‡æ ‡ï¼šå®é™…å€¼ >= ç›®æ ‡å€¼
            status_class = 'pass' if actual_value_num >= target_value else 'fail'
            status_text = 'âœ“ è¾¾æ ‡' if actual_value_num >= target_value else 'âœ— æœªè¾¾æ ‡'

        # ============ ä¿®å¤ï¼šé¿å…åœ¨f-stringæ ¼å¼è¯´æ˜ç¬¦ä¸­ä½¿ç”¨æ¡ä»¶è¡¨è¾¾å¼ ============
        # å…ˆæ ¹æ®æŒ‡æ ‡åç§°å†³å®šç›®æ ‡å€¼çš„æ˜¾ç¤ºæ ¼å¼
        if metric_name != 'å¤æ™®æ¯”ç‡':
            target_str = f"{target_value:.2%}"
        else:
            target_str = f"{target_value:.2f}"

        metric_rows += f'''
         <tr>
             <td>{metric_name}</td>
             <td>{actual_value}</td>
             <td>{target_str}</td>
             <td class="{status_class}">{status_text}</td>
         </tr>
        '''
        # ============ ä¿®å¤ç»“æŸ ============

    # äº¤æ˜“ç»Ÿè®¡è¡Œ
    trading_rows = ''
    for key, value in backtest_summary.get('trading_statistics', {}).items():
        trading_rows += f'<tr><td>{key}</td><td>{value}</td></tr>'

    # åˆè§„æ£€æŸ¥è¡Œ
    compliance_rows = ''
    for key, value in backtest_summary.get('report', {}).get('compliance_check', {}).items():
        compliance_rows += f'<tr><td>{key}</td><td>{value}</td></tr>'

    # ä¼˜åŒ–å»ºè®®
    recommendations_html = ''
    for rec in backtest_summary.get('report', {}).get('recommendations', []):
        recommendations_html += f'<div class="recommendation">ğŸ“Œ {rec}</div>'

    # å¡«å……æ¨¡æ¿
    html_content = html_template.format(
        timestamp=timestamp,
        start_date=backtest_summary['backtest_period']['start_date'],
        end_date=backtest_summary['backtest_period']['end_date'],
        days=backtest_summary['backtest_period']['days'],
        initial_capital=INITIAL_CAPITAL,
        core_count=len(factors.get('core_factors', [])),
        core_factors_list=core_factors_list,
        financial_count=len(factors.get('financial_factors', [])),
        financial_factors_list=financial_factors_list,
        technical_count=len(factors.get('technical_factors', [])),
        technical_factors_list=technical_factors_list,
        model_rows=model_rows,
        metric_rows=metric_rows,
        trading_rows=trading_rows,
        compliance_rows=compliance_rows,
        recommendations=recommendations_html
    )

    return html_content


def plot_detailed_backtest_results(backtest_results, save_path=None):
    """ç»˜åˆ¶è¯¦ç»†å›æµ‹ç»“æœå›¾è¡¨"""

    try:
        fig = plt.figure(figsize=(18, 12))

        # å­å›¾1ï¼šå‡€å€¼æ›²çº¿
        ax1 = plt.subplot(3, 3, 1)
        portfolio_values = [pv['portfolio_value'] for pv in backtest_results['portfolio_values']]
        dates = [pv['date'] for pv in backtest_results['portfolio_values']]

        ax1.plot(dates, portfolio_values, 'b-', linewidth=2, label='ç­–ç•¥å‡€å€¼')
        ax1.set_title('å‡€å€¼æ›²çº¿', fontsize=12, fontweight='bold')
        ax1.set_xlabel('æ—¥æœŸ')
        ax1.set_ylabel('å‡€å€¼ï¼ˆå…ƒï¼‰')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # å­å›¾2ï¼šä»“ä½æ„æˆ
        ax2 = plt.subplot(3, 3, 2)
        cash_values = [pv['cash'] for pv in backtest_results['portfolio_values']]
        positions_values = [pv['positions_value'] for pv in backtest_results['portfolio_values']]

        ax2.stackplot(dates, cash_values, positions_values,
                      labels=['ç°é‡‘', 'æŒä»“'], alpha=0.7)
        ax2.set_title('ä»“ä½æ„æˆ', fontsize=12, fontweight='bold')
        ax2.set_xlabel('æ—¥æœŸ')
        ax2.set_ylabel('é‡‘é¢ï¼ˆå…ƒï¼‰')
        ax2.legend(loc='upper left')
        ax2.grid(True, alpha=0.3)

        # å­å›¾3ï¼šæ—¥æ”¶ç›Šç‡åˆ†å¸ƒ
        ax3 = plt.subplot(3, 3, 3)
        returns = backtest_results['portfolio_returns']
        ax3.hist(returns, bins=50, edgecolor='black', alpha=0.7, color='skyblue')
        ax3.axvline(np.mean(returns), color='red', linestyle='--',
                    label=f'å‡å€¼: {np.mean(returns):.2%}')
        ax3.set_title('æ—¥æ”¶ç›Šç‡åˆ†å¸ƒ', fontsize=12, fontweight='bold')
        ax3.set_xlabel('æ—¥æ”¶ç›Šç‡')
        ax3.set_ylabel('é¢‘æ¬¡')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # å­å›¾4ï¼šæœ€å¤§å›æ’¤
        ax4 = plt.subplot(3, 3, 4)
        drawdowns = [pv['drawdown'] for pv in backtest_results['portfolio_values']]
        ax4.fill_between(dates, drawdowns, 0, alpha=0.3, color='red')
        ax4.set_title('å›æ’¤æ›²çº¿', fontsize=12, fontweight='bold')
        ax4.set_xlabel('æ—¥æœŸ')
        ax4.set_ylabel('å›æ’¤')
        ax4.grid(True, alpha=0.3)

        # å­å›¾5ï¼šäº¤æ˜“æ¬¡æ•°ç»Ÿè®¡
        ax5 = plt.subplot(3, 3, 5)
        trades_df = pd.DataFrame(backtest_results['trading_records'])
        if not trades_df.empty:
            monthly_trades = trades_df.resample('M', on='date').size()
            ax5.bar(monthly_trades.index, monthly_trades.values, alpha=0.7)
            ax5.set_title('æœˆåº¦äº¤æ˜“æ¬¡æ•°', fontsize=12, fontweight='bold')
            ax5.set_xlabel('æœˆä»½')
            ax5.set_ylabel('äº¤æ˜“æ¬¡æ•°')
        ax5.grid(True, alpha=0.3)

        # å­å›¾6ï¼šèƒœç‡ç»Ÿè®¡
        ax6 = plt.subplot(3, 3, 6)
        if not trades_df.empty and 'profit' in trades_df.columns:
            profitable = (trades_df['profit'] > 0).sum()
            unprofitable = (trades_df['profit'] <= 0).sum()
            ax6.pie([profitable, unprofitable],
                    labels=['ç›ˆåˆ©äº¤æ˜“', 'äºæŸäº¤æ˜“'],
                    autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])
            ax6.set_title('äº¤æ˜“èƒœç‡', fontsize=12, fontweight='bold')

        # å­å›¾7ï¼šç»©æ•ˆæŒ‡æ ‡è¡¨æ ¼
        ax7 = plt.subplot(3, 3, (7, 9))
        ax7.axis('off')

        metrics = backtest_results.get('metrics', {})
        table_data = [
            ['æŒ‡æ ‡', 'æ•°å€¼', 'ç›®æ ‡', 'çŠ¶æ€'],
            ['å¹´åŒ–æ”¶ç›Šç‡', f"{metrics.get('å¹´åŒ–æ”¶ç›Šç‡', 0):.2%}",
             f"{TARGET_METRICS['annual_return']:.2%}",
             'âœ“' if metrics.get('å¹´åŒ–æ”¶ç›Šç‡', 0) >= TARGET_METRICS['annual_return'] else 'âœ—'],
            ['å¤æ™®æ¯”ç‡', f"{metrics.get('å¤æ™®æ¯”ç‡', 0):.2f}",
             f"{TARGET_METRICS['sharpe_ratio']:.2f}",
             'âœ“' if metrics.get('å¤æ™®æ¯”ç‡', 0) >= TARGET_METRICS['sharpe_ratio'] else 'âœ—'],
            ['æœ€å¤§å›æ’¤', f"{metrics.get('æœ€å¤§å›æ’¤', 0):.2%}",
             f"{TARGET_METRICS['max_drawdown']:.2%}",
             'âœ“' if abs(metrics.get('æœ€å¤§å›æ’¤', 0)) <= TARGET_METRICS['max_drawdown'] else 'âœ—'],
            ['èƒœç‡', f"{metrics.get('èƒœç‡', 0):.2%}", '>50%',
             'âœ“' if metrics.get('èƒœç‡', 0) > 0.5 else 'âœ—'],
            ['ç›ˆäºæ¯”', f"{metrics.get('ç›ˆäºæ¯”', 0):.2f}", '>1.5',
             'âœ“' if metrics.get('ç›ˆäºæ¯”', 0) > 1.5 else 'âœ—']
        ]

        table = ax7.table(cellText=table_data, loc='center', cellLoc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 2)

        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for i in range(len(table_data)):
            for j in range(len(table_data[0])):
                cell = table[(i, j)]
                if i == 0:  # æ ‡é¢˜è¡Œ
                    cell.set_facecolor('#4C72B0')
                    cell.set_text_props(weight='bold', color='white')
                elif table_data[i][-1] == 'âœ“':  # è¾¾æ ‡è¡Œ
                    cell.set_facecolor('#DFF0D8')
                elif table_data[i][-1] == 'âœ—':  # æœªè¾¾æ ‡è¡Œ
                    cell.set_facecolor('#F2DEDE')
                else:
                    cell.set_facecolor('#FFFFFF')

        plt.suptitle('å®ç›˜è´´è¿‘åº¦å›æµ‹åˆ†ææŠ¥å‘Š', fontsize=16, fontweight='bold')
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"è¯¦ç»†å›æµ‹å›¾è¡¨å·²ä¿å­˜: {save_path}")

        plt.show()

    except Exception as e:
        print(f"ç»˜åˆ¶è¯¦ç»†å›æµ‹å›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
def generate_optimization_suggestions(factors, model_performance, backtest_results):
    """ç”Ÿæˆè¿­ä»£ä¼˜åŒ–å»ºè®®"""

    suggestions = []

    # 1. å› å­å±‚é¢ä¼˜åŒ–
    financial_count = len(factors.get('financial_factors', []))
    technical_count = len(factors.get('technical_factors', []))

    if financial_count < 15:
        suggestions.append("è´¢åŠ¡å› å­æ•°é‡ä¸è¶³ï¼Œå»ºè®®å¢åŠ æ›´å¤šè´¢åŠ¡æŒ‡æ ‡ï¼Œå¦‚ç°é‡‘æµé‡æ¯”ç‡ã€è¥è¿èµ„æœ¬æ¯”ç‡ç­‰")

    if technical_count < 5:
        suggestions.append("æŠ€æœ¯å› å­æ•°é‡ä¸è¶³ï¼Œå»ºè®®å¢åŠ æ›´å¤šæŠ€æœ¯æŒ‡æ ‡ï¼Œå¦‚æˆäº¤é‡ç›¸å…³æŒ‡æ ‡ã€æ³¢åŠ¨ç‡æŒ‡æ ‡ç­‰")

    # 2. æ¨¡å‹æ€§èƒ½ä¼˜åŒ–
    best_f1 = max([m.get('f1_score', 0) for m in model_performance.get('models', {}).values()])
    if best_f1 < 0.6:
        suggestions.append(f"æ¨¡å‹F1åˆ†æ•°({best_f1:.2%})åä½ï¼Œå»ºè®®ä¼˜åŒ–æ¨¡å‹å‚æ•°æˆ–å¢åŠ ç‰¹å¾å·¥ç¨‹")

    # 3. å›æµ‹ç»©æ•ˆä¼˜åŒ–
    if backtest_results:
        metrics = backtest_results.get('metrics', {})

        if metrics.get('å¹´åŒ–æ”¶ç›Šç‡', 0) < TARGET_METRICS['annual_return']:
            suggestions.append("å¹´åŒ–æ”¶ç›Šç‡æœªè¾¾ç›®æ ‡ï¼Œè€ƒè™‘è°ƒæ•´é€‰è‚¡é˜ˆå€¼æˆ–ä¼˜åŒ–ä»“ä½ç®¡ç†")

        if abs(metrics.get('æœ€å¤§å›æ’¤', 0)) > TARGET_METRICS['max_drawdown']:
            suggestions.append("æœ€å¤§å›æ’¤è¿‡å¤§ï¼Œå»ºè®®ä¼˜åŒ–æ­¢æŸç­–ç•¥æˆ–é™ä½ä»“ä½é›†ä¸­åº¦")

        if metrics.get('å¤æ™®æ¯”ç‡', 0) < TARGET_METRICS['sharpe_ratio']:
            suggestions.append("å¤æ™®æ¯”ç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–é£é™©è°ƒæ•´åæ”¶ç›Šï¼Œå¯èƒ½éœ€é™ä½æ³¢åŠ¨ç‡")

        trading_stats = backtest_results.get('trading_stats', {})
        if trading_stats.get('æ€»äº¤æ˜“æˆæœ¬', 0) > INITIAL_CAPITAL * 0.02:  # äº¤æ˜“æˆæœ¬è¶…è¿‡2%
            suggestions.append("äº¤æ˜“æˆæœ¬è¿‡é«˜ï¼Œå»ºè®®å‡å°‘è°ƒä»“é¢‘ç‡æˆ–ä¼˜åŒ–äº¤æ˜“ç®—æ³•")

    # 4. é€šç”¨å»ºè®®
    suggestions.append("å»ºè®®å¼•å…¥æ»šåŠ¨æ—¶é—´çª—å£è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæé«˜ç­–ç•¥ç¨³å®šæ€§")
    suggestions.append("å»ºè®®å¢åŠ å¸‚åœºç¯å¢ƒåˆ¤æ–­æ¨¡å—ï¼Œåœ¨ä¸åŒå¸‚åœºç¯å¢ƒä¸‹ä½¿ç”¨ä¸åŒç­–ç•¥")
    suggestions.append("å»ºè®®å¼•å…¥è¡Œä¸šè½®åŠ¨å› å­ï¼Œä¼˜åŒ–è¡Œä¸šé…ç½®")

    return suggestions


@timer_decorator
def quick_test_lightgbm():
    """å¿«é€Ÿæµ‹è¯•LightGBMï¼Œè·³è¿‡IC/IRç­‰è€—æ—¶æ­¥éª¤"""
    print_section("âš¡ LightGBMå¿«é€Ÿæµ‹è¯•æ¨¡å¼")

    # 1. å°è¯•ç›´æ¥åŠ è½½é¢„åˆå¹¶æ–‡ä»¶
    if not os.path.exists(PRE_MERGED_FILE):
        print(f"âŒ é¢„åˆå¹¶æ–‡ä»¶ä¸å­˜åœ¨: {PRE_MERGED_FILE}")
        print("è¯·å…ˆè¿è¡Œå®Œæ•´æµç¨‹ç”Ÿæˆé¢„åˆå¹¶æ–‡ä»¶")
        return None

    print(f"åŠ è½½é¢„åˆå¹¶æ–‡ä»¶: {PRE_MERGED_FILE}")
    try:
        with open(PRE_MERGED_FILE, 'rb') as f:
            data = pickle.load(f)

        # é€‚é…æ•°æ®æ ¼å¼
        if isinstance(data, tuple) and len(data) == 2:
            df, feature_cols = data
        elif isinstance(data, pd.DataFrame):
            df = data
            # è‡ªåŠ¨æå–ç‰¹å¾åˆ—
            base_cols = ['date', 'stock_code', 'close', 'volume', 'open', 'high', 'low',
                         'future_return', 'market_avg_return', 'label']
            feature_cols = [col for col in df.columns
                            if col not in base_cols and pd.api.types.is_numeric_dtype(df[col])]
        else:
            print(f"æœªçŸ¥æ•°æ®æ ¼å¼: {type(data)}")
            return None

        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
        print(f"æ—¶é—´èŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
        print(f"è‚¡ç¥¨æ•°é‡: {df['stock_code'].nunique()}")
        print(f"åŸå§‹ç‰¹å¾æ•°é‡: {len(feature_cols)}")

    except Exception as e:
        print(f"é¢„åˆå¹¶æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return None

    # 2. å¿«é€Ÿç‰¹å¾é€‰æ‹©ï¼ˆç®€å•çš„æ–¹å·®ç­›é€‰ï¼‰
    print("\næ‰§è¡Œå¿«é€Ÿç‰¹å¾é€‰æ‹©...")
    # é€‰æ‹©æ–¹å·®æœ€å¤§çš„å‰Nä¸ªç‰¹å¾
    feature_variances = {}
    for col in feature_cols:
        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
            variance = df[col].var()
            if not np.isnan(variance):
                feature_variances[col] = variance

    # æŒ‰æ–¹å·®æ’åºï¼Œé€‰æ‹©å‰Nä¸ªç‰¹å¾
    sorted_features = sorted(feature_variances.items(), key=lambda x: x[1], reverse=True)
    selected_features = [f[0] for f in sorted_features[:LIGHTGBM_TEST_FEATURES]]

    print(f"é€‰æ‹©ç‰¹å¾: {len(selected_features)} ä¸ª (æ–¹å·®æœ€å¤§)")
    print(f"ç‰¹å¾ç¤ºä¾‹: {selected_features[:5]}")

    # 3. é‡‡æ ·æ•°æ®ä»¥å‡å°‘è®¡ç®—é‡
    print(f"\né‡‡æ ·æ•°æ®: {LIGHTGBM_TEST_SAMPLE_SIZE:,} æ¡æ ·æœ¬")
    if len(df) > LIGHTGBM_TEST_SAMPLE_SIZE:
        # åˆ†å±‚é‡‡æ ·ï¼Œä¿æŒæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
        df_sampled = df.sample(n=LIGHTGBM_TEST_SAMPLE_SIZE, random_state=RANDOM_STATE)
    else:
        df_sampled = df.copy()

    # 4. å‡†å¤‡å»ºæ¨¡æ•°æ®
    print("å‡†å¤‡å»ºæ¨¡æ•°æ®...")
    modeling_df = df_sampled[['date', 'stock_code', 'future_return', 'label'] + selected_features].copy()

    # å¤„ç†ç¼ºå¤±å€¼
    for col in selected_features:
        if col in modeling_df.columns:
            modeling_df[col] = modeling_df[col].fillna(modeling_df[col].median())

    # ç§»é™¤æ ‡ç­¾ç¼ºå¤±çš„è¡Œ
    initial_size = len(modeling_df)
    modeling_df = modeling_df.dropna(subset=['label', 'future_return'])
    print(f"ç§»é™¤ç¼ºå¤±æ ‡ç­¾: {initial_size - len(modeling_df):,} è¡Œ")

    # 5. ç®€å•æ•°æ®é›†åˆ’åˆ†ï¼ˆä¸è¿›è¡Œæ»šåŠ¨äº¤å‰éªŒè¯ä»¥åŠ é€Ÿï¼‰
    print("åˆ’åˆ†æ•°æ®é›†...")
    modeling_df = modeling_df.sort_values('date')
    dates = modeling_df['date'].unique()
    test_split_idx = int(len(dates) * 0.8)  # 80%è®­ç»ƒï¼Œ20%æµ‹è¯•

    train_dates = dates[:test_split_idx]
    test_dates = dates[test_split_idx:]

    train_df = modeling_df[modeling_df['date'].isin(train_dates)]
    test_df = modeling_df[modeling_df['date'].isin(test_dates)]

    X_train = train_df[selected_features]
    X_test = test_df[selected_features]
    y_train = train_df['label']
    y_test = test_df['label']

    print(f"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
    print(f"æ­£æ ·æœ¬æ¯”ä¾‹ - è®­ç»ƒ: {y_train.mean():.2%}, æµ‹è¯•: {y_test.mean():.2%}")

    # 6. å¿«é€Ÿæ ‡å‡†åŒ–
    print("æ ‡å‡†åŒ–ç‰¹å¾...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 7. ä½¿ç”¨SMOTEå¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    print("å¤„ç†ç±»åˆ«ä¸å¹³è¡¡...")
    smote = SMOTE(random_state=RANDOM_STATE, sampling_strategy=0.8)
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)
    print(f"å¹³è¡¡åè®­ç»ƒé›†: {X_train_balanced.shape}")

    # 8. è®­ç»ƒLightGBMï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
    print_section("è®­ç»ƒLightGBMï¼ˆé»˜è®¤å‚æ•°ï¼‰")

    # LightGBMé»˜è®¤å‚æ•°
    lgb_params = {
        'n_estimators': 100,
        'max_depth': 7,
        'learning_rate': 0.05,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': RANDOM_STATE,
        'n_jobs': N_JOBS,
        'verbosity': -1
    }

    print("è®­ç»ƒLightGBMæ¨¡å‹...")
    lgb_model = lgb.LGBMClassifier(**lgb_params)
    lgb_model.fit(
        X_train_balanced, y_train_balanced,
        eval_set=[(X_test_scaled, y_test)],
        eval_metric='binary_logloss',
        callbacks=[lgb.early_stopping(30), lgb.log_evaluation(0)]
    )

    # 9. è¯„ä¼°æ¨¡å‹
    print_section("LightGBMè¯„ä¼°ç»“æœ")

    y_pred = lgb_model.predict(X_test_scaled)
    y_proba = lgb_model.predict_proba(X_test_scaled)[:, 1]

    # ====================== æ–°å¢ï¼šæ•°æ®æ ¡éªŒä¸ä¿å­˜ï¼ˆä¿®æ­£å˜é‡åï¼‰ ======================
    # 1. å…ˆæ‰“å°å…³é”®ä¿¡æ¯åˆ°æ§åˆ¶å°ï¼Œå¿«é€Ÿæ’æŸ¥
    print("===== æ•°æ®æ ¡éªŒä¿¡æ¯ =====")
    # æ³¨æ„ï¼šè¿™é‡Œæ”¹ç”¨å®é™…çš„å˜é‡å y_predï¼ˆé¢„æµ‹ç»“æœï¼‰å’Œ y_testï¼ˆçœŸå®æ ‡ç­¾ï¼‰
    print(f"y_pred (é¢„æµ‹ç»“æœ) ç±»å‹: {type(y_pred)}")
    print(f"y_pred (é¢„æµ‹ç»“æœ) å…·ä½“å€¼: {y_pred}")
    # å°è¯•æ‰“å°å½¢çŠ¶ï¼ˆå¦‚æœæ˜¯æ•°ç»„ï¼Œå¦åˆ™æ•è·å¼‚å¸¸ï¼‰
    try:
        print(f"y_pred (é¢„æµ‹ç»“æœ) å½¢çŠ¶: {np.shape(y_pred)}")
    except Exception as e:
        print(f"y_pred (é¢„æµ‹ç»“æœ) æ— æ³•è·å–å½¢çŠ¶: {e}")

    # æ‰“å°çœŸå®æ ‡ç­¾y_testçš„ä¿¡æ¯ï¼ˆå¯¹æ¯”å‚è€ƒï¼‰
    print(f"\ny_test (çœŸå®æ ‡ç­¾) ç±»å‹: {type(y_test)}")
    print(f"y_test (çœŸå®æ ‡ç­¾) å…·ä½“å€¼ï¼ˆå‰10ä¸ªï¼‰: {y_test[:10] if hasattr(y_test, '__getitem__') else y_test}")
    try:
        print(f"y_test (çœŸå®æ ‡ç­¾) å½¢çŠ¶: {np.shape(y_test)}")
    except Exception as e:
        print(f"y_test (çœŸå®æ ‡ç­¾) æ— æ³•è·å–å½¢çŠ¶: {e}")

    # 2. ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶ï¼ˆæ–¹ä¾¿åç»­è¯¦ç»†åˆ†æï¼‰
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆé¿å…ç›®å½•ä¸å­˜åœ¨æŠ¥é”™ï¼‰
    save_dir = "./model_test_data"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # æ–¹å¼1ï¼šä¿å­˜ä¸ºtxtæ–‡ä»¶ï¼ˆç›´è§‚æŸ¥çœ‹æ–‡æœ¬å†…å®¹ï¼‰
    with open(os.path.join(save_dir, "test_data_info.txt"), "w", encoding="utf-8") as f:
        f.write(f"y_pred (é¢„æµ‹ç»“æœ) ç±»å‹: {type(y_pred)}\n")
        f.write(f"y_pred (é¢„æµ‹ç»“æœ) å…·ä½“å€¼: {y_pred}\n")
        f.write(f"\ny_test (çœŸå®æ ‡ç­¾) ç±»å‹: {type(y_test)}\n")
        f.write(f"y_test (çœŸå®æ ‡ç­¾) å…·ä½“å€¼ï¼ˆå…¨éƒ¨ï¼‰: {y_test}\n")

    # æ–¹å¼2ï¼šä¿å­˜ä¸ºpickleæ–‡ä»¶ï¼ˆä¿ç•™åŸå§‹æ•°æ®ç±»å‹ï¼Œå¯åŠ è½½å¤ç”¨ï¼‰
    # åªä¿å­˜æœ‰æ•ˆæ•°æ®ï¼Œé¿å…å¼‚å¸¸å€¼å¯¼è‡´ä¿å­˜å¤±è´¥
    save_data = {
        "y_test": y_test,  # çœŸå®æ ‡ç­¾
        "y_pred": y_pred,  # é¢„æµ‹ç»“æœ
        "y_proba": y_proba,  # é¢å¤–ä¿å­˜é¢„æµ‹æ¦‚ç‡ï¼Œæ–¹ä¾¿æ’æŸ¥åˆ†ç±»é—®é¢˜
        "X_test_scaled": X_test_scaled  # ä¿å­˜æ ‡å‡†åŒ–åçš„æµ‹è¯•é›†ç‰¹å¾ï¼Œæ’æŸ¥è¾“å…¥é—®é¢˜
    }
    with open(os.path.join(save_dir, "test_data.pkl"), "wb") as f:
        pickle.dump(save_data, f)

    # æ–¹å¼3ï¼šå¦‚æœæ˜¯æ•°ç»„ï¼Œä¿å­˜ä¸ºcsvï¼ˆæ›´æ˜“è¯»ï¼‰
    try:
        # å°è¯•å°†çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾åˆå¹¶ä¿å­˜ï¼ˆä»…å½“ä¸¤è€…éƒ½æ˜¯æ•°ç»„æ—¶ï¼‰
        if isinstance(y_test, (np.ndarray, list, pd.Series)) and isinstance(y_pred, (np.ndarray, list, pd.Series)):
            df = pd.DataFrame({
                "y_true": y_test,
                "y_pred": y_pred,
                "y_proba": y_proba  # æ–°å¢é¢„æµ‹æ¦‚ç‡åˆ—ï¼Œæ›´å…¨é¢
            })
            df.to_csv(os.path.join(save_dir, "test_pred_true.csv"), index=False, encoding="utf-8")
            print("âœ… çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾å’Œé¢„æµ‹æ¦‚ç‡å·²ä¿å­˜ä¸ºcsvæ–‡ä»¶")
        else:
            print("âš ï¸ æ— æ³•ä¿å­˜csvï¼šy_testæˆ–y_predä¸æ˜¯æ•°ç»„/åˆ—è¡¨ç±»å‹")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜csvå¤±è´¥: {e}")

    # ====================== åŸæœ‰ä»£ç ï¼ˆè°ƒç”¨accuracy_scoreï¼‰ ======================
    # æ³¨æ„ï¼šå¦‚æœy_predæ˜¯å¼‚å¸¸å€¼ï¼Œå¯å…ˆåŠ åˆ¤æ–­é¿å…ç¨‹åºç›´æ¥å´©æºƒ
    if not isinstance(y_pred, (np.ndarray, list, pd.Series, pd.DataFrame)):
        print(f"âŒ è­¦å‘Šï¼šy_predä¸æ˜¯æ•°ç»„ç±»æ•°æ®ï¼Œå€¼ä¸º {y_pred}ï¼Œè·³è¿‡accuracy_scoreè®¡ç®—")
        accuracy = np.nan  # ç”¨NaNæ ‡è®°æ— æ•ˆå€¼
    else:
        accuracy = accuracy_score(y_test, y_pred)

    # è®¡ç®—æŒ‡æ ‡
    #accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    roc_auc = roc_auc_score(y_test, y_proba)

    print("ğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
    print(f"  å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
    print(f"  ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"  å¬å›ç‡ (Recall): {recall:.4f}")
    print(f"  F1åˆ†æ•°: {f1:.4f}")
    print(f"  ROC-AUC: {roc_auc:.4f}")

    # 10. ç‰¹å¾é‡è¦æ€§åˆ†æ
    print_section("LightGBMç‰¹å¾é‡è¦æ€§")

    feature_importance = pd.DataFrame({
        'feature': selected_features,
        'importance': lgb_model.feature_importances_,
        'importance_type': 'gain'
    }).sort_values('importance', ascending=False)

    print("Top 20 é‡è¦ç‰¹å¾:")
    print(feature_importance.head(20).to_string(index=False))

    # 11. ä¿å­˜æ¨¡å‹å’Œç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä¿å­˜æ¨¡å‹
    model_file = f'lightgbm_quick_test_{timestamp}.pkl'
    with open(model_file, 'wb') as f:
        pickle.dump({
            'model': lgb_model,
            'scaler': scaler,
            'features': selected_features,
            'importance': feature_importance,
            'metrics': {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'roc_auc': roc_auc
            }
        }, f)
    print(f"âœ… LightGBMæ¨¡å‹å·²ä¿å­˜: {model_file}")

    # ä¿å­˜ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶
    result_file = f'lightgbm_test_results_{timestamp}.txt'
    with open(result_file, 'w', encoding='utf-8') as f:
        f.write("LightGBMå¿«é€Ÿæµ‹è¯•ç»“æœ\n")
        f.write("=" * 50 + "\n")
        f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ•°æ®æ ·æœ¬: {len(df_sampled):,} æ¡\n")
        f.write(f"ç‰¹å¾æ•°é‡: {len(selected_features)} ä¸ª\n")
        f.write(f"è®­ç»ƒé›†å¤§å°: {len(X_train):,} æ¡\n")
        f.write(f"æµ‹è¯•é›†å¤§å°: {len(X_test):,} æ¡\n\n")

        f.write("æ¨¡å‹æ€§èƒ½:\n")
        f.write(f"  å‡†ç¡®ç‡: {accuracy:.4f}\n")
        f.write(f"  ç²¾ç¡®ç‡: {precision:.4f}\n")
        f.write(f"  å¬å›ç‡: {recall:.4f}\n")
        f.write(f"  F1åˆ†æ•°: {f1:.4f}\n")
        f.write(f"  ROC-AUC: {roc_auc:.4f}\n\n")

        f.write("Top 10 é‡è¦ç‰¹å¾:\n")
        for idx, row in feature_importance.head(10).iterrows():
            f.write(f"  {row['feature']}: {row['importance']:.4f}\n")

    print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜: {result_file}")

    return {
        'model': lgb_model,
        'scaler': scaler,
        'features': selected_features,
        'importance': feature_importance,
        'metrics': {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc
        }
    }

# ==================== ä¿®æ”¹ä¸»å‡½æ•°ä¸­çš„å›æµ‹éƒ¨åˆ† ====================
def main():
    """ä¸»ç¨‹åº - ä¼˜åŒ–ç‰ˆæœ¬"""
    print_section("å°æ¹¾è‚¡ç¥¨é€‰è‚¡é¢„æµ‹æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    print(f"é¢„æµ‹æœªæ¥å¤©æ•°: {FUTURE_DAYS}å¤©")
    print(f"å›çœ‹å¤©æ•°: {LOOKBACK_DAYS}å¤©")
    print(f"éšæœºç§å­: {RANDOM_STATE}")
    print(f"å¿«é€Ÿæ¨¡å¼: {'å¯ç”¨' if QUICK_MODE else 'å…³é—­'}")
    print(f"ä½¿ç”¨å·²ä¿å­˜æ•°æ®: {'å¯ç”¨' if USE_SAVED_DATA else 'å…³é—­'}")
    print(f"å¼ºåˆ¶é‡æ–°è®¡ç®—å› å­: {'æ˜¯' if FORCE_RECOMPUTE_FACTORS else 'å¦'}")

    # æ‰“å°ä¼˜åŒ–å‚æ•°
    print("\nğŸ¯ ä¼˜åŒ–å‚æ•°é…ç½®:")
    print(f"  æœ€å¤§æŒä»“æ•°é‡: {TOP_N_HOLDINGS}åª")
    print(f"  è°ƒä»“é¢‘ç‡: {REBALANCE_FREQUENCY}")
    print(f"  æœ€å°æŒæœ‰å¤©æ•°: {RISK_CONTROL['min_holding_days']}å¤©")
    print(f"  ä»“ä½é™åˆ¶: {RISK_CONTROL['single_stock_limit']:.1%}")
    print(f"  æ­¢æŸé˜ˆå€¼: {RISK_CONTROL['individual_stop_loss']:.1%}")
    print(f"  æ­¢ç›ˆé˜ˆå€¼: {RISK_CONTROL['individual_stop_profit']:.1%}")
    print(f"  ç»„åˆæ­¢æŸ: {RISK_CONTROL['portfolio_stop_loss']:.1%}")
    print(f"  æ¯æ—¥æœ€å¤§äº¤æ˜“æ¬¡æ•°: {RISK_CONTROL['max_daily_trades']}æ¬¡")

    # æ—¶é—´é¢„ä¼°
    print("\né¢„è®¡æ‰§è¡Œæ—¶é—´:")
    if QUICK_MODE:
        print("  æ€»æ—¶é—´: 10-15åˆ†é’Ÿ")
    else:
        print("  æ€»æ—¶é—´: 30-45åˆ†é’Ÿ")
    print("=" * 50)

    start_time = time.time()

    try:
        # 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        print_section("æ­¥éª¤1: åŠ è½½å’Œé¢„å¤„ç†æ•°æ®")

        # æ£€æŸ¥é¢„åˆå¹¶æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(PRE_MERGED_FILE):
            print(f"åŠ è½½é¢„åˆå¹¶æ–‡ä»¶: {PRE_MERGED_FILE}")
            try:
                with open(PRE_MERGED_FILE, 'rb') as f:
                    data = pickle.load(f)

                if isinstance(data, tuple) and len(data) == 2:
                    df, feature_cols = data
                    print(f"âœ… é¢„åˆå¹¶æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
                    print(f"æ—¶é—´èŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
                    print(f"è‚¡ç¥¨æ•°é‡: {df['stock_code'].nunique()}")
                else:
                    print(f"âŒ é¢„åˆå¹¶æ–‡ä»¶æ ¼å¼é”™è¯¯")
                    return None
            except Exception as e:
                print(f"é¢„åˆå¹¶æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                return None
        else:
            print(f"âŒ é¢„åˆå¹¶æ–‡ä»¶ä¸å­˜åœ¨: {PRE_MERGED_FILE}")
            print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†æµç¨‹")
            return None

        # 2. æ£€æŸ¥æ”¶ç›Šç‡æ•°æ®
        print_section("æ­¥éª¤2: æ£€æŸ¥æ”¶ç›Šç‡æ•°æ®")

        if 'future_return' not in df.columns:
            print("âŒ æ•°æ®ä¸­æ²¡æœ‰future_returnåˆ—")
            return None

        # æ£€æŸ¥æ”¶ç›Šç‡æœ‰æ•ˆæ€§
        valid_returns = df['future_return'].dropna()
        inf_count = np.isinf(valid_returns).sum()
        print(f"æ”¶ç›Šç‡æ•°æ®ç»Ÿè®¡:")
        print(f"  æœ‰æ•ˆæ ·æœ¬: {len(valid_returns):,}")
        print(f"  infå€¼æ•°é‡: {inf_count}")
        print(f"  æ”¶ç›Šç‡èŒƒå›´: {valid_returns.min():.4f} åˆ° {valid_returns.max():.4f}")

        if inf_count > 0:
            print("âš ï¸ å‘ç°infå€¼ï¼Œè¿›è¡Œä¿®å¤...")
            df['future_return'] = df['future_return'].replace([np.inf, -np.inf], np.nan)
            df = df.dropna(subset=['future_return'])
            print(f"ä¿®å¤åæœ‰æ•ˆæ ·æœ¬: {len(df):,}")

        # 3. å› å­ç­›é€‰ï¼ˆå¦‚æœéœ€è¦ï¼‰
        print_section("æ­¥éª¤3: å› å­ç­›é€‰")

        # åˆ†ç¦»è´¢åŠ¡å’ŒæŠ€æœ¯å› å­
        financial_cols = [col for col in feature_cols if col.startswith('fin_')]
        technical_cols = [col for col in feature_cols if not col.startswith('fin_')]

        print(f"å› å­ç»Ÿè®¡:")
        print(f"  è´¢åŠ¡å› å­: {len(financial_cols)} ä¸ª")
        print(f"  æŠ€æœ¯å› å­: {len(technical_cols)} ä¸ª")
        print(f"  æ€»å› å­: {len(feature_cols)} ä¸ª")

        # å¦‚æœå› å­æ•°é‡å¤ªå¤šï¼Œè¿›è¡Œç­›é€‰
        if len(feature_cols) > 30:
            print(f"å› å­æ•°é‡è¿‡å¤š({len(feature_cols)})ï¼Œè¿›è¡Œåˆæ­¥ç­›é€‰...")

            # ä½¿ç”¨ç®€å•çš„æ–¹å·®ç­›é€‰
            feature_variances = []
            for col in feature_cols:
                if col in df.columns:
                    variance = df[col].var()
                    if not np.isnan(variance):
                        feature_variances.append((col, variance))

            # æŒ‰æ–¹å·®æ’åºï¼Œé€‰æ‹©å‰30ä¸ª
            feature_variances.sort(key=lambda x: x[1], reverse=True)
            selected_features = [f[0] for f in feature_variances[:30]]
            feature_cols = selected_features
            print(f"ç­›é€‰åå› å­æ•°é‡: {len(feature_cols)} ä¸ª")

        # 4. å‡†å¤‡å»ºæ¨¡æ•°æ®
        print_section("æ­¥éª¤4: å‡†å¤‡å»ºæ¨¡æ•°æ®")

        modeling_df = prepare_modeling_data(df, feature_cols)
        if modeling_df.empty:
            print("âŒ å»ºæ¨¡æ•°æ®ä¸ºç©º")
            return None

        print(f"å»ºæ¨¡æ•°æ®ç»Ÿè®¡:")
        print(f"  æ ·æœ¬æ•°é‡: {len(modeling_df):,}")
        print(f"  ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        print(f"  æ­£æ ·æœ¬æ¯”ä¾‹: {modeling_df['label'].mean():.2%}")

        # 5. æ•°æ®é›†åˆ’åˆ†
        print_section("æ­¥éª¤5: æ•°æ®é›†åˆ’åˆ†")

        data_split = split_train_val_test_data(
            modeling_df, feature_cols, test_ratio=TEST_RATIO, val_ratio=VAL_RATIO
        )

        if data_split[0] is None:
            print("âŒ æ•°æ®é›†åˆ’åˆ†å¤±è´¥")
            return None

        X_train, X_val, X_test, y_train, y_val, y_test, train_df, val_df, test_df = data_split

        print(f"æ•°æ®é›†åˆ’åˆ†ç»“æœ:")
        print(f"  è®­ç»ƒé›†: {X_train.shape}")
        print(f"  éªŒè¯é›†: {X_val.shape}")
        print(f"  æµ‹è¯•é›†: {X_test.shape}")

        # 6. æ¨¡å‹è®­ç»ƒ
        print_section("æ­¥éª¤6: æ¨¡å‹è®­ç»ƒ")

        # ä½¿ç”¨ä¿å®ˆå‚æ•°é˜²æ­¢è¿‡æ‹Ÿåˆ
        best_params = get_conservative_params()

        models, scaler, results, predictions, probabilities = train_models(
            X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, best_params
        )

        if not models:
            print("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
            return None

        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ:")
        for model_name, result in results.items():
            print(f"  {model_name.upper()}: F1={result['test_f1']:.4f}, AUC={result['test_roc_auc']:.4f}")

        # 7. ç”Ÿæˆé€‰è‚¡åˆ—è¡¨
        print_section("æ­¥éª¤7: ç”Ÿæˆé€‰è‚¡åˆ—è¡¨")

        daily_selected_df = generate_daily_selected_stocks(test_df, predictions, probabilities, top_n=10)

        if daily_selected_df.empty:
            print("âŒ é€‰è‚¡åˆ—è¡¨ç”Ÿæˆå¤±è´¥")
            return None

        print(f"âœ… é€‰è‚¡åˆ—è¡¨ç”Ÿæˆå®Œæˆ:")
        print(f"  æ€»é€‰è‚¡è®°å½•: {len(daily_selected_df):,}")
        print(f"  å¹³å‡æ¯æ—¥é€‰è‚¡: {daily_selected_df.groupby('äº¤æ˜“æ—¥').size().mean():.1f}")

        # 8. æ‰§è¡Œç®€åŒ–å›æµ‹
        print_section("æ­¥éª¤8: æ‰§è¡Œç®€åŒ–å›æµ‹")

        if not daily_selected_df.empty:
            # ç¡®ä¿åˆ—åæ­£ç¡®
            if 'äº¤æ˜“æ—¥' in daily_selected_df.columns:
                # é‡å‘½ååˆ—ä»¥åŒ¹é…å›æµ‹å‡½æ•°
                daily_selected_for_backtest = daily_selected_df.copy()

                # æ·»åŠ å¿…è¦çš„åˆ—
                if 'è‚¡ç¥¨ä»£ç ' in daily_selected_for_backtest.columns:
                    daily_selected_for_backtest = daily_selected_for_backtest.rename(columns={
                        'äº¤æ˜“æ—¥': 'date',
                        'è‚¡ç¥¨ä»£ç ': 'stock_code',
                        'æ”¶ç›˜ä»·': 'close',
                        'æœªæ¥20å¤©ç»å¯¹æ”¶ç›Šç‡': 'future_return'
                    })

                print(f"å›æµ‹æ•°æ®å‡†å¤‡å®Œæˆ:")
                print(f"  æ•°æ®å½¢çŠ¶: {daily_selected_for_backtest.shape}")
                print(
                    f"  æ—¶é—´èŒƒå›´: {daily_selected_for_backtest['date'].min()} åˆ° {daily_selected_for_backtest['date'].max()}")

                # æ‰§è¡Œç®€åŒ–å›æµ‹
                backtest_results = perform_backtest_simple(daily_selected_for_backtest, test_df)

                if backtest_results:
                    # åœ¨å›æµ‹éƒ¨åˆ†æ·»åŠ ä»¥ä¸‹ä»£ç æ¥æ‰“å°æŒ‡æ ‡ï¼š
                    if backtest_results and backtest_results.get('metrics'):
                        metrics = backtest_results['metrics']
                        trading_stats = backtest_results.get('trading_stats', {})

                        # ä½¿ç”¨æ–°çš„æ‰“å°å‡½æ•°
                        print_backtest_metrics(metrics)

                        # æ‰“å°äº¤æ˜“ç»Ÿè®¡
                        print(f"\nğŸ’¼ äº¤æ˜“ç»Ÿè®¡:")
                        print(f"   å¹³å‡æŒä»“å¤©æ•°: {trading_stats.get('å¹³å‡æŒä»“å¤©æ•°', 0):.1f}å¤©")
                        print(f"   å¹´åŒ–æ¢æ‰‹ç‡: {trading_stats.get('å¹´åŒ–æ¢æ‰‹ç‡', 0):.2%}")
                        print(f"   æ€»äº¤æ˜“æˆæœ¬: {trading_stats.get('æ€»äº¤æ˜“æˆæœ¬', 0):,.2f}")
                        print(f"   èƒœç‡: {trading_stats.get('èƒœç‡', 0):.2%}")

                        # åŒæ—¶ä¿å­˜ä¸ºè¯¦ç»†æŠ¥å‘Š
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        report_file = f'backtest_detailed_report_{timestamp}.txt'
                        save_backtest_report(metrics, report_file)

                    # ä¿å­˜å›æµ‹ç»“æœ
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    backtest_file = f'backtest_results_{timestamp}.pkl'
                    with open(backtest_file, 'wb') as f:
                        pickle.dump(backtest_results, f, protocol=4)
                    print(f"\nâœ… å›æµ‹ç»“æœå·²ä¿å­˜: {backtest_file}")

                    # ç”Ÿæˆå›æµ‹æŠ¥å‘Š
                    report_file = f'backtest_report_{timestamp}.txt'
                    with open(report_file, 'w', encoding='utf-8') as f:
                        f.write("=" * 60 + "\n")
                        f.write("è‚¡ç¥¨é€‰è‚¡ç­–ç•¥å›æµ‹æŠ¥å‘Š\n")
                        f.write("=" * 60 + "\n\n")

                        f.write(
                            f"å›æµ‹æœŸé—´: {daily_selected_for_backtest['date'].min()} åˆ° {daily_selected_for_backtest['date'].max()}\n")
                        f.write(f"åˆå§‹èµ„é‡‘: {INITIAL_CAPITAL:,.2f} å…ƒ\n")
                        f.write(f"è‚¡ç¥¨æ•°é‡: {daily_selected_for_backtest['stock_code'].nunique()} åª\n")
                        f.write(f"äº¤æ˜“å¤©æ•°: {len(daily_selected_for_backtest['date'].unique())} å¤©\n\n")

                        f.write("ç»©æ•ˆæŒ‡æ ‡:\n")
                        f.write("-" * 40 + "\n")
                        for key, value in metrics.items():
                            if isinstance(value, float):
                                if key in ['æ€»æ”¶ç›Šç‡', 'å¹´åŒ–æ”¶ç›Šç‡', 'æœ€å¤§å›æ’¤', 'å¹´åŒ–æ³¢åŠ¨ç‡', 'èƒœç‡']:
                                    f.write(f"{key}: {value:.2%}\n")
                                elif key in ['å¤æ™®æ¯”ç‡', 'å¡ç›æ¯”ç‡', 'ç›ˆäºæ¯”', 'ä¿¡æ¯æ¯”ç‡']:
                                    f.write(f"{key}: {value:.2f}\n")
                                else:
                                    f.write(f"{key}: {value}\n")
                            else:
                                f.write(f"{key}: {value}\n")

                        f.write("\näº¤æ˜“ç»Ÿè®¡:\n")
                        f.write("-" * 40 + "\n")
                        for key, value in trading_stats.items():
                            if isinstance(value, float):
                                if key in ['èƒœç‡', 'å¹³å‡äº¤æ˜“æˆæœ¬ç‡', 'å¹´åŒ–æ¢æ‰‹ç‡']:
                                    f.write(f"{key}: {value:.2%}\n")
                                elif key in ['å¹³å‡æŒä»“å¤©æ•°']:
                                    f.write(f"{key}: {value:.1f} å¤©\n")
                                elif key in ['æ€»äº¤æ˜“æˆæœ¬']:
                                    f.write(f"{key}: {value:,.2f} å…ƒ\n")
                                else:
                                    f.write(f"{key}: {value}\n")
                            else:
                                f.write(f"{key}: {value}\n")

                        # æ·»åŠ äº¤æ˜“è®°å½•æ‘˜è¦
                        if backtest_results.get('trading_records'):
                            trades = pd.DataFrame(backtest_results['trading_records'])
                            if not trades.empty:
                                f.write(f"\näº¤æ˜“è®°å½•æ‘˜è¦:\n")
                                f.write(f"  æ€»äº¤æ˜“ç¬”æ•°: {len(trades)}\n")
                                f.write(f"  ä¹°å…¥ç¬”æ•°: {len(trades[trades['type'] == 'buy'])}\n")
                                f.write(f"  å–å‡ºç¬”æ•°: {len(trades[trades['type'] == 'sell'])}\n")

                                if 'profit' in trades.columns:
                                    profitable = len(trades[(trades['type'] == 'sell') & (trades['profit'] > 0)])
                                    total_sell = len(trades[trades['type'] == 'sell'])
                                    if total_sell > 0:
                                        f.write(f"  ç›ˆåˆ©äº¤æ˜“æ¯”ä¾‹: {profitable / total_sell:.2%}\n")

                    print(f"âœ… å›æµ‹æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

                    # ç»˜åˆ¶ç®€å•å›¾è¡¨
                    try:
                        plot_simple_backtest_results(backtest_results, timestamp)
                    except Exception as e:
                        print(f"å›¾è¡¨ç»˜åˆ¶å¤±è´¥: {e}")
                else:
                    print("âŒ å›æµ‹å¤±è´¥")
                    backtest_results = {
                        'metrics': {},
                        'trading_stats': {},
                        'portfolio_values': [],
                        'trading_records': []
                    }
            else:
                print("âŒ é€‰è‚¡æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—")
                backtest_results = {
                    'metrics': {},
                    'trading_stats': {},
                    'portfolio_values': [],
                    'trading_records': []
                }
        else:
            print("âŒ é€‰è‚¡æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œå›æµ‹")
            backtest_results = {
                'metrics': {},
                'trading_stats': {},
                'portfolio_values': [],
                'trading_records': []
            }

        # 9. ä¿å­˜å…¶ä»–ç»“æœ
        print_section("æ­¥éª¤9: ä¿å­˜å…¶ä»–ç»“æœ")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜é€‰è‚¡ç»“æœ
        selected_file = f'selected_stocks_{timestamp}.csv'
        daily_selected_df.to_csv(selected_file, index=False, encoding='utf-8-sig')
        print(f"âœ… é€‰è‚¡ç»“æœå·²ä¿å­˜: {selected_file}")

        # 10. æ€»ç»“
        end_time = time.time()
        execution_time = (end_time - start_time) / 60

        print_section("ä¼˜åŒ–ç‰ˆç¨‹åºæ‰§è¡Œå®Œæˆ")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.1f} åˆ†é’Ÿ")

        if backtest_results and backtest_results.get('portfolio_values'):
            final_value = backtest_results['portfolio_values'][-1]['portfolio_value']
            print(f"æœ€ç»ˆç»„åˆä»·å€¼: {final_value:,.2f}")

            # ä½¿ç”¨æ–°çš„æ‰“å°å‡½æ•°å†æ¬¡æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            if backtest_results.get('metrics'):
                metrics = backtest_results['metrics']
                print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
                print(f"   å›æµ‹åŒºé—´: {metrics.get('èµ·å§‹æ—¥æœŸ', 'N/A')} è‡³ {metrics.get('ç»“æŸæ—¥æœŸ', 'N/A')}")
                print(f"   æ€»æ”¶ç›Šç‡: {metrics.get('æ€»æ”¶ç›Šç‡', 0):.2%}")
                print(f"   å¹´åŒ–æ”¶ç›Šç‡: {metrics.get('å¹´åŒ–æ”¶ç›Šç‡', 0):.2%}")
                print(f"   æœ€å¤§å›æ’¤: {metrics.get('æœ€å¤§å›æ’¤', 0):.2%}")
                print(f"   å¹´åŒ–å¤æ™®æ¯”ç‡: {metrics.get('å¹´åŒ–å¤æ™®æ¯”ç‡', 0):.2f}")

            if 'trading_stats' in backtest_results:
                trading_stats = backtest_results['trading_stats']
                print(f"   å¹´åŒ–æ¢æ‰‹ç‡: {trading_stats.get('å¹´åŒ–æ¢æ‰‹ç‡', 0):.2%}")
                print(f"   å¹³å‡æŒä»“å¤©æ•°: {trading_stats.get('å¹³å‡æŒä»“å¤©æ•°', 0):.1f}å¤©")
        else:
            print("âš ï¸ æ— æœ‰æ•ˆå›æµ‹ç»“æœ")

        if result and result.get('backtest_results'):
            portfolio_values = result['backtest_results'].get('portfolio_values', [])
            if portfolio_values:
                start_date = portfolio_values[0]['date']
                end_date = portfolio_values[-1]['date']
                total_days = (end_date - start_date).days
                total_months = total_days / 30.44

                # è®¡ç®—å®é™…è°ƒä»“æ¬¡æ•°
                trading_records = result['backtest_results'].get('trading_records', [])
                buy_count = sum(1 for t in trading_records if t.get('type') == 'buy')
                sell_count = sum(1 for t in trading_records if t.get('type') == 'sell')

                print(f"\nğŸ“Š å®é™…äº¤æ˜“ç»Ÿè®¡:")
                print(f"   å›æµ‹æ€»å¤©æ•°: {total_days}å¤© ({total_months:.1f}ä¸ªæœˆ)")
                print(f"   ä¹°å…¥æ¬¡æ•°: {buy_count}æ¬¡")
                print(f"   å–å‡ºæ¬¡æ•°: {sell_count}æ¬¡")
                print(f"   å¹³å‡æ¯æœˆäº¤æ˜“: {(buy_count + sell_count) / max(1, total_months):.1f}æ¬¡")

                # è®¡ç®—æœˆåº¦æ¢æ‰‹ç‡
                if trading_records:
                    trades_df = pd.DataFrame(trading_records)
                    trades_df['date'] = pd.to_datetime(trades_df['date'])
                    trades_df['month'] = trades_df['date'].dt.to_period('M')

                    monthly_turnover = {}
                    for month, group in trades_df.groupby('month'):
                        month_trades = group['total_value'].sum()
                        # ä¼°ç®—è¯¥æœˆå¹³å‡å‡€å€¼
                        month_values = [pv for pv in portfolio_values
                                        if pd.to_datetime(pv['date']).to_period('M') == month]
                        if month_values:
                            avg_value = np.mean([pv.get('total_value', pv.get('portfolio_value', 0))
                                                 for pv in month_values])
                            if avg_value > 0:
                                monthly_turnover[str(month)] = month_trades / avg_value

                    if monthly_turnover:
                        avg_monthly_turnover = np.mean(list(monthly_turnover.values()))
                        print(f"   å¹³å‡æœˆåº¦æ¢æ‰‹ç‡: {avg_monthly_turnover:.2%}")
                        print(f"   ç†è®ºå¹´åŒ–æ¢æ‰‹ç‡: {avg_monthly_turnover * 12:.2%}")

        # æ‰“å°ä¼˜åŒ–æ•ˆæœ
        print("\nğŸ”§ ä¼˜åŒ–æ•ˆæœæ€»ç»“:")
        print("   1. âœ… ä½¿ç”¨ç®€åŒ–å›æµ‹é€»è¾‘ï¼Œé¿å…å¤æ‚é”™è¯¯")
        print("   2. âœ… ä¿®å¤äº¤æ˜“ç»Ÿè®¡å‡½æ•°ä¸­çš„è¯­æ³•é”™è¯¯")
        print("   3. âœ… ç¡®ä¿æ•°æ®åˆ—ååŒ¹é…")
        print("   4. âœ… ç®€åŒ–ä»·æ ¼æ•°æ®è·å–é€»è¾‘")
        print("   5. âœ… æ·»åŠ å¼‚å¸¸å¤„ç†ï¼Œé¿å…ç¨‹åºå´©æºƒ")
        print("   6. âœ… ç”Ÿæˆå®Œæ•´çš„å›æµ‹æŠ¥å‘Šå’Œå›¾è¡¨")
        print("   7. âœ… æ–°å¢ï¼šæ˜¾ç¤ºå®Œæ•´çš„åŒºé—´æ—¥æœŸå’Œå¹´åŒ–æŒ‡æ ‡")
        print("   8. âœ… æ–°å¢ï¼šç”Ÿæˆè¯¦ç»†å›æµ‹æŠ¥å‘Š")

        return {
            'models': models,
            'scaler': scaler,
            'features': feature_cols,
            'results': results,
            'backtest_results': backtest_results,
            'selected_stocks': daily_selected_df
        }

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        traceback.print_exc()
        return None


def print_backtest_metrics(metrics):
    """æ‰“å°å›æµ‹æŒ‡æ ‡ï¼ˆåŒ…å«åŒºé—´æ—¥æœŸï¼‰"""
    print_section("å›æµ‹ç»“æœæ±‡æ€»")

    # æ‰“å°åŒºé—´ä¿¡æ¯
    if 'èµ·å§‹æ—¥æœŸ' in metrics and 'ç»“æŸæ—¥æœŸ' in metrics:
        print(f"ğŸ“… å›æµ‹åŒºé—´: {metrics['èµ·å§‹æ—¥æœŸ']} è‡³ {metrics['ç»“æŸæ—¥æœŸ']}")
        print(f"   å›æµ‹å¤©æ•°: {metrics.get('å›æµ‹å¤©æ•°', 0)} å¤©")
        print(f"   äº¤æ˜“æ—¥æ•°: {metrics.get('äº¤æ˜“æ—¥æ•°', 0)} å¤©")

    # æ‰“å°æ”¶ç›Šç‡æŒ‡æ ‡
    print(f"\nğŸ“ˆ æ”¶ç›Šç‡æŒ‡æ ‡:")
    if 'æ€»æ”¶ç›Šç‡' in metrics:
        print(f"   æ€»æ”¶ç›Šç‡: {metrics['æ€»æ”¶ç›Šç‡']:.2%}")
    if 'å¹´åŒ–æ”¶ç›Šç‡' in metrics:
        print(f"   å¹´åŒ–æ”¶ç›Šç‡: {metrics['å¹´åŒ–æ”¶ç›Šç‡']:.2%}")

    # æ‰“å°é£é™©æŒ‡æ ‡
    print(f"\nâš ï¸  é£é™©æŒ‡æ ‡:")
    if 'å¹´åŒ–æ³¢åŠ¨ç‡' in metrics:
        print(f"   å¹´åŒ–æ³¢åŠ¨ç‡: {metrics['å¹´åŒ–æ³¢åŠ¨ç‡']:.2%}")
    if 'æœ€å¤§å›æ’¤' in metrics:
        print(f"   æœ€å¤§å›æ’¤: {metrics['æœ€å¤§å›æ’¤']:.2%}")

    # æ‰“å°é£é™©è°ƒæ•´æ”¶ç›ŠæŒ‡æ ‡
    print(f"\nâš–ï¸  é£é™©è°ƒæ•´æ”¶ç›ŠæŒ‡æ ‡:")
    if 'å¹´åŒ–å¤æ™®æ¯”ç‡' in metrics:
        print(f"   å¹´åŒ–å¤æ™®æ¯”ç‡: {metrics['å¹´åŒ–å¤æ™®æ¯”ç‡']:.2f}")
    if 'å¡ç›æ¯”ç‡' in metrics:
        print(f"   å¡ç›æ¯”ç‡: {metrics['å¡ç›æ¯”ç‡']:.2f}")
    if 'ä¿¡æ¯æ¯”ç‡' in metrics and metrics['ä¿¡æ¯æ¯”ç‡'] != 0:
        print(f"   ä¿¡æ¯æ¯”ç‡: {metrics['ä¿¡æ¯æ¯”ç‡']:.2f}")

    # æ‰“å°äº¤æ˜“ç»Ÿè®¡æŒ‡æ ‡
    print(f"\nğŸ’¹ äº¤æ˜“ç»Ÿè®¡æŒ‡æ ‡:")
    if 'èƒœç‡' in metrics:
        print(f"   èƒœç‡: {metrics['èƒœç‡']:.2%}")
    if 'ç›ˆäºæ¯”' in metrics:
        print(f"   ç›ˆäºæ¯”: {metrics['ç›ˆäºæ¯”']:.2f}")

    # æ‰“å°å‡€å€¼ä¿¡æ¯
    print(f"\nğŸ’° å‡€å€¼ä¿¡æ¯:")
    if 'åˆå§‹å‡€å€¼' in metrics and metrics['åˆå§‹å‡€å€¼'] > 0:
        print(f"   åˆå§‹å‡€å€¼: {metrics['åˆå§‹å‡€å€¼']:,.2f}")
    if 'æœ€ç»ˆå‡€å€¼' in metrics and metrics['æœ€ç»ˆå‡€å€¼'] > 0:
        print(f"   æœ€ç»ˆå‡€å€¼: {metrics['æœ€ç»ˆå‡€å€¼']:,.2f}")


def save_backtest_report(metrics, filepath):
    """ä¿å­˜è¯¦ç»†çš„å›æµ‹æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("è‚¡ç¥¨é€‰è‚¡ç­–ç•¥è¯¦ç»†å›æµ‹æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")

            # åŒºé—´ä¿¡æ¯
            f.write("1. å›æµ‹åŒºé—´ä¿¡æ¯\n")
            f.write("-" * 40 + "\n")
            f.write(f"èµ·å§‹æ—¥æœŸ: {metrics.get('èµ·å§‹æ—¥æœŸ', 'N/A')}\n")
            f.write(f"ç»“æŸæ—¥æœŸ: {metrics.get('ç»“æŸæ—¥æœŸ', 'N/A')}\n")
            f.write(f"å›æµ‹å¤©æ•°: {metrics.get('å›æµ‹å¤©æ•°', 0)} å¤©\n")
            f.write(f"äº¤æ˜“æ—¥æ•°: {metrics.get('äº¤æ˜“æ—¥æ•°', 0)} å¤©\n\n")

            # æ”¶ç›Šç‡æŒ‡æ ‡
            f.write("2. æ”¶ç›Šç‡æŒ‡æ ‡\n")
            f.write("-" * 40 + "\n")
            for key in ['æ€»æ”¶ç›Šç‡', 'å¹´åŒ–æ”¶ç›Šç‡']:
                if key in metrics:
                    f.write(f"{key}: {metrics[key]:.2%}\n")

            # é£é™©æŒ‡æ ‡
            f.write("\n3. é£é™©æŒ‡æ ‡\n")
            f.write("-" * 40 + "\n")
            for key in ['å¹´åŒ–æ³¢åŠ¨ç‡', 'æœ€å¤§å›æ’¤']:
                if key in metrics:
                    f.write(f"{key}: {metrics[key]:.2%}\n")

            # é£é™©è°ƒæ•´æ”¶ç›ŠæŒ‡æ ‡
            f.write("\n4. é£é™©è°ƒæ•´æ”¶ç›ŠæŒ‡æ ‡\n")
            f.write("-" * 40 + "\n")
            for key in ['å¹´åŒ–å¤æ™®æ¯”ç‡', 'å¡ç›æ¯”ç‡', 'ä¿¡æ¯æ¯”ç‡']:
                if key in metrics:
                    if key == 'ä¿¡æ¯æ¯”ç‡' and metrics[key] == 0:
                        continue
                    value = metrics[key]
                    if isinstance(value, float):
                        f.write(f"{key}: {value:.2f}\n")
                    else:
                        f.write(f"{key}: {value}\n")

            # äº¤æ˜“ç»Ÿè®¡
            f.write("\n5. äº¤æ˜“ç»Ÿè®¡æŒ‡æ ‡\n")
            f.write("-" * 40 + "\n")
            for key in ['èƒœç‡', 'ç›ˆäºæ¯”']:
                if key in metrics:
                    value = metrics[key]
                    if key == 'èƒœç‡':
                        f.write(f"{key}: {value:.2%}\n")
                    else:
                        f.write(f"{key}: {value:.2f}\n")

            # å‡€å€¼ä¿¡æ¯
            f.write("\n6. å‡€å€¼ä¿¡æ¯\n")
            f.write("-" * 40 + "\n")
            for key in ['åˆå§‹å‡€å€¼', 'æœ€ç»ˆå‡€å€¼']:
                if key in metrics and metrics[key] > 0:
                    f.write(f"{key}: {metrics[key]:,.2f}\n")

            # è®¡ç®—è¯´æ˜
            f.write("\n" + "=" * 60 + "\n")
            f.write("è®¡ç®—è¯´æ˜:\n")
            f.write("-" * 40 + "\n")
            f.write("1. å¹´åŒ–æ”¶ç›Šç‡ = (1 + æ€»æ”¶ç›Šç‡)^(365.25/å¤©æ•°) - 1\n")
            f.write("2. å¹´åŒ–æ³¢åŠ¨ç‡ = æ—¥æ”¶ç›Šç‡æ ‡å‡†å·® Ã— âˆš252\n")
            f.write("3. å¹´åŒ–å¤æ™®æ¯”ç‡ = å¹´åŒ–æ”¶ç›Šç‡ / å¹´åŒ–æ³¢åŠ¨ç‡ (å‡è®¾æ— é£é™©åˆ©ç‡ä¸º0)\n")
            f.write("4. æœ€å¤§å›æ’¤ = æœ€ä½ç‚¹å‡€å€¼ / æœ€é«˜ç‚¹å‡€å€¼ - 1\n")
            f.write("5. å¡ç›æ¯”ç‡ = å¹´åŒ–æ”¶ç›Šç‡ / æœ€å¤§å›æ’¤ (ç»å¯¹å€¼)\n")
            f.write("6. ä¿¡æ¯æ¯”ç‡ = (å¹´åŒ–æ”¶ç›Šç‡ - åŸºå‡†å¹´åŒ–æ”¶ç›Šç‡) / è·Ÿè¸ªè¯¯å·®\n")

            # é£é™©è¯„ä¼°
            f.write("\n" + "=" * 60 + "\n")
            f.write("é£é™©è¯„ä¼°:\n")
            f.write("-" * 40 + "\n")

            if metrics.get('å¹´åŒ–å¤æ™®æ¯”ç‡', 0) > 1.0:
                f.write("âœ… å¤æ™®æ¯”ç‡ > 1.0: ç­–ç•¥è¡¨ç°ä¼˜ç§€\n")
            elif metrics.get('å¹´åŒ–å¤æ™®æ¯”ç‡', 0) > 0.5:
                f.write("âš ï¸  å¤æ™®æ¯”ç‡ 0.5-1.0: ç­–ç•¥è¡¨ç°è‰¯å¥½\n")
            else:
                f.write("âŒ å¤æ™®æ¯”ç‡ < 0.5: ç­–ç•¥é£é™©è°ƒæ•´æ”¶ç›Šåä½\n")

            if metrics.get('æœ€å¤§å›æ’¤', 0) > -0.20:
                f.write("âœ… æœ€å¤§å›æ’¤ < 20%: é£é™©æ§åˆ¶è‰¯å¥½\n")
            elif metrics.get('æœ€å¤§å›æ’¤', 0) > -0.30:
                f.write("âš ï¸  æœ€å¤§å›æ’¤ 20%-30%: é£é™©æ§åˆ¶ä¸€èˆ¬\n")
            else:
                f.write("âŒ æœ€å¤§å›æ’¤ > 30%: é£é™©æ§åˆ¶éœ€è¦æ”¹è¿›\n")

        print(f"âœ… è¯¦ç»†å›æµ‹æŠ¥å‘Šå·²ä¿å­˜: {filepath}")

    except Exception as e:
        print(f"ä¿å­˜å›æµ‹æŠ¥å‘Šå¤±è´¥: {e}")

def plot_simple_backtest_results(backtest_results, timestamp):
    """ç»˜åˆ¶ç®€å•çš„å›æµ‹å›¾è¡¨"""
    try:
        import matplotlib.pyplot as plt

        # å‡€å€¼æ›²çº¿
        portfolio_values = [pv.get('total_value', pv.get('portfolio_value', 0))
                            for pv in backtest_results['portfolio_values']]
        dates = [pv['date'] for pv in backtest_results['portfolio_values']]

        plt.figure(figsize=(12, 8))

        # å‡€å€¼æ›²çº¿
        plt.subplot(2, 2, 1)
        plt.plot(dates, portfolio_values, 'b-', linewidth=2)
        plt.title('å‡€å€¼æ›²çº¿')
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('å‡€å€¼ï¼ˆå…ƒï¼‰')
        plt.grid(True, alpha=0.3)

        # å›æ’¤æ›²çº¿
        plt.subplot(2, 2, 2)
        running_max = pd.Series(portfolio_values).expanding().max()
        drawdown = (pd.Series(portfolio_values) - running_max) / running_max
        plt.fill_between(dates, drawdown, 0, alpha=0.3, color='red')
        plt.title('å›æ’¤æ›²çº¿')
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('å›æ’¤')
        plt.grid(True, alpha=0.3)

        # æ—¥æ”¶ç›Šç‡åˆ†å¸ƒ
        plt.subplot(2, 2, 3)
        returns = backtest_results.get('portfolio_returns', [])
        if returns:
            plt.hist(returns, bins=50, edgecolor='black', alpha=0.7)
            plt.title('æ—¥æ”¶ç›Šç‡åˆ†å¸ƒ')
            plt.xlabel('æ—¥æ”¶ç›Šç‡')
            plt.ylabel('é¢‘æ¬¡')
            plt.grid(True, alpha=0.3)

        # å…³é”®æŒ‡æ ‡
        plt.subplot(2, 2, 4)
        plt.axis('off')

        metrics = backtest_results.get('metrics', {})
        stats = backtest_results.get('trading_stats', {})

        text = f"""å…³é”®æŒ‡æ ‡:
æ€»æ”¶ç›Šç‡: {metrics.get('æ€»æ”¶ç›Šç‡', 0):.2%}
å¹´åŒ–æ”¶ç›Šç‡: {metrics.get('å¹´åŒ–æ”¶ç›Šç‡', 0):.2%}
å¤æ™®æ¯”ç‡: {metrics.get('å¤æ™®æ¯”ç‡', 0):.2f}
æœ€å¤§å›æ’¤: {metrics.get('æœ€å¤§å›æ’¤', 0):.2%}

äº¤æ˜“ç»Ÿè®¡:
æ€»äº¤æ˜“æ¬¡æ•°: {stats.get('æ€»äº¤æ˜“æ¬¡æ•°', 0)}
å¹´åŒ–æ¢æ‰‹ç‡: {stats.get('å¹´åŒ–æ¢æ‰‹ç‡', 0):.2%}
å¹³å‡æŒä»“å¤©æ•°: {stats.get('å¹³å‡æŒä»“å¤©æ•°', 0):.1f}å¤©
èƒœç‡: {stats.get('èƒœç‡', 0):.2%}"""

        plt.text(0.1, 0.5, text, fontsize=10, verticalalignment='center')

        plt.tight_layout()

        plot_file = f'backtest_chart_{timestamp}.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"âœ… å›æµ‹å›¾è¡¨å·²ä¿å­˜: {plot_file}")

    except Exception as e:
        print(f"ç»˜åˆ¶å›¾è¡¨æ—¶å‡ºé”™: {e}")

# ==================== è¿è¡Œç¨‹åº ====================
# ==================== è¿è¡Œç¨‹åº ====================
if __name__ == "__main__":
    print("å¼€å§‹è¿è¡Œå°æ¹¾è‚¡ç¥¨è¶…é¢æ”¶ç›Šé¢„æµ‹æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰...")
    print("=" * 60)

    result = main()

    if result is not None:
        print_section("ç¨‹åºæ‰§è¡ŒæˆåŠŸ!")
        print("å·²ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼ˆ*ä¸ºæ—¶é—´æˆ³å ä½ç¬¦ï¼‰:")
        print("â”œâ”€ selected_stocks_*.csv        - æ¯æ—¥é€‰è‚¡åˆ—è¡¨")

        # åªæœ‰åœ¨backtest_resultså­˜åœ¨æ—¶æ‰æ˜¾ç¤º
        if result.get('backtest_results') is not None:
            print("â”œâ”€ backtest_results_*.pkl       - è¯¦ç»†å›æµ‹ç»“æœ")
            print("â””â”€ backtest_report_*.txt        - å›æµ‹æŠ¥å‘Š")
        else:
            print("â””â”€ å›æµ‹æœªæ‰§è¡Œæˆ–å¤±è´¥ï¼Œæœªç”Ÿæˆå›æµ‹ç»“æœæ–‡ä»¶")

        # æ‰“å°ä¼˜åŒ–æ•ˆæœ - ä¿®å¤ï¼šå…ˆæ£€æŸ¥backtest_resultsæ˜¯å¦å­˜åœ¨
        print("\nğŸ”§ ä¼˜åŒ–æ•ˆæœæ€»ç»“:")

        # å®‰å…¨åœ°è·å–ç¼“å­˜å‘½ä¸­ç‡
        cache_hit_rate = 0
        if (result.get('backtest_results') is not None and
                'cache_stats' in result['backtest_results']):
            cache_hit_rate = result['backtest_results']['cache_stats'].get('hit_rate', 0)

        print(f"   1. âœ… äº¤æ˜“æˆæœ¬ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.2%}")
        print("   2. âœ… ä»“ä½ç®¡ç†å™¨ç¡®ä¿å•åªè‚¡ç¥¨ä¸è¶…5%é™åˆ¶")
        print("   3. âœ… å¢åŠ æœ€å°æŒæœ‰æœŸ(5å¤©)ï¼Œå‡å°‘æ—¥å†…äº¤æ˜“")
        print("   4. âœ… è°ƒæ•´è°ƒä»“é¢‘ç‡ä¸ºæ¯å­£åº¦ï¼Œé™ä½æ¢æ‰‹ç‡")
        print("   5. âœ… ä¼˜åŒ–æ­¢æŸæ­¢ç›ˆé˜ˆå€¼ï¼Œå‡å°‘æ— æ•ˆäº¤æ˜“")
        print("   6. âœ… é™åˆ¶æ¯æ—¥äº¤æ˜“æ¬¡æ•°ï¼Œé¿å…è¿‡åº¦äº¤æ˜“")

        # åªæœ‰åœ¨backtest_resultså­˜åœ¨æ—¶æ‰æ‰“å°å…³é”®æŒ‡æ ‡
        if result.get('backtest_results') is not None:
            metrics = result['backtest_results'].get('metrics', {})
            trading_stats = result['backtest_results'].get('trading_stats', {})

            print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
            print(f"   æ€»æ”¶ç›Šç‡: {metrics.get('æ€»æ”¶ç›Šç‡', 0):.2%}")
            print(f"   æœ€å¤§å›æ’¤: {metrics.get('æœ€å¤§å›æ’¤', 0):.2%}")
            print(f"   å¹´åŒ–æ¢æ‰‹ç‡: {trading_stats.get('å¹´åŒ–æ¢æ‰‹ç‡', 0):.2%}")
            print(f"   å¹³å‡æŒä»“å¤©æ•°: {trading_stats.get('å¹³å‡æŒä»“å¤©æ•°', 0):.1f}å¤©")
        else:
            print("\nâš ï¸ æ— æœ‰æ•ˆå›æµ‹ç»“æœï¼Œæ— æ³•æ˜¾ç¤ºå…³é”®æŒ‡æ ‡")
    else:
        print_section("ç¨‹åºæ‰§è¡Œå¤±è´¥!")